# Quick Test Configuration (small model + custom processed data)
model:
  model_name: "distilgpt2"
  model_type: "gpt2"
  trust_remote_code: false
  cache_dir: "./cache"

lora:
  r: 8
  lora_alpha: 16
  target_modules:
    - "c_attn"
    - "c_proj"
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

training:
  output_dir: "./results/models/distilgpt2-lora-test"
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 0.0002
  warmup_steps: 10
  logging_steps: 5
  save_steps: 50
  eval_steps: 50
  dataloader_num_workers: 0
  report_to: []
  seed: 42
  data_seed: 42
  group_by_length: true

data:
  dataset_name: "custom"
  train_file: "./data/processed/train.jsonl"
  validation_file: "./data/processed/val.jsonl"
  max_seq_length: 256
