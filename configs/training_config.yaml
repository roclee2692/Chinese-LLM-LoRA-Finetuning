# 训练配置文件
training:
  # 基础训练参数
  output_dir: "./results/models"
  overwrite_output_dir: true
  do_train: true
  do_eval: true
  
  # 训练轮次和批次
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  
  # 学习率和优化器
  learning_rate: 1e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # 学习率调度
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  
  # 保存和评估
  save_strategy: "steps"
  save_steps: 500
  eval_strategy: "steps"
  eval_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  
  # 日志记录
  logging_strategy: "steps"
  logging_steps: 100
  report_to: ["tensorboard", "wandb"]
  
  # 数据处理
  dataloader_num_workers: 4
  remove_unused_columns: false
  
  # 混合精度训练
  fp16: false
  bf16: true
  
  # DeepSpeed配置（可选）
  deepspeed: null
  
  # 其他设置
  seed: 42
  data_seed: 42
  group_by_length: true
  length_column_name: "input_length"

# 早停配置
early_stopping:
  early_stopping_patience: 3
  early_stopping_threshold: 0.001

# 验证配置
validation:
  max_eval_samples: 1000
  eval_accumulation_steps: 1

# 生成配置（用于评估）
generation:
  max_new_tokens: 256
  do_sample: true
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  pad_token_id: 0
  eos_token_id: 2

# W&B配置
wandb:
  project: "chinese-llm-lora"
  name: null  # 自动生成
  tags: ["lora", "chinese", "finetune"]
  notes: "Chinese LLM LoRA fine-tuning experiment"