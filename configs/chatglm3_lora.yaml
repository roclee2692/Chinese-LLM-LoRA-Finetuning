# ChatGLM3 LoRA 微调配置
model:
  model_name: "THUDM/chatglm3-6b"
  model_type: "chatglm3"
  cache_dir: "./cache"

lora:
  r: 8
  lora_alpha: 32
  target_modules: ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"]
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

training:
  output_dir: "./results/chatglm3-lora"
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 1e-4
  warmup_ratio: 0.03
  logging_steps: 50
  save_steps: 500
  eval_steps: 500
  bf16: true

data:
  dataset_name: "belle"
  max_seq_length: 512
  train_file: null
  validation_file: null