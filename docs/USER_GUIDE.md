# ğŸ“– ç”¨æˆ·ä½¿ç”¨æŒ‡å—

æ¬¢è¿ä½¿ç”¨ Chinese-LLM-LoRA-Finetuning æ¡†æ¶ï¼æœ¬æŒ‡å—å°†è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨æ¡†æ¶çš„å„é¡¹åŠŸèƒ½ã€‚

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå…¥é—¨](#-å¿«é€Ÿå…¥é—¨)
- [é…ç½®è¯¦è§£](#-é…ç½®è¯¦è§£)
- [æ•°æ®å‡†å¤‡](#-æ•°æ®å‡†å¤‡)
- [æ¨¡å‹è®­ç»ƒ](#-æ¨¡å‹è®­ç»ƒ)
- [Webç•Œé¢ä½¿ç”¨](#-webç•Œé¢ä½¿ç”¨)
- [æ¨¡å‹è¯„ä¼°](#-æ¨¡å‹è¯„ä¼°)
- [æ•…éšœæ’é™¤](#-æ•…éšœæ’é™¤)
- [é«˜çº§åŠŸèƒ½](#-é«˜çº§åŠŸèƒ½)

---

## ğŸš€ å¿«é€Ÿå…¥é—¨

### ç¬¬ä¸€æ¬¡ä½¿ç”¨

1. **ç¯å¢ƒæ£€æŸ¥**
```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬ (éœ€è¦3.8+)
python --version

# æ£€æŸ¥CUDA (å¯é€‰ï¼Œç”¨äºGPUåŠ é€Ÿ)
nvidia-smi
```

2. **å®‰è£…æ¡†æ¶**
```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/roclee2692/Chinese-LLM-LoRA-Finetuning.git
cd Chinese-LLM-LoRA-Finetuning

# è¿è¡Œè‡ªåŠ¨å®‰è£…è„šæœ¬
python setup_environment.py
```

3. **éªŒè¯å®‰è£…**
```bash
# è¿è¡Œç¯å¢ƒéªŒè¯
python verify_installation.py

# é¢„æœŸè¾“å‡º
âœ… Pythonç‰ˆæœ¬: 3.11.5
âœ… PyTorch: 2.5.1+cu121  
âœ… CUDAå¯ç”¨: True
âœ… æ˜¾å­˜: 8.0GB
âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²æ­£ç¡®å®‰è£…
```

4. **å¿«é€Ÿä½“éªŒ**
```bash
# è¿è¡Œå¿«é€Ÿæµ‹è¯• (çº¦3åˆ†é’Ÿ)
python src/train.py --config configs/quick_test.yaml

# å¯åŠ¨Webç•Œé¢
python demo/gradio_demo.py
```

---

## âš™ï¸ é…ç½®è¯¦è§£

### é…ç½®æ–‡ä»¶ç»“æ„

æ¡†æ¶ä½¿ç”¨YAMLæ ¼å¼çš„é…ç½®æ–‡ä»¶ï¼Œä½äº `configs/` ç›®å½•ï¼š

```
configs/
â”œâ”€â”€ quick_test.yaml          # å¿«é€Ÿæµ‹è¯•é…ç½®
â”œâ”€â”€ production_training.yaml # ç”Ÿäº§ç¯å¢ƒé…ç½®  
â”œâ”€â”€ chatglm3_lora.yaml      # ChatGLM3ä¸“ç”¨é…ç½®
â”œâ”€â”€ lightweight_training.yaml # è½»é‡çº§è®­ç»ƒé…ç½®
â””â”€â”€ model_config.yaml       # åŸºç¡€æ¨¡å‹é…ç½®
```

### åŸºç¡€é…ç½®è¯´æ˜

#### æ¨¡å‹é…ç½®
```yaml
model:
  model_name: "distilgpt2"      # HuggingFaceæ¨¡å‹åç§°
  model_type: "gpt2"            # æ¨¡å‹ç±»å‹
  trust_remote_code: false      # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
  torch_dtype: "auto"           # æ•°æ®ç±»å‹ (auto/float16/bfloat16)
  device_map: "auto"            # è®¾å¤‡æ˜ å°„ç­–ç•¥
```

**å¸¸ç”¨æ¨¡å‹åç§°:**
- `distilgpt2` - å¿«é€Ÿæµ‹è¯•ç”¨
- `THUDM/chatglm3-6b` - ä¸­æ–‡å¯¹è¯
- `Qwen/Qwen-7B-Chat` - é€šç”¨ä»»åŠ¡
- `baichuan-inc/Baichuan2-7B-Chat` - ä¸­æ–‡ç†è§£

#### LoRAé…ç½®
```yaml
lora:
  r: 8                          # LoRAç§© (å»ºè®®4-32)
  lora_alpha: 16               # LoRAç¼©æ”¾å‚æ•° (é€šå¸¸ä¸º2*r)
  target_modules:              # ç›®æ ‡æ¨¡å—
    - "c_attn"                 # æ³¨æ„åŠ›å±‚
    - "c_proj"                 # æŠ•å½±å±‚
  lora_dropout: 0.1            # Dropoutç‡
  bias: "none"                 # åç½®å¤„ç† (none/all/lora_only)
  task_type: "CAUSAL_LM"       # ä»»åŠ¡ç±»å‹
```

**å‚æ•°é€‰æ‹©æŒ‡å—:**
- **r=4**: æœ€å°å‚æ•°ï¼Œå¿«é€Ÿè®­ç»ƒ
- **r=8**: å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡ (æ¨è)
- **r=16**: æ›´å¥½æ€§èƒ½ï¼Œéœ€è¦æ›´å¤šèµ„æº
- **r=32**: æœ€ä½³æ€§èƒ½ï¼Œé«˜èµ„æºéœ€æ±‚

#### è®­ç»ƒé…ç½®
```yaml
training:
  output_dir: "./results/my_model"    # è¾“å‡ºç›®å½•
  num_train_epochs: 3                 # è®­ç»ƒè½®æ•°
  per_device_train_batch_size: 1      # æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°
  gradient_accumulation_steps: 8      # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
  learning_rate: 2e-4                # å­¦ä¹ ç‡
  weight_decay: 0.01                 # æƒé‡è¡°å‡
  logging_steps: 10                  # æ—¥å¿—é—´éš”
  save_steps: 500                    # ä¿å­˜é—´éš”
  eval_steps: 500                    # è¯„ä¼°é—´éš”
  max_seq_length: 512                # æœ€å¤§åºåˆ—é•¿åº¦
  warmup_ratio: 0.1                  # é¢„çƒ­æ¯”ä¾‹
  lr_scheduler_type: "cosine"        # å­¦ä¹ ç‡è°ƒåº¦å™¨
```

---

## ğŸ“Š æ•°æ®å‡†å¤‡

### æ•°æ®æ ¼å¼è¦æ±‚

æ¡†æ¶æ”¯æŒæ ‡å‡†çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®æ ¼å¼ï¼š

```json
{
  "instruction": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
  "input": "",
  "output": "äººå·¥æ™ºèƒ½æ˜¯è®©è®¡ç®—æœºæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æŠ€æœ¯..."
}
```

æˆ–å¯¹è¯æ ¼å¼ï¼š

```json
{
  "conversations": [
    {
      "from": "human", 
      "value": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"
    },
    {
      "from": "gpt",
      "value": "æ‚¨å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹..."
    }
  ]
}
```

### æ•°æ®é¢„å¤„ç†

1. **è‡ªåŠ¨æ•°æ®ä¸‹è½½**
```bash
# ä¸‹è½½å¸¸ç”¨ä¸­æ–‡æ•°æ®é›†
python scripts/download_data.py --dataset alpaca_chinese

# å¯é€‰æ•°æ®é›†:
# - alpaca_chinese: ä¸­æ–‡Alpacaæ•°æ®
# - belle: BELLEä¸­æ–‡æŒ‡ä»¤æ•°æ®  
# - firefly: æµè¤ä¸­æ–‡å¯¹è¯æ•°æ®
```

2. **æ•°æ®æ ¼å¼ä¿®å¤**
```bash
# ä¿®å¤æ•°æ®æ ¼å¼é—®é¢˜
python fix_data_format.py --input data/raw/your_data.json --output data/processed/

# æ”¯æŒçš„ä¿®å¤åŠŸèƒ½:
# âœ… åµŒå¥—å­—å…¸å±•å¹³
# âœ… ç¼–ç æ ¼å¼è½¬æ¢
# âœ… å­—æ®µåæ ‡å‡†åŒ–
# âœ… æ•°æ®ç±»å‹è½¬æ¢
```

3. **æ•°æ®è´¨é‡æ£€æŸ¥**
```bash
# è¿è¡Œæ•°æ®åˆ†æ
jupyter notebook notebooks/data_analysis.ipynb

# æˆ–ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·
python src/utils.py --check_data data/processed/train.json
```

### è‡ªå®šä¹‰æ•°æ®é›†

å¦‚æœæ‚¨æœ‰è‡ªå·±çš„æ•°æ®é›†ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å¤„ç†ï¼š

1. **è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼**
```python
import json

# ç¤ºä¾‹è½¬æ¢è„šæœ¬
def convert_to_standard_format(your_data):
    standard_data = []
    for item in your_data:
        standard_item = {
            "instruction": item["question"],  # æ›¿æ¢ä¸ºæ‚¨çš„å­—æ®µå
            "input": "",
            "output": item["answer"]          # æ›¿æ¢ä¸ºæ‚¨çš„å­—æ®µå
        }
        standard_data.append(standard_item)
    return standard_data

# ä¿å­˜ä¸ºJSONæ–‡ä»¶
with open("data/processed/my_dataset.json", "w", encoding="utf-8") as f:
    json.dump(standard_data, f, ensure_ascii=False, indent=2)
```

2. **æ•°æ®åˆ†å‰²**
```python
# ä½¿ç”¨å†…ç½®å·¥å…·åˆ†å‰²æ•°æ®
from src.utils import split_dataset

train_data, val_data, test_data = split_dataset(
    "data/processed/my_dataset.json",
    train_ratio=0.8,
    val_ratio=0.1,
    test_ratio=0.1
)
```

---

## ğŸ¯ æ¨¡å‹è®­ç»ƒ

### è®­ç»ƒæµç¨‹

1. **é€‰æ‹©é…ç½®**
```bash
# å¿«é€Ÿæµ‹è¯• (5åˆ†é’Ÿï¼ŒéªŒè¯æµç¨‹)
python src/train.py --config configs/quick_test.yaml

# è½»é‡çº§è®­ç»ƒ (1å°æ—¶ï¼Œå°è§„æ¨¡æ•°æ®)  
python src/train.py --config configs/lightweight_training.yaml

# ç”Ÿäº§ç¯å¢ƒè®­ç»ƒ (æ•°å°æ—¶ï¼Œå®Œæ•´æ•°æ®é›†)
python src/train.py --config configs/production_training.yaml
```

2. **ç›‘æ§è®­ç»ƒ**

è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼ç›‘æ§ï¼š

**ç»ˆç«¯è¾“å‡º:**
```
Step 10/100: loss=3.45, lr=2.0e-4, time=00:32
Step 20/100: loss=3.12, lr=2.0e-4, time=01:05  
Step 30/100: loss=2.98, lr=2.0e-4, time=01:38
```

**TensorBoard:**
```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir results/logs --port 6006
```

**Weights & Biases (å¯é€‰):**
```bash
# å®‰è£…wandb
pip install wandb

# ç™»å½•å¹¶é…ç½®
wandb login
```

3. **è®­ç»ƒå‚æ•°è°ƒä¼˜**

æ ¹æ®æ‚¨çš„ç¡¬ä»¶é…ç½®è°ƒæ•´å‚æ•°ï¼š

**8GB GPU (RTX 4060):**
```yaml
training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  max_seq_length: 256
  fp16: true
```

**16GB GPU (RTX 4080):**
```yaml
training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  max_seq_length: 512
  fp16: true
```

**24GB+ GPU (RTX 4090):**
```yaml
training:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  max_seq_length: 1024
  bf16: true
```

### è®­ç»ƒæŠ€å·§

1. **æ˜¾å­˜ä¼˜åŒ–**
```yaml
# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
gradient_checkpointing: true

# ä½¿ç”¨DeepSpeed ZeRO
deepspeed_config: "configs/ds_config.json"

# 8-bitä¼˜åŒ–å™¨
optim: "adamw_8bit"
```

2. **è®­ç»ƒç¨³å®šæ€§**
```yaml
# æ¢¯åº¦è£å‰ª
max_grad_norm: 1.0

# å­¦ä¹ ç‡é¢„çƒ­
warmup_ratio: 0.1

# æ ‡ç­¾å¹³æ»‘
label_smoothing_factor: 0.1
```

3. **æ”¶æ•›åŠ é€Ÿ**
```yaml
# è‡ªé€‚åº”å­¦ä¹ ç‡
lr_scheduler_type: "cosine_with_restarts"

# æ—©åœç­–ç•¥
early_stopping_patience: 5
early_stopping_threshold: 0.01
```

---

## ğŸŒ Webç•Œé¢ä½¿ç”¨

### å¯åŠ¨ç•Œé¢

```bash
# åŸºæœ¬å¯åŠ¨
python demo/gradio_demo.py

# è‡ªå®šä¹‰ç«¯å£
python demo/gradio_demo.py --port 7860

# å…¬å¼€è®¿é—®
python demo/gradio_demo.py --share

# æŒ‡å®šæ¨¡å‹
python demo/gradio_demo.py --model_path results/models/my_model
```

### ç•Œé¢åŠŸèƒ½

#### 1. æ¨¡å‹å¯¹è¯
- **å•è½®å¯¹è¯**: è¾“å…¥é—®é¢˜ï¼Œè·å–å›ç­”
- **å¤šè½®å¯¹è¯**: æ”¯æŒä¸Šä¸‹æ–‡è®°å¿†
- **å‚æ•°è°ƒèŠ‚**: è°ƒæ•´temperatureã€top_pç­‰å‚æ•°

#### 2. æ¨¡å‹å¯¹æ¯”
- **å¹¶æ’å¯¹æ¯”**: åŒæ—¶æµ‹è¯•å¤šä¸ªæ¨¡å‹
- **æ€§èƒ½è¯„ä¼°**: è‡ªåŠ¨è®¡ç®—BLEUã€ROUGEåˆ†æ•°
- **æ‰¹é‡æµ‹è¯•**: ä¸Šä¼ æµ‹è¯•æ–‡ä»¶è¿›è¡Œæ‰¹é‡è¯„ä¼°

#### 3. ç³»ç»Ÿä¿¡æ¯
- **ç¯å¢ƒæ£€æŸ¥**: æ˜¾ç¤ºPythonã€PyTorchç‰ˆæœ¬
- **ç¡¬ä»¶çŠ¶æ€**: GPUä½¿ç”¨ç‡ã€å†…å­˜å ç”¨
- **æ¨¡å‹ä¿¡æ¯**: å‚æ•°é‡ã€é‡åŒ–çŠ¶æ€

#### 4. é…ç½®ç®¡ç†
- **åœ¨çº¿ç¼–è¾‘**: ç›´æ¥ä¿®æ”¹è®­ç»ƒé…ç½®
- **é…ç½®ä¸‹è½½**: ä¿å­˜è‡ªå®šä¹‰é…ç½®
- **é¢„è®¾æ¨¡æ¿**: é€‰æ‹©é¢„å®šä¹‰é…ç½®

### ä½¿ç”¨æŠ€å·§

1. **å¯¹è¯ä¼˜åŒ–**
```python
# è°ƒæ•´ç”Ÿæˆå‚æ•°è·å¾—æ›´å¥½æ•ˆæœ
temperature: 0.7     # æ§åˆ¶éšæœºæ€§
top_p: 0.9          # æ ¸é‡‡æ ·
max_length: 512     # æœ€å¤§ç”Ÿæˆé•¿åº¦
repetition_penalty: 1.1  # é‡å¤æƒ©ç½š
```

2. **æ‰¹é‡è¯„ä¼°**
```json
# å‡†å¤‡æµ‹è¯•æ–‡ä»¶ test_cases.json
[
  {
    "instruction": "è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†",
    "expected": "é‡å­è®¡ç®—æ˜¯åŸºäºé‡å­åŠ›å­¦åŸç†..."
  },
  {
    "instruction": "æ¯”è¾ƒPythonå’ŒJavaçš„åŒºåˆ«", 
    "expected": "Pythonå’ŒJavaéƒ½æ˜¯æµè¡Œçš„ç¼–ç¨‹è¯­è¨€..."
  }
]
```

---

## ğŸ“ˆ æ¨¡å‹è¯„ä¼°

### è‡ªåŠ¨è¯„ä¼°

```bash
# åŸºç¡€è¯„ä¼°
python src/evaluate.py --model_path results/models/my_model

# è¯¦ç»†è¯„ä¼°
python src/evaluate.py \
  --model_path results/models/my_model \
  --test_file data/processed/test.json \
  --output_file results/evaluation/detailed_report.json

# å¯¹æ¯”è¯„ä¼°
python src/evaluate.py \
  --models results/models/model1 results/models/model2 \
  --compare
```

### è¯„ä¼°æŒ‡æ ‡

æ¡†æ¶æ”¯æŒå¤šç§è¯„ä¼°æŒ‡æ ‡ï¼š

1. **è‡ªåŠ¨æŒ‡æ ‡**
   - **BLEU**: æœºå™¨ç¿»è¯‘è´¨é‡è¯„ä¼°
   - **ROUGE**: æ–‡æœ¬æ‘˜è¦è´¨é‡è¯„ä¼°  
   - **BERTScore**: åŸºäºBERTçš„è¯­ä¹‰ç›¸ä¼¼åº¦
   - **Perplexity**: è¯­è¨€æ¨¡å‹å›°æƒ‘åº¦

2. **äººå·¥è¯„ä¼°**
   - **æµç•…åº¦**: è¯­è¨€è¡¨è¾¾çš„è‡ªç„¶ç¨‹åº¦
   - **ç›¸å…³æ€§**: å›ç­”ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦
   - **æœ‰ç”¨æ€§**: å›ç­”çš„å®ç”¨ä»·å€¼
   - **å®‰å…¨æ€§**: å†…å®¹çš„å®‰å…¨å’Œåˆè§„æ€§

### äº¤äº’å¼è¯„ä¼°

```bash
# å¯åŠ¨äº¤äº’å¼è¯„ä¼°
python src/evaluate.py --model_path results/models/my_model --interactive

# ç¤ºä¾‹ä¼šè¯:
>>> è¯·ä»‹ç»ä¸€ä¸‹åŒ—äº¬çš„å¤©æ°”ç‰¹ç‚¹
æ¨¡å‹å›ç­”: åŒ—äº¬å±äºæ¸©å¸¦å¤§é™†æ€§å­£é£æ°”å€™ï¼Œå››å­£åˆ†æ˜...
>>> è¯„åˆ† (1-10): 8
>>> è¯„è¯­: å›ç­”å‡†ç¡®ä½†å¯ä»¥æ›´è¯¦ç»†
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. ç¯å¢ƒé—®é¢˜

**Q: ModuleNotFoundError: No module named 'torch'**
```bash
# è§£å†³æ–¹æ¡ˆ: é‡æ–°å®‰è£…PyTorch
pip uninstall torch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

**Q: CUDA out of memory**
```bash
# è§£å†³æ–¹æ¡ˆ: è°ƒæ•´è®­ç»ƒå‚æ•°
per_device_train_batch_size: 1  # å‡å°æ‰¹æ¬¡å¤§å°
gradient_accumulation_steps: 8  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
max_seq_length: 256            # å‡å°åºåˆ—é•¿åº¦
fp16: true                     # å¯ç”¨æ··åˆç²¾åº¦
```

#### 2. æ•°æ®é—®é¢˜

**Q: æ•°æ®æ ¼å¼é”™è¯¯**
```bash
# è§£å†³æ–¹æ¡ˆ: è¿è¡Œæ•°æ®ä¿®å¤è„šæœ¬
python fix_data_format.py --input your_data.json

# æˆ–æ‰‹åŠ¨æ£€æŸ¥æ•°æ®æ ¼å¼
python -c "
import json
with open('your_data.json') as f:
    data = json.load(f)
    print(f'æ•°æ®æ¡æ•°: {len(data)}')
    print(f'ç¤ºä¾‹æ•°æ®: {data[0]}')
"
```

**Q: ç¼–ç é”™è¯¯**
```bash
# è§£å†³æ–¹æ¡ˆ: è½¬æ¢æ–‡ä»¶ç¼–ç 
python -c "
import codecs
with codecs.open('input.txt', 'r', 'gbk') as f:
    content = f.read()
with codecs.open('output.txt', 'w', 'utf-8') as f:
    f.write(content)
"
```

#### 3. è®­ç»ƒé—®é¢˜

**Q: è®­ç»ƒæŸå¤±ä¸ä¸‹é™**
```yaml
# è§£å†³æ–¹æ¡ˆ: è°ƒæ•´å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨
learning_rate: 1e-4  # é™ä½å­¦ä¹ ç‡
warmup_ratio: 0.1    # å¢åŠ é¢„çƒ­
optim: "adamw_torch" # ä½¿ç”¨ä¸åŒä¼˜åŒ–å™¨
```

**Q: è®­ç»ƒè¿‡ç¨‹ä¸­æ–­**
```yaml
# è§£å†³æ–¹æ¡ˆ: å¯ç”¨æ–­ç‚¹ç»­è®­
resume_from_checkpoint: "results/checkpoint-1000"
save_steps: 100  # æ›´é¢‘ç¹ä¿å­˜
```

#### 4. æ¨ç†é—®é¢˜

**Q: ç”Ÿæˆå†…å®¹é‡å¤**
```python
# è§£å†³æ–¹æ¡ˆ: è°ƒæ•´ç”Ÿæˆå‚æ•°
generation_config = {
    "repetition_penalty": 1.2,
    "no_repeat_ngram_size": 3,
    "do_sample": True,
    "temperature": 0.8
}
```

**Q: ç”Ÿæˆé€Ÿåº¦æ…¢**
```python
# è§£å†³æ–¹æ¡ˆ: ä¼˜åŒ–æ¨ç†è®¾ç½®
model.half()  # ä½¿ç”¨åŠç²¾åº¦
torch.backends.cudnn.benchmark = True  # å¯ç”¨cudnnä¼˜åŒ–
```

### è°ƒè¯•å·¥å…·

1. **è¯¦ç»†æ—¥å¿—**
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

2. **æ€§èƒ½åˆ†æ**
```bash
# å®‰è£…æ€§èƒ½åˆ†æå·¥å…·
pip install py-spy

# åˆ†æPythonç¨‹åº
py-spy record -o profile.svg -- python src/train.py --config configs/quick_test.yaml
```

3. **å†…å­˜ç›‘æ§**
```bash
# å®‰è£…å†…å­˜ç›‘æ§å·¥å…·
pip install memory_profiler

# ç›‘æ§å†…å­˜ä½¿ç”¨
mprof run python src/train.py --config configs/quick_test.yaml
mprof plot
```

---

## ğŸš€ é«˜çº§åŠŸèƒ½

### 1. å¤šGPUè®­ç»ƒ

```bash
# ä½¿ç”¨DistributedDataParallel
torchrun --nproc_per_node=2 src/train.py --config configs/multi_gpu.yaml

# ä½¿ç”¨DeepSpeed
deepspeed src/train.py --config configs/deepspeed.yaml --deepspeed configs/ds_config.json
```

### 2. é‡åŒ–è®­ç»ƒ

```yaml
# QLoRAé…ç½®
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
```

### 3. è‡ªå®šä¹‰æ¨¡å‹

```python
# æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹
from transformers import AutoModelForCausalLM, AutoTokenizer

class MyCustomModel(AutoModelForCausalLM):
    def __init__(self, config):
        super().__init__(config)
        # è‡ªå®šä¹‰å®ç°

# åœ¨train.pyä¸­ä½¿ç”¨
AutoModelForCausalLM.register("my_model", MyCustomModel)
```

### 4. é«˜çº§æ•°æ®å¤„ç†

```python
# è‡ªå®šä¹‰æ•°æ®å¤„ç†å™¨
from src.data_preprocessing import BaseDataProcessor

class MyDataProcessor(BaseDataProcessor):
    def process(self, raw_data):
        # è‡ªå®šä¹‰å¤„ç†é€»è¾‘
        return processed_data

# åœ¨é…ç½®ä¸­æŒ‡å®š
data:
  processor: "MyDataProcessor"
  processor_kwargs:
    special_tokens: ["<|user|>", "<|assistant|>"]
```

---

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœæœ¬æŒ‡å—æ²¡æœ‰è§£å†³æ‚¨çš„é—®é¢˜ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å–å¸®åŠ©ï¼š

1. **æŸ¥çœ‹æ–‡æ¡£**
   - [README.md](../README.md) - é¡¹ç›®æ¦‚è¿°
   - [TRAINING_REPORT.md](TRAINING_REPORT.md) - è®­ç»ƒå®éªŒæŠ¥å‘Š
   - [FAQ.md](FAQ.md) - å¸¸è§é—®é¢˜è§£ç­”

2. **ç¤¾åŒºæ”¯æŒ**
   - [GitHub Issues](https://github.com/roclee2692/Chinese-LLM-LoRA-Finetuning/issues)
   - [GitHub Discussions](https://github.com/roclee2692/Chinese-LLM-LoRA-Finetuning/discussions)

3. **è´¡çŒ®ä»£ç **
   - [CONTRIBUTING.md](../CONTRIBUTING.md) - è´¡çŒ®æŒ‡å—
   - [å¼€å‘è€…æ–‡æ¡£](DEVELOPER.md) - å¼€å‘è€…æŒ‡å—

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ19æ—¥  
**ç‰ˆæœ¬**: v1.0.0