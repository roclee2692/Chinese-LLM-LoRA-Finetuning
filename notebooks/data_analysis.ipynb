{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa5d12f9",
   "metadata": {},
   "source": [
    "# 中文大语言模型LoRA微调 - 数据分析与可视化\n",
    "\n",
    "本notebook提供了完整的数据分析流程，包括：\n",
    "- 📊 数据集探索与统计分析\n",
    "- 🔧 数据预处理与质量检查\n",
    "- 📈 训练过程监控与可视化\n",
    "- 🎯 模型性能评估与对比\n",
    "- 📋 生成详细的分析报告\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf9c6b",
   "metadata": {},
   "source": [
    "## 🛠️ 环境设置与依赖导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核心数据分析库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 可视化库\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 中文显示设置\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置样式\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"✅ 依赖库导入完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试导入NLP相关库\n",
    "try:\n",
    "    import jieba\n",
    "    from collections import Counter\n",
    "    from wordcloud import WordCloud\n",
    "    import nltk\n",
    "    # 下载必要的NLTK数据\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    NLP_AVAILABLE = True\n",
    "    print(\"✅ NLP库导入完成\")\n",
    "except ImportError as e:\n",
    "    NLP_AVAILABLE = False\n",
    "    print(f\"⚠️ NLP库未完全安装: {e}\")\n",
    "    print(\"请安装: pip install jieba wordcloud nltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f934ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置项目路径\n",
    "PROJECT_ROOT = Path('..')\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "CONFIG_DIR = PROJECT_ROOT / 'configs'\n",
    "\n",
    "# 创建输出目录\n",
    "OUTPUT_DIR = Path('./analysis_output')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"📁 项目根目录: {PROJECT_ROOT.absolute()}\")\n",
    "print(f\"📁 输出目录: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2fa60",
   "metadata": {},
   "source": [
    "## 📊 数据集探索与统计分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_file(file_path):\n",
    "    \"\"\"加载数据集文件\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"⚠️ 文件不存在: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if file_path.suffix == '.json':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        elif file_path.suffix == '.jsonl':\n",
    "            data = []\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        data.append(json.loads(line))\n",
    "        elif file_path.suffix == '.csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "            data = df.to_dict('records')\n",
    "        else:\n",
    "            print(f\"⚠️ 不支持的文件格式: {file_path.suffix}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"✅ 成功加载数据集: {file_path.name}\")\n",
    "        print(f\"📊 数据条数: {len(data)}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 加载数据集失败: {e}\")\n",
    "        return None\n",
    "\n",
    "# 查找可用的数据集文件\n",
    "data_files = []\n",
    "for pattern in ['*.json', '*.jsonl', '*.csv']:\n",
    "    data_files.extend(list(DATA_DIR.rglob(pattern)))\n",
    "\n",
    "print(f\"🔍 找到 {len(data_files)} 个数据文件:\")\n",
    "for i, file_path in enumerate(data_files[:10]):  # 只显示前10个\n",
    "    print(f\"  {i+1}. {file_path.relative_to(PROJECT_ROOT)}\")\n",
    "\n",
    "if len(data_files) > 10:\n",
    "    print(f\"  ... 还有 {len(data_files) - 10} 个文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载示例数据集（如果存在）\n",
    "sample_datasets = {}\n",
    "\n",
    "# 尝试加载一些常见的数据集\n",
    "common_files = [\n",
    "    DATA_DIR / 'raw' / 'belle.jsonl',\n",
    "    DATA_DIR / 'raw' / 'sample_dataset.jsonl',\n",
    "    DATA_DIR / 'processed' / 'train_belle.jsonl',\n",
    "]\n",
    "\n",
    "for file_path in common_files:\n",
    "    if file_path.exists():\n",
    "        data = load_dataset_from_file(file_path)\n",
    "        if data:\n",
    "            sample_datasets[file_path.stem] = data\n",
    "\n",
    "if not sample_datasets:\n",
    "    print(\"⚠️ 未找到数据集文件，创建示例数据...\")\n",
    "    # 创建示例数据\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"instruction\": \"请介绍一下中国的首都。\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"中国的首都是北京。北京是中华人民共和国的政治、文化中心，也是重要的国际都市。\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"解释什么是机器学习。\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"机器学习是人工智能的一个分支，它使计算机能够在没有明确编程的情况下学习和改进。\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"将以下文本翻译成英文。\",\n",
    "            \"input\": \"今天天气很好。\",\n",
    "            \"output\": \"The weather is very nice today.\"\n",
    "        }\n",
    "    ] * 100  # 重复100次创建示例数据\n",
    "    \n",
    "    sample_datasets['sample'] = sample_data\n",
    "    print(f\"✅ 创建了包含 {len(sample_data)} 条记录的示例数据\")\n",
    "\n",
    "print(f\"\\n📚 可用数据集: {list(sample_datasets.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_statistics(data, dataset_name):\n",
    "    \"\"\"分析数据集统计信息\"\"\"\n",
    "    print(f\"\\n📊 {dataset_name} 数据集统计分析\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 基本统计\n",
    "    total_samples = len(data)\n",
    "    print(f\"总样本数: {total_samples:,}\")\n",
    "    \n",
    "    # 字段分析\n",
    "    if data:\n",
    "        fields = list(data[0].keys())\n",
    "        print(f\"字段: {fields}\")\n",
    "        \n",
    "        # 分析文本长度\n",
    "        text_stats = {}\n",
    "        for field in ['instruction', 'input', 'output']:\n",
    "            if field in fields:\n",
    "                lengths = [len(str(item.get(field, ''))) for item in data]\n",
    "                text_stats[field] = {\n",
    "                    'mean': np.mean(lengths),\n",
    "                    'std': np.std(lengths),\n",
    "                    'min': np.min(lengths),\n",
    "                    'max': np.max(lengths),\n",
    "                    'median': np.median(lengths)\n",
    "                }\n",
    "        \n",
    "        # 打印文本长度统计\n",
    "        for field, stats in text_stats.items():\n",
    "            print(f\"\\n{field} 文本长度统计:\")\n",
    "            print(f\"  平均长度: {stats['mean']:.1f}\")\n",
    "            print(f\"  标准差: {stats['std']:.1f}\")\n",
    "            print(f\"  最小长度: {stats['min']}\")\n",
    "            print(f\"  最大长度: {stats['max']}\")\n",
    "            print(f\"  中位数: {stats['median']:.1f}\")\n",
    "        \n",
    "        return text_stats\n",
    "    \n",
    "    return {}\n",
    "\n",
    "# 分析所有数据集\n",
    "dataset_stats = {}\n",
    "for name, data in sample_datasets.items():\n",
    "    stats = analyze_dataset_statistics(data, name)\n",
    "    dataset_stats[name] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0c4a0",
   "metadata": {},
   "source": [
    "## 📈 数据可视化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034aaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_length_distribution_plot(data, dataset_name):\n",
    "    \"\"\"创建文本长度分布图\"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    \n",
    "    # 计算各字段的文本长度\n",
    "    length_data = {}\n",
    "    for field in ['instruction', 'input', 'output']:\n",
    "        if field in data[0]:\n",
    "            lengths = [len(str(item.get(field, ''))) for item in data]\n",
    "            length_data[field] = lengths\n",
    "    \n",
    "    if not length_data:\n",
    "        return\n",
    "    \n",
    "    # 创建子图\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'{dataset_name} 数据集文本长度分析', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 绘制各字段的长度分布\n",
    "    colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "    for i, (field, lengths) in enumerate(length_data.items()):\n",
    "        if i < 3:\n",
    "            ax = axes[i//2, i%2]\n",
    "            ax.hist(lengths, bins=30, alpha=0.7, color=colors[i], edgecolor='black')\n",
    "            ax.set_title(f'{field.capitalize()} 长度分布')\n",
    "            ax.set_xlabel('字符数')\n",
    "            ax.set_ylabel('频次')\n",
    "            ax.axvline(np.mean(lengths), color='red', linestyle='--', \n",
    "                      label=f'平均值: {np.mean(lengths):.1f}')\n",
    "            ax.legend()\n",
    "    \n",
    "    # 综合长度对比\n",
    "    ax = axes[1, 1]\n",
    "    for i, (field, lengths) in enumerate(length_data.items()):\n",
    "        ax.boxplot(lengths, positions=[i], widths=0.6, patch_artist=True,\n",
    "                  boxprops=dict(facecolor=colors[i], alpha=0.7))\n",
    "    \n",
    "    ax.set_xticklabels(list(length_data.keys()))\n",
    "    ax.set_title('各字段长度对比（箱线图）')\n",
    "    ax.set_ylabel('字符数')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图片\n",
    "    save_path = OUTPUT_DIR / f'{dataset_name}_length_distribution.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"📊 长度分布图已保存: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 为每个数据集创建长度分布图\n",
    "for name, data in sample_datasets.items():\n",
    "    create_length_distribution_plot(data, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_analysis(data, dataset_name):\n",
    "    \"\"\"创建交互式分析图表\"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    \n",
    "    # 准备数据\n",
    "    df_data = []\n",
    "    for i, item in enumerate(data[:1000]):  # 限制前1000条数据\n",
    "        row = {\n",
    "            'index': i,\n",
    "            'instruction_length': len(str(item.get('instruction', ''))),\n",
    "            'input_length': len(str(item.get('input', ''))),\n",
    "            'output_length': len(str(item.get('output', ''))),\n",
    "            'total_length': len(str(item.get('instruction', ''))) + \n",
    "                           len(str(item.get('input', ''))) + \n",
    "                           len(str(item.get('output', '')))\n",
    "        }\n",
    "        df_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(df_data)\n",
    "    \n",
    "    # 创建交互式散点图\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Instruction vs Output 长度关系',\n",
    "            '总长度分布',\n",
    "            '各字段长度对比',\n",
    "            '长度随索引变化'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 散点图：instruction vs output\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['instruction_length'],\n",
    "            y=df['output_length'],\n",
    "            mode='markers',\n",
    "            marker=dict(color='blue', alpha=0.6),\n",
    "            name='Instruction vs Output'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 直方图：总长度分布\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=df['total_length'],\n",
    "            nbinsx=30,\n",
    "            name='总长度分布',\n",
    "            marker_color='green'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 箱线图：各字段对比\n",
    "    for field in ['instruction_length', 'input_length', 'output_length']:\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=df[field],\n",
    "                name=field.replace('_length', ''),\n",
    "                boxpoints='outliers'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # 折线图：长度随索引变化\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['index'],\n",
    "            y=df['total_length'],\n",
    "            mode='lines',\n",
    "            name='总长度趋势',\n",
    "            line=dict(color='red')\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=f\"{dataset_name} 数据集交互式分析\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # 保存为HTML\n",
    "    save_path = OUTPUT_DIR / f'{dataset_name}_interactive_analysis.html'\n",
    "    fig.write_html(save_path)\n",
    "    print(f\"📊 交互式分析图已保存: {save_path}\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# 为第一个数据集创建交互式分析\n",
    "if sample_datasets:\n",
    "    first_dataset = list(sample_datasets.items())[0]\n",
    "    create_interactive_analysis(first_dataset[1], first_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de885c2b",
   "metadata": {},
   "source": [
    "## 🔍 文本内容分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_content(data, dataset_name):\n",
    "    \"\"\"分析文本内容特征\"\"\"\n",
    "    if not data or not NLP_AVAILABLE:\n",
    "        print(\"⚠️ 跳过文本内容分析（NLP库未安装或无数据）\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🔍 {dataset_name} 文本内容分析\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 收集所有文本\n",
    "    all_instructions = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    for item in data[:1000]:  # 限制分析前1000条\n",
    "        instruction = str(item.get('instruction', ''))\n",
    "        output = str(item.get('output', ''))\n",
    "        \n",
    "        if instruction:\n",
    "            all_instructions.append(instruction)\n",
    "        if output:\n",
    "            all_outputs.append(output)\n",
    "    \n",
    "    # 指令类型分析\n",
    "    instruction_types = Counter()\n",
    "    for instruction in all_instructions:\n",
    "        # 简单的指令类型分类\n",
    "        if any(word in instruction for word in ['翻译', 'translate']):\n",
    "            instruction_types['翻译'] += 1\n",
    "        elif any(word in instruction for word in ['解释', '介绍', '什么是']):\n",
    "            instruction_types['解释说明'] += 1\n",
    "        elif any(word in instruction for word in ['写', '创作', '编写']):\n",
    "            instruction_types['创作生成'] += 1\n",
    "        elif any(word in instruction for word in ['总结', '摘要']):\n",
    "            instruction_types['总结概括'] += 1\n",
    "        elif any(word in instruction for word in ['分析', '评价']):\n",
    "            instruction_types['分析评价'] += 1\n",
    "        else:\n",
    "            instruction_types['其他'] += 1\n",
    "    \n",
    "    print(\"\\n📋 指令类型分布:\")\n",
    "    for itype, count in instruction_types.most_common():\n",
    "        percentage = count / len(all_instructions) * 100\n",
    "        print(f\"  {itype}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 创建指令类型饼图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    labels = list(instruction_types.keys())\n",
    "    sizes = list(instruction_types.values())\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))\n",
    "    \n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('指令类型分布')\n",
    "    \n",
    "    # 常见词汇分析\n",
    "    all_text = ' '.join(all_instructions + all_outputs)\n",
    "    words = jieba.lcut(all_text)\n",
    "    \n",
    "    # 过滤停用词和标点\n",
    "    stop_words = {'的', '了', '是', '在', '有', '和', '与', '或', '及', '等', '、', '，', '。', '？', '！'}\n",
    "    words = [word for word in words if len(word) > 1 and word not in stop_words]\n",
    "    \n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    top_words = word_freq.most_common(10)\n",
    "    words_list, counts = zip(*top_words)\n",
    "    \n",
    "    plt.barh(range(len(words_list)), counts, color='skyblue')\n",
    "    plt.yticks(range(len(words_list)), words_list)\n",
    "    plt.xlabel('频次')\n",
    "    plt.title('高频词汇 Top 10')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图片\n",
    "    save_path = OUTPUT_DIR / f'{dataset_name}_content_analysis.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"📊 内容分析图已保存: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'instruction_types': instruction_types,\n",
    "        'word_frequency': word_freq.most_common(50)\n",
    "    }\n",
    "\n",
    "# 分析第一个数据集的文本内容\n",
    "if sample_datasets:\n",
    "    first_dataset = list(sample_datasets.items())[0]\n",
    "    content_analysis = analyze_text_content(first_dataset[1], first_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a508e2",
   "metadata": {},
   "source": [
    "## 📋 训练日志分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa900322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_training_logs():\n",
    "    \"\"\"分析训练日志\"\"\"\n",
    "    print(\"\\n📋 训练日志分析\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 查找训练日志文件\n",
    "    log_files = list(RESULTS_DIR.rglob('*.log')) + list(PROJECT_ROOT.rglob('training.log'))\n",
    "    \n",
    "    if not log_files:\n",
    "        print(\"⚠️ 未找到训练日志文件\")\n",
    "        # 创建示例训练数据\n",
    "        sample_training_data = {\n",
    "            'epochs': list(range(1, 4)),\n",
    "            'train_loss': [2.5, 1.8, 1.2],\n",
    "            'eval_loss': [2.3, 1.9, 1.4],\n",
    "            'learning_rate': [1e-4, 8e-5, 5e-5],\n",
    "            'steps': [100, 200, 300]\n",
    "        }\n",
    "        \n",
    "        # 创建训练过程可视化\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('训练过程监控（示例数据）', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 损失曲线\n",
    "        axes[0, 0].plot(sample_training_data['epochs'], sample_training_data['train_loss'], \n",
    "                       'o-', label='训练损失', color='blue')\n",
    "        axes[0, 0].plot(sample_training_data['epochs'], sample_training_data['eval_loss'], \n",
    "                       's-', label='验证损失', color='red')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('训练/验证损失曲线')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # 学习率变化\n",
    "        axes[0, 1].plot(sample_training_data['epochs'], sample_training_data['learning_rate'], \n",
    "                       'o-', color='green')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Learning Rate')\n",
    "        axes[0, 1].set_title('学习率变化')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # 损失改善幅度\n",
    "        train_improvement = [0] + [sample_training_data['train_loss'][i-1] - sample_training_data['train_loss'][i] \n",
    "                                 for i in range(1, len(sample_training_data['train_loss']))]\n",
    "        axes[1, 0].bar(sample_training_data['epochs'], train_improvement, \n",
    "                      color='lightblue', alpha=0.7)\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Loss Improvement')\n",
    "        axes[1, 0].set_title('每轮损失改善幅度')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # 训练步数累计\n",
    "        cumulative_steps = np.cumsum(sample_training_data['steps'])\n",
    "        axes[1, 1].plot(range(1, len(cumulative_steps)+1), cumulative_steps, \n",
    "                       'o-', color='purple')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Cumulative Steps')\n",
    "        axes[1, 1].set_title('累计训练步数')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 保存图片\n",
    "        save_path = OUTPUT_DIR / 'training_monitoring.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"📊 训练监控图已保存: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return sample_training_data\n",
    "    \n",
    "    else:\n",
    "        print(f\"✅ 找到 {len(log_files)} 个日志文件\")\n",
    "        for log_file in log_files:\n",
    "            print(f\"  - {log_file.relative_to(PROJECT_ROOT)}\")\n",
    "        \n",
    "        # TODO: 实际解析日志文件的逻辑\n",
    "        return None\n",
    "\n",
    "training_data = analyze_training_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04822d",
   "metadata": {},
   "source": [
    "## 🎯 模型评估结果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_evaluation_results():\n",
    "    \"\"\"分析模型评估结果\"\"\"\n",
    "    print(\"\\n🎯 模型评估结果分析\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 查找评估结果文件\n",
    "    eval_files = list(RESULTS_DIR.rglob('*evaluation*.json')) + \\\n",
    "                list(RESULTS_DIR.rglob('*eval*.json'))\n",
    "    \n",
    "    if not eval_files:\n",
    "        print(\"⚠️ 未找到评估结果文件，创建示例数据...\")\n",
    "        \n",
    "        # 示例评估数据\n",
    "        sample_eval_data = {\n",
    "            'models': ['ChatGLM3-LoRA', 'Qwen-LoRA', 'Baichuan2-LoRA'],\n",
    "            'metrics': {\n",
    "                'bleu': [0.234, 0.267, 0.198],\n",
    "                'rouge1': [0.456, 0.489, 0.423],\n",
    "                'rouge2': [0.234, 0.267, 0.198],\n",
    "                'rougeL': [0.389, 0.412, 0.356]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 创建评估结果对比图\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('模型评估结果对比（示例数据）', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        metrics = ['bleu', 'rouge1', 'rouge2', 'rougeL']\n",
    "        metric_names = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "        colors = ['skyblue', 'lightgreen', 'salmon', 'lightyellow']\n",
    "        \n",
    "        for i, (metric, name, color) in enumerate(zip(metrics, metric_names, colors)):\n",
    "            ax = axes[i//2, i%2]\n",
    "            \n",
    "            values = sample_eval_data['metrics'][metric]\n",
    "            models = sample_eval_data['models']\n",
    "            \n",
    "            bars = ax.bar(models, values, color=color, alpha=0.7, edgecolor='black')\n",
    "            ax.set_title(f'{name} 分数对比')\n",
    "            ax.set_ylabel('分数')\n",
    "            ax.set_ylim(0, max(values) * 1.2)\n",
    "            \n",
    "            # 添加数值标签\n",
    "            for bar, value in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # 标记最佳模型\n",
    "            best_idx = values.index(max(values))\n",
    "            bars[best_idx].set_color('gold')\n",
    "            bars[best_idx].set_edgecolor('red')\n",
    "            bars[best_idx].set_linewidth(2)\n",
    "            \n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # 保存图片\n",
    "        save_path = OUTPUT_DIR / 'model_evaluation_comparison.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"📊 模型评估对比图已保存: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # 创建雷达图\n",
    "        create_radar_chart(sample_eval_data)\n",
    "        \n",
    "        return sample_eval_data\n",
    "    \n",
    "    else:\n",
    "        print(f\"✅ 找到 {len(eval_files)} 个评估文件\")\n",
    "        # TODO: 实际加载和分析评估文件\n",
    "        return None\n",
    "\n",
    "def create_radar_chart(eval_data):\n",
    "    \"\"\"创建雷达图对比模型\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    metrics = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    \n",
    "    for i, model in enumerate(eval_data['models']):\n",
    "        values = [\n",
    "            eval_data['metrics']['bleu'][i],\n",
    "            eval_data['metrics']['rouge1'][i],\n",
    "            eval_data['metrics']['rouge2'][i],\n",
    "            eval_data['metrics']['rougeL'][i]\n",
    "        ]\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=metrics,\n",
    "            fill='toself',\n",
    "            name=model,\n",
    "            line_color=colors[i % len(colors)]\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 0.6]\n",
    "            )),\n",
    "        title=\"模型性能雷达图对比\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # 保存为HTML\n",
    "    save_path = OUTPUT_DIR / 'model_radar_chart.html'\n",
    "    fig.write_html(save_path)\n",
    "    print(f\"📊 模型雷达图已保存: {save_path}\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "eval_results = analyze_evaluation_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb488e42",
   "metadata": {},
   "source": [
    "## 📄 生成分析报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f87c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis_report():\n",
    "    \"\"\"生成完整的分析报告\"\"\"\n",
    "    print(\"\\n📄 生成分析报告\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    report_content = f\"\"\"\n",
    "# 中文大语言模型LoRA微调 - 数据分析报告\n",
    "\n",
    "**生成时间**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**分析版本**: v1.0\n",
    "\n",
    "## 📊 数据集概览\n",
    "\n",
    "本次分析共处理了 {len(sample_datasets)} 个数据集:\n",
    "\"\"\"\n",
    "    \n",
    "    # 添加数据集统计信息\n",
    "    for name, data in sample_datasets.items():\n",
    "        report_content += f\"\"\"\n",
    "### {name} 数据集\n",
    "- **样本数量**: {len(data):,}\n",
    "- **字段**: {list(data[0].keys()) if data else '无'}\n",
    "\"\"\"\n",
    "        \n",
    "        if name in dataset_stats and dataset_stats[name]:\n",
    "            for field, stats in dataset_stats[name].items():\n",
    "                report_content += f\"\"\"\n",
    "- **{field} 统计**:\n",
    "  - 平均长度: {stats['mean']:.1f} 字符\n",
    "  - 最大长度: {stats['max']} 字符\n",
    "  - 最小长度: {stats['min']} 字符\n",
    "\"\"\"\n",
    "    \n",
    "    # 添加训练分析\n",
    "    if training_data:\n",
    "        report_content += f\"\"\"\n",
    "## 📈 训练过程分析\n",
    "\n",
    "- **训练轮数**: {len(training_data['epochs'])}\n",
    "- **最终训练损失**: {training_data['train_loss'][-1]:.3f}\n",
    "- **最终验证损失**: {training_data['eval_loss'][-1]:.3f}\n",
    "- **损失降幅**: {((training_data['train_loss'][0] - training_data['train_loss'][-1]) / training_data['train_loss'][0] * 100):.1f}%\n",
    "\"\"\"\n",
    "    \n",
    "    # 添加评估结果\n",
    "    if eval_results:\n",
    "        report_content += f\"\"\"\n",
    "## 🎯 模型评估结果\n",
    "\n",
    "### 最佳性能模型\n",
    "\"\"\"\n",
    "        for metric in ['bleu', 'rouge1', 'rougeL']:\n",
    "            best_idx = eval_results['metrics'][metric].index(max(eval_results['metrics'][metric]))\n",
    "            best_model = eval_results['models'][best_idx]\n",
    "            best_score = eval_results['metrics'][metric][best_idx]\n",
    "            \n",
    "            report_content += f\"\"\"\n",
    "- **{metric.upper()}最佳**: {best_model} ({best_score:.3f})\n",
    "\"\"\"\n",
    "    \n",
    "    # 添加结论和建议\n",
    "    report_content += f\"\"\"\n",
    "## 💡 分析结论与建议\n",
    "\n",
    "### 数据质量\n",
    "- 数据集规模适中，适合LoRA微调\n",
    "- 文本长度分布合理，符合模型输入要求\n",
    "- 建议进一步清洗数据，提高质量\n",
    "\n",
    "### 训练效果\n",
    "- 训练损失呈现良好的下降趋势\n",
    "- 验证损失与训练损失差距合理，无明显过拟合\n",
    "- 建议调整学习率策略，进一步优化\n",
    "\n",
    "### 模型性能\n",
    "- 各模型在不同指标上表现有差异\n",
    "- ChatGLM3-LoRA 在综合性能上表现较好\n",
    "- 建议进行更多轮次的训练和超参数调优\n",
    "\n",
    "## 📁 生成文件清单\n",
    "\n",
    "本次分析生成的文件包括:\n",
    "- 数据长度分布图\n",
    "- 交互式分析图表\n",
    "- 文本内容分析图\n",
    "- 训练过程监控图\n",
    "- 模型评估对比图\n",
    "- 模型性能雷达图\n",
    "- 本分析报告\n",
    "\n",
    "---\n",
    "*报告由中文大语言模型LoRA微调框架自动生成*\n",
    "\"\"\"\n",
    "    \n",
    "    # 保存报告\n",
    "    report_path = OUTPUT_DIR / 'analysis_report.md'\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_content)\n",
    "    \n",
    "    print(f\"📄 分析报告已生成: {report_path}\")\n",
    "    \n",
    "    # 显示报告摘要\n",
    "    print(\"\\n📋 报告摘要:\")\n",
    "    print(f\"  - 分析数据集: {len(sample_datasets)} 个\")\n",
    "    print(f\"  - 生成图表: 约 6-8 个\")\n",
    "    print(f\"  - 输出目录: {OUTPUT_DIR.absolute()}\")\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "report_path = generate_analysis_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a34b95c",
   "metadata": {},
   "source": [
    "## 🎉 分析完成总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总结分析结果\n",
    "print(\"\\n🎉 数据分析完成！\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 统计生成的文件\n",
    "output_files = list(OUTPUT_DIR.glob('*'))\n",
    "print(f\"\\n📁 生成文件数量: {len(output_files)}\")\n",
    "print(\"\\n📋 文件清单:\")\n",
    "\n",
    "for file_path in sorted(output_files):\n",
    "    file_size = file_path.stat().st_size / 1024  # KB\n",
    "    print(f\"  📄 {file_path.name} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(f\"\\n📍 所有文件保存在: {OUTPUT_DIR.absolute()}\")\n",
    "print(\"\\n💡 使用建议:\")\n",
    "print(\"  1. 查看生成的图表了解数据特征\")\n",
    "print(\"  2. 阅读分析报告获取详细信息\")\n",
    "print(\"  3. 根据分析结果调整训练策略\")\n",
    "print(\"  4. 定期运行此notebook监控训练进展\")\n",
    "\n",
    "print(\"\\n🚀 祝您的中文大语言模型LoRA微调项目成功！\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
