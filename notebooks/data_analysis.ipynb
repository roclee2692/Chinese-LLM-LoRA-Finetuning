{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa5d12f9",
   "metadata": {},
   "source": [
    "# ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹LoRAå¾®è°ƒ - æ•°æ®åˆ†æä¸å¯è§†åŒ–\n",
    "\n",
    "æœ¬notebookæä¾›äº†å®Œæ•´çš„æ•°æ®åˆ†ææµç¨‹ï¼ŒåŒ…æ‹¬ï¼š\n",
    "- ğŸ“Š æ•°æ®é›†æ¢ç´¢ä¸ç»Ÿè®¡åˆ†æ\n",
    "- ğŸ”§ æ•°æ®é¢„å¤„ç†ä¸è´¨é‡æ£€æŸ¥\n",
    "- ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹ç›‘æ§ä¸å¯è§†åŒ–\n",
    "- ğŸ¯ æ¨¡å‹æ€§èƒ½è¯„ä¼°ä¸å¯¹æ¯”\n",
    "- ğŸ“‹ ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdf9c6b",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ ç¯å¢ƒè®¾ç½®ä¸ä¾èµ–å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¸å¿ƒæ•°æ®åˆ†æåº“\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å¯è§†åŒ–åº“\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ä¸­æ–‡æ˜¾ç¤ºè®¾ç½®\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®æ ·å¼\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"âœ… ä¾èµ–åº“å¯¼å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°è¯•å¯¼å…¥NLPç›¸å…³åº“\n",
    "try:\n",
    "    import jieba\n",
    "    from collections import Counter\n",
    "    from wordcloud import WordCloud\n",
    "    import nltk\n",
    "    # ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    NLP_AVAILABLE = True\n",
    "    print(\"âœ… NLPåº“å¯¼å…¥å®Œæˆ\")\n",
    "except ImportError as e:\n",
    "    NLP_AVAILABLE = False\n",
    "    print(f\"âš ï¸ NLPåº“æœªå®Œå…¨å®‰è£…: {e}\")\n",
    "    print(\"è¯·å®‰è£…: pip install jieba wordcloud nltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f934ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®é¡¹ç›®è·¯å¾„\n",
    "PROJECT_ROOT = Path('..')\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "CONFIG_DIR = PROJECT_ROOT / 'configs'\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "OUTPUT_DIR = Path('./analysis_output')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT.absolute()}\")\n",
    "print(f\"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2fa60",
   "metadata": {},
   "source": [
    "## ğŸ“Š æ•°æ®é›†æ¢ç´¢ä¸ç»Ÿè®¡åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_file(file_path):\n",
    "    \"\"\"åŠ è½½æ•°æ®é›†æ–‡ä»¶\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if file_path.suffix == '.json':\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        elif file_path.suffix == '.jsonl':\n",
    "            data = []\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        data.append(json.loads(line))\n",
    "        elif file_path.suffix == '.csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "            data = df.to_dict('records')\n",
    "        else:\n",
    "            print(f\"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†: {file_path.name}\")\n",
    "        print(f\"ğŸ“Š æ•°æ®æ¡æ•°: {len(data)}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "# æŸ¥æ‰¾å¯ç”¨çš„æ•°æ®é›†æ–‡ä»¶\n",
    "data_files = []\n",
    "for pattern in ['*.json', '*.jsonl', '*.csv']:\n",
    "    data_files.extend(list(DATA_DIR.rglob(pattern)))\n",
    "\n",
    "print(f\"ğŸ” æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶:\")\n",
    "for i, file_path in enumerate(data_files[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª\n",
    "    print(f\"  {i+1}. {file_path.relative_to(PROJECT_ROOT)}\")\n",
    "\n",
    "if len(data_files) > 10:\n",
    "    print(f\"  ... è¿˜æœ‰ {len(data_files) - 10} ä¸ªæ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½ç¤ºä¾‹æ•°æ®é›†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
    "sample_datasets = {}\n",
    "\n",
    "# å°è¯•åŠ è½½ä¸€äº›å¸¸è§çš„æ•°æ®é›†\n",
    "common_files = [\n",
    "    DATA_DIR / 'raw' / 'belle.jsonl',\n",
    "    DATA_DIR / 'raw' / 'sample_dataset.jsonl',\n",
    "    DATA_DIR / 'processed' / 'train_belle.jsonl',\n",
    "]\n",
    "\n",
    "for file_path in common_files:\n",
    "    if file_path.exists():\n",
    "        data = load_dataset_from_file(file_path)\n",
    "        if data:\n",
    "            sample_datasets[file_path.stem] = data\n",
    "\n",
    "if not sample_datasets:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®...\")\n",
    "    # åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"instruction\": \"è¯·ä»‹ç»ä¸€ä¸‹ä¸­å›½çš„é¦–éƒ½ã€‚\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚åŒ—äº¬æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„æ”¿æ²»ã€æ–‡åŒ–ä¸­å¿ƒï¼Œä¹Ÿæ˜¯é‡è¦çš„å›½é™…éƒ½å¸‚ã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ã€‚\",\n",
    "            \"input\": \"ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚\",\n",
    "            \"output\": \"The weather is very nice today.\"\n",
    "        }\n",
    "    ] * 100  # é‡å¤100æ¬¡åˆ›å»ºç¤ºä¾‹æ•°æ®\n",
    "    \n",
    "    sample_datasets['sample'] = sample_data\n",
    "    print(f\"âœ… åˆ›å»ºäº†åŒ…å« {len(sample_data)} æ¡è®°å½•çš„ç¤ºä¾‹æ•°æ®\")\n",
    "\n",
    "print(f\"\\nğŸ“š å¯ç”¨æ•°æ®é›†: {list(sample_datasets.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_statistics(data, dataset_name):\n",
    "    \"\"\"åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "    print(f\"\\nğŸ“Š {dataset_name} æ•°æ®é›†ç»Ÿè®¡åˆ†æ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # åŸºæœ¬ç»Ÿè®¡\n",
    "    total_samples = len(data)\n",
    "    print(f\"æ€»æ ·æœ¬æ•°: {total_samples:,}\")\n",
    "    \n",
    "    # å­—æ®µåˆ†æ\n",
    "    if data:\n",
    "        fields = list(data[0].keys())\n",
    "        print(f\"å­—æ®µ: {fields}\")\n",
    "        \n",
    "        # åˆ†ææ–‡æœ¬é•¿åº¦\n",
    "        text_stats = {}\n",
    "        for field in ['instruction', 'input', 'output']:\n",
    "            if field in fields:\n",
    "                lengths = [len(str(item.get(field, ''))) for item in data]\n",
    "                text_stats[field] = {\n",
    "                    'mean': np.mean(lengths),\n",
    "                    'std': np.std(lengths),\n",
    "                    'min': np.min(lengths),\n",
    "                    'max': np.max(lengths),\n",
    "                    'median': np.median(lengths)\n",
    "                }\n",
    "        \n",
    "        # æ‰“å°æ–‡æœ¬é•¿åº¦ç»Ÿè®¡\n",
    "        for field, stats in text_stats.items():\n",
    "            print(f\"\\n{field} æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:\")\n",
    "            print(f\"  å¹³å‡é•¿åº¦: {stats['mean']:.1f}\")\n",
    "            print(f\"  æ ‡å‡†å·®: {stats['std']:.1f}\")\n",
    "            print(f\"  æœ€å°é•¿åº¦: {stats['min']}\")\n",
    "            print(f\"  æœ€å¤§é•¿åº¦: {stats['max']}\")\n",
    "            print(f\"  ä¸­ä½æ•°: {stats['median']:.1f}\")\n",
    "        \n",
    "        return text_stats\n",
    "    \n",
    "    return {}\n",
    "\n",
    "# åˆ†ææ‰€æœ‰æ•°æ®é›†\n",
    "dataset_stats = {}\n",
    "for name, data in sample_datasets.items():\n",
    "    stats = analyze_dataset_statistics(data, name)\n",
    "    dataset_stats[name] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0c4a0",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ æ•°æ®å¯è§†åŒ–åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034aaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_length_distribution_plot(data, dataset_name):\n",
    "    \"\"\"åˆ›å»ºæ–‡æœ¬é•¿åº¦åˆ†å¸ƒå›¾\"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    \n",
    "    # è®¡ç®—å„å­—æ®µçš„æ–‡æœ¬é•¿åº¦\n",
    "    length_data = {}\n",
    "    for field in ['instruction', 'input', 'output']:\n",
    "        if field in data[0]:\n",
    "            lengths = [len(str(item.get(field, ''))) for item in data]\n",
    "            length_data[field] = lengths\n",
    "    \n",
    "    if not length_data:\n",
    "        return\n",
    "    \n",
    "    # åˆ›å»ºå­å›¾\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'{dataset_name} æ•°æ®é›†æ–‡æœ¬é•¿åº¦åˆ†æ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # ç»˜åˆ¶å„å­—æ®µçš„é•¿åº¦åˆ†å¸ƒ\n",
    "    colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "    for i, (field, lengths) in enumerate(length_data.items()):\n",
    "        if i < 3:\n",
    "            ax = axes[i//2, i%2]\n",
    "            ax.hist(lengths, bins=30, alpha=0.7, color=colors[i], edgecolor='black')\n",
    "            ax.set_title(f'{field.capitalize()} é•¿åº¦åˆ†å¸ƒ')\n",
    "            ax.set_xlabel('å­—ç¬¦æ•°')\n",
    "            ax.set_ylabel('é¢‘æ¬¡')\n",
    "            ax.axvline(np.mean(lengths), color='red', linestyle='--', \n",
    "                      label=f'å¹³å‡å€¼: {np.mean(lengths):.1f}')\n",
    "            ax.legend()\n",
    "    \n",
    "    # ç»¼åˆé•¿åº¦å¯¹æ¯”\n",
    "    ax = axes[1, 1]\n",
    "    for i, (field, lengths) in enumerate(length_data.items()):\n",
    "        ax.boxplot(lengths, positions=[i], widths=0.6, patch_artist=True,\n",
    "                  boxprops=dict(facecolor=colors[i], alpha=0.7))\n",
    "    \n",
    "    ax.set_xticklabels(list(length_data.keys()))\n",
    "    ax.set_title('å„å­—æ®µé•¿åº¦å¯¹æ¯”ï¼ˆç®±çº¿å›¾ï¼‰')\n",
    "    ax.set_ylabel('å­—ç¬¦æ•°')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜å›¾ç‰‡\n",
    "    save_path = OUTPUT_DIR / f'{dataset_name}_length_distribution.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š é•¿åº¦åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºé•¿åº¦åˆ†å¸ƒå›¾\n",
    "for name, data in sample_datasets.items():\n",
    "    create_length_distribution_plot(data, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_analysis(data, dataset_name):\n",
    "    \"\"\"åˆ›å»ºäº¤äº’å¼åˆ†æå›¾è¡¨\"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    df_data = []\n",
    "    for i, item in enumerate(data[:1000]):  # é™åˆ¶å‰1000æ¡æ•°æ®\n",
    "        row = {\n",
    "            'index': i,\n",
    "            'instruction_length': len(str(item.get('instruction', ''))),\n",
    "            'input_length': len(str(item.get('input', ''))),\n",
    "            'output_length': len(str(item.get('output', ''))),\n",
    "            'total_length': len(str(item.get('instruction', ''))) + \n",
    "                           len(str(item.get('input', ''))) + \n",
    "                           len(str(item.get('output', '')))\n",
    "        }\n",
    "        df_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(df_data)\n",
    "    \n",
    "    # åˆ›å»ºäº¤äº’å¼æ•£ç‚¹å›¾\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Instruction vs Output é•¿åº¦å…³ç³»',\n",
    "            'æ€»é•¿åº¦åˆ†å¸ƒ',\n",
    "            'å„å­—æ®µé•¿åº¦å¯¹æ¯”',\n",
    "            'é•¿åº¦éšç´¢å¼•å˜åŒ–'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # æ•£ç‚¹å›¾ï¼šinstruction vs output\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['instruction_length'],\n",
    "            y=df['output_length'],\n",
    "            mode='markers',\n",
    "            marker=dict(color='blue', alpha=0.6),\n",
    "            name='Instruction vs Output'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # ç›´æ–¹å›¾ï¼šæ€»é•¿åº¦åˆ†å¸ƒ\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=df['total_length'],\n",
    "            nbinsx=30,\n",
    "            name='æ€»é•¿åº¦åˆ†å¸ƒ',\n",
    "            marker_color='green'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # ç®±çº¿å›¾ï¼šå„å­—æ®µå¯¹æ¯”\n",
    "    for field in ['instruction_length', 'input_length', 'output_length']:\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=df[field],\n",
    "                name=field.replace('_length', ''),\n",
    "                boxpoints='outliers'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # æŠ˜çº¿å›¾ï¼šé•¿åº¦éšç´¢å¼•å˜åŒ–\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['index'],\n",
    "            y=df['total_length'],\n",
    "            mode='lines',\n",
    "            name='æ€»é•¿åº¦è¶‹åŠ¿',\n",
    "            line=dict(color='red')\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=f\"{dataset_name} æ•°æ®é›†äº¤äº’å¼åˆ†æ\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # ä¿å­˜ä¸ºHTML\n",
    "    save_path = OUTPUT_DIR / f'{dataset_name}_interactive_analysis.html'\n",
    "    fig.write_html(save_path)\n",
    "    print(f\"ğŸ“Š äº¤äº’å¼åˆ†æå›¾å·²ä¿å­˜: {save_path}\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# ä¸ºç¬¬ä¸€ä¸ªæ•°æ®é›†åˆ›å»ºäº¤äº’å¼åˆ†æ\n",
    "if sample_datasets:\n",
    "    first_dataset = list(sample_datasets.items())[0]\n",
    "    create_interactive_analysis(first_dataset[1], first_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de885c2b",
   "metadata": {},
   "source": [
    "## ğŸ” æ–‡æœ¬å†…å®¹åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_content(data, dataset_name):\n",
    "    \"\"\"åˆ†ææ–‡æœ¬å†…å®¹ç‰¹å¾\"\"\"\n",
    "    if not data or not NLP_AVAILABLE:\n",
    "        print(\"âš ï¸ è·³è¿‡æ–‡æœ¬å†…å®¹åˆ†æï¼ˆNLPåº“æœªå®‰è£…æˆ–æ— æ•°æ®ï¼‰\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ” {dataset_name} æ–‡æœ¬å†…å®¹åˆ†æ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æ”¶é›†æ‰€æœ‰æ–‡æœ¬\n",
    "    all_instructions = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    for item in data[:1000]:  # é™åˆ¶åˆ†æå‰1000æ¡\n",
    "        instruction = str(item.get('instruction', ''))\n",
    "        output = str(item.get('output', ''))\n",
    "        \n",
    "        if instruction:\n",
    "            all_instructions.append(instruction)\n",
    "        if output:\n",
    "            all_outputs.append(output)\n",
    "    \n",
    "    # æŒ‡ä»¤ç±»å‹åˆ†æ\n",
    "    instruction_types = Counter()\n",
    "    for instruction in all_instructions:\n",
    "        # ç®€å•çš„æŒ‡ä»¤ç±»å‹åˆ†ç±»\n",
    "        if any(word in instruction for word in ['ç¿»è¯‘', 'translate']):\n",
    "            instruction_types['ç¿»è¯‘'] += 1\n",
    "        elif any(word in instruction for word in ['è§£é‡Š', 'ä»‹ç»', 'ä»€ä¹ˆæ˜¯']):\n",
    "            instruction_types['è§£é‡Šè¯´æ˜'] += 1\n",
    "        elif any(word in instruction for word in ['å†™', 'åˆ›ä½œ', 'ç¼–å†™']):\n",
    "            instruction_types['åˆ›ä½œç”Ÿæˆ'] += 1\n",
    "        elif any(word in instruction for word in ['æ€»ç»“', 'æ‘˜è¦']):\n",
    "            instruction_types['æ€»ç»“æ¦‚æ‹¬'] += 1\n",
    "        elif any(word in instruction for word in ['åˆ†æ', 'è¯„ä»·']):\n",
    "            instruction_types['åˆ†æè¯„ä»·'] += 1\n",
    "        else:\n",
    "            instruction_types['å…¶ä»–'] += 1\n",
    "    \n",
    "    print(\"\\nğŸ“‹ æŒ‡ä»¤ç±»å‹åˆ†å¸ƒ:\")\n",
    "    for itype, count in instruction_types.most_common():\n",
    "        percentage = count / len(all_instructions) * 100\n",
    "        print(f\"  {itype}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # åˆ›å»ºæŒ‡ä»¤ç±»å‹é¥¼å›¾\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    labels = list(instruction_types.keys())\n",
    "    sizes = list(instruction_types.values())\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))\n",
    "    \n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('æŒ‡ä»¤ç±»å‹åˆ†å¸ƒ')\n",
    "    \n",
    "    # å¸¸è§è¯æ±‡åˆ†æ\n",
    "    all_text = ' '.join(all_instructions + all_outputs)\n",
    "    words = jieba.lcut(all_text)\n",
    "    \n",
    "    # è¿‡æ»¤åœç”¨è¯å’Œæ ‡ç‚¹\n",
    "    stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'æˆ–', 'åŠ', 'ç­‰', 'ã€', 'ï¼Œ', 'ã€‚', 'ï¼Ÿ', 'ï¼'}\n",
    "    words = [word for word in words if len(word) > 1 and word not in stop_words]\n",
    "    \n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    top_words = word_freq.most_common(10)\n",
    "    words_list, counts = zip(*top_words)\n",
    "    \n",
    "    plt.barh(range(len(words_list)), counts, color='skyblue')\n",
    "    plt.yticks(range(len(words_list)), words_list)\n",
    "    plt.xlabel('é¢‘æ¬¡')\n",
    "    plt.title('é«˜é¢‘è¯æ±‡ Top 10')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜å›¾ç‰‡\n",
    "    save_path = OUTPUT_DIR / f'{dataset_name}_content_analysis.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š å†…å®¹åˆ†æå›¾å·²ä¿å­˜: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'instruction_types': instruction_types,\n",
    "        'word_frequency': word_freq.most_common(50)\n",
    "    }\n",
    "\n",
    "# åˆ†æç¬¬ä¸€ä¸ªæ•°æ®é›†çš„æ–‡æœ¬å†…å®¹\n",
    "if sample_datasets:\n",
    "    first_dataset = list(sample_datasets.items())[0]\n",
    "    content_analysis = analyze_text_content(first_dataset[1], first_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a508e2",
   "metadata": {},
   "source": [
    "## ğŸ“‹ è®­ç»ƒæ—¥å¿—åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa900322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_training_logs():\n",
    "    \"\"\"åˆ†æè®­ç»ƒæ—¥å¿—\"\"\"\n",
    "    print(\"\\nğŸ“‹ è®­ç»ƒæ—¥å¿—åˆ†æ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æŸ¥æ‰¾è®­ç»ƒæ—¥å¿—æ–‡ä»¶\n",
    "    log_files = list(RESULTS_DIR.rglob('*.log')) + list(PROJECT_ROOT.rglob('training.log'))\n",
    "    \n",
    "    if not log_files:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒæ—¥å¿—æ–‡ä»¶\")\n",
    "        # åˆ›å»ºç¤ºä¾‹è®­ç»ƒæ•°æ®\n",
    "        sample_training_data = {\n",
    "            'epochs': list(range(1, 4)),\n",
    "            'train_loss': [2.5, 1.8, 1.2],\n",
    "            'eval_loss': [2.3, 1.9, 1.4],\n",
    "            'learning_rate': [1e-4, 8e-5, 5e-5],\n",
    "            'steps': [100, 200, 300]\n",
    "        }\n",
    "        \n",
    "        # åˆ›å»ºè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('è®­ç»ƒè¿‡ç¨‹ç›‘æ§ï¼ˆç¤ºä¾‹æ•°æ®ï¼‰', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # æŸå¤±æ›²çº¿\n",
    "        axes[0, 0].plot(sample_training_data['epochs'], sample_training_data['train_loss'], \n",
    "                       'o-', label='è®­ç»ƒæŸå¤±', color='blue')\n",
    "        axes[0, 0].plot(sample_training_data['epochs'], sample_training_data['eval_loss'], \n",
    "                       's-', label='éªŒè¯æŸå¤±', color='red')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # å­¦ä¹ ç‡å˜åŒ–\n",
    "        axes[0, 1].plot(sample_training_data['epochs'], sample_training_data['learning_rate'], \n",
    "                       'o-', color='green')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Learning Rate')\n",
    "        axes[0, 1].set_title('å­¦ä¹ ç‡å˜åŒ–')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # æŸå¤±æ”¹å–„å¹…åº¦\n",
    "        train_improvement = [0] + [sample_training_data['train_loss'][i-1] - sample_training_data['train_loss'][i] \n",
    "                                 for i in range(1, len(sample_training_data['train_loss']))]\n",
    "        axes[1, 0].bar(sample_training_data['epochs'], train_improvement, \n",
    "                      color='lightblue', alpha=0.7)\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Loss Improvement')\n",
    "        axes[1, 0].set_title('æ¯è½®æŸå¤±æ”¹å–„å¹…åº¦')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # è®­ç»ƒæ­¥æ•°ç´¯è®¡\n",
    "        cumulative_steps = np.cumsum(sample_training_data['steps'])\n",
    "        axes[1, 1].plot(range(1, len(cumulative_steps)+1), cumulative_steps, \n",
    "                       'o-', color='purple')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Cumulative Steps')\n",
    "        axes[1, 1].set_title('ç´¯è®¡è®­ç»ƒæ­¥æ•°')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜å›¾ç‰‡\n",
    "        save_path = OUTPUT_DIR / 'training_monitoring.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ“Š è®­ç»ƒç›‘æ§å›¾å·²ä¿å­˜: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return sample_training_data\n",
    "    \n",
    "    else:\n",
    "        print(f\"âœ… æ‰¾åˆ° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶\")\n",
    "        for log_file in log_files:\n",
    "            print(f\"  - {log_file.relative_to(PROJECT_ROOT)}\")\n",
    "        \n",
    "        # TODO: å®é™…è§£ææ—¥å¿—æ–‡ä»¶çš„é€»è¾‘\n",
    "        return None\n",
    "\n",
    "training_data = analyze_training_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04822d",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ¨¡å‹è¯„ä¼°ç»“æœåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_evaluation_results():\n",
    "    \"\"\"åˆ†ææ¨¡å‹è¯„ä¼°ç»“æœ\"\"\"\n",
    "    print(\"\\nğŸ¯ æ¨¡å‹è¯„ä¼°ç»“æœåˆ†æ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æŸ¥æ‰¾è¯„ä¼°ç»“æœæ–‡ä»¶\n",
    "    eval_files = list(RESULTS_DIR.rglob('*evaluation*.json')) + \\\n",
    "                list(RESULTS_DIR.rglob('*eval*.json'))\n",
    "    \n",
    "    if not eval_files:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶ï¼Œåˆ›å»ºç¤ºä¾‹æ•°æ®...\")\n",
    "        \n",
    "        # ç¤ºä¾‹è¯„ä¼°æ•°æ®\n",
    "        sample_eval_data = {\n",
    "            'models': ['ChatGLM3-LoRA', 'Qwen-LoRA', 'Baichuan2-LoRA'],\n",
    "            'metrics': {\n",
    "                'bleu': [0.234, 0.267, 0.198],\n",
    "                'rouge1': [0.456, 0.489, 0.423],\n",
    "                'rouge2': [0.234, 0.267, 0.198],\n",
    "                'rougeL': [0.389, 0.412, 0.356]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # åˆ›å»ºè¯„ä¼°ç»“æœå¯¹æ¯”å›¾\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('æ¨¡å‹è¯„ä¼°ç»“æœå¯¹æ¯”ï¼ˆç¤ºä¾‹æ•°æ®ï¼‰', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        metrics = ['bleu', 'rouge1', 'rouge2', 'rougeL']\n",
    "        metric_names = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "        colors = ['skyblue', 'lightgreen', 'salmon', 'lightyellow']\n",
    "        \n",
    "        for i, (metric, name, color) in enumerate(zip(metrics, metric_names, colors)):\n",
    "            ax = axes[i//2, i%2]\n",
    "            \n",
    "            values = sample_eval_data['metrics'][metric]\n",
    "            models = sample_eval_data['models']\n",
    "            \n",
    "            bars = ax.bar(models, values, color=color, alpha=0.7, edgecolor='black')\n",
    "            ax.set_title(f'{name} åˆ†æ•°å¯¹æ¯”')\n",
    "            ax.set_ylabel('åˆ†æ•°')\n",
    "            ax.set_ylim(0, max(values) * 1.2)\n",
    "            \n",
    "            # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "            for bar, value in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # æ ‡è®°æœ€ä½³æ¨¡å‹\n",
    "            best_idx = values.index(max(values))\n",
    "            bars[best_idx].set_color('gold')\n",
    "            bars[best_idx].set_edgecolor('red')\n",
    "            bars[best_idx].set_linewidth(2)\n",
    "            \n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # ä¿å­˜å›¾ç‰‡\n",
    "        save_path = OUTPUT_DIR / 'model_evaluation_comparison.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ“Š æ¨¡å‹è¯„ä¼°å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # åˆ›å»ºé›·è¾¾å›¾\n",
    "        create_radar_chart(sample_eval_data)\n",
    "        \n",
    "        return sample_eval_data\n",
    "    \n",
    "    else:\n",
    "        print(f\"âœ… æ‰¾åˆ° {len(eval_files)} ä¸ªè¯„ä¼°æ–‡ä»¶\")\n",
    "        # TODO: å®é™…åŠ è½½å’Œåˆ†æè¯„ä¼°æ–‡ä»¶\n",
    "        return None\n",
    "\n",
    "def create_radar_chart(eval_data):\n",
    "    \"\"\"åˆ›å»ºé›·è¾¾å›¾å¯¹æ¯”æ¨¡å‹\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    metrics = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    \n",
    "    for i, model in enumerate(eval_data['models']):\n",
    "        values = [\n",
    "            eval_data['metrics']['bleu'][i],\n",
    "            eval_data['metrics']['rouge1'][i],\n",
    "            eval_data['metrics']['rouge2'][i],\n",
    "            eval_data['metrics']['rougeL'][i]\n",
    "        ]\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=metrics,\n",
    "            fill='toself',\n",
    "            name=model,\n",
    "            line_color=colors[i % len(colors)]\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 0.6]\n",
    "            )),\n",
    "        title=\"æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾å¯¹æ¯”\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # ä¿å­˜ä¸ºHTML\n",
    "    save_path = OUTPUT_DIR / 'model_radar_chart.html'\n",
    "    fig.write_html(save_path)\n",
    "    print(f\"ğŸ“Š æ¨¡å‹é›·è¾¾å›¾å·²ä¿å­˜: {save_path}\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "eval_results = analyze_evaluation_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb488e42",
   "metadata": {},
   "source": [
    "## ğŸ“„ ç”Ÿæˆåˆ†ææŠ¥å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f87c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis_report():\n",
    "    \"\"\"ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š\"\"\"\n",
    "    print(\"\\nğŸ“„ ç”Ÿæˆåˆ†ææŠ¥å‘Š\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    report_content = f\"\"\"\n",
    "# ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹LoRAå¾®è°ƒ - æ•°æ®åˆ†ææŠ¥å‘Š\n",
    "\n",
    "**ç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**åˆ†æç‰ˆæœ¬**: v1.0\n",
    "\n",
    "## ğŸ“Š æ•°æ®é›†æ¦‚è§ˆ\n",
    "\n",
    "æœ¬æ¬¡åˆ†æå…±å¤„ç†äº† {len(sample_datasets)} ä¸ªæ•°æ®é›†:\n",
    "\"\"\"\n",
    "    \n",
    "    # æ·»åŠ æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯\n",
    "    for name, data in sample_datasets.items():\n",
    "        report_content += f\"\"\"\n",
    "### {name} æ•°æ®é›†\n",
    "- **æ ·æœ¬æ•°é‡**: {len(data):,}\n",
    "- **å­—æ®µ**: {list(data[0].keys()) if data else 'æ— '}\n",
    "\"\"\"\n",
    "        \n",
    "        if name in dataset_stats and dataset_stats[name]:\n",
    "            for field, stats in dataset_stats[name].items():\n",
    "                report_content += f\"\"\"\n",
    "- **{field} ç»Ÿè®¡**:\n",
    "  - å¹³å‡é•¿åº¦: {stats['mean']:.1f} å­—ç¬¦\n",
    "  - æœ€å¤§é•¿åº¦: {stats['max']} å­—ç¬¦\n",
    "  - æœ€å°é•¿åº¦: {stats['min']} å­—ç¬¦\n",
    "\"\"\"\n",
    "    \n",
    "    # æ·»åŠ è®­ç»ƒåˆ†æ\n",
    "    if training_data:\n",
    "        report_content += f\"\"\"\n",
    "## ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹åˆ†æ\n",
    "\n",
    "- **è®­ç»ƒè½®æ•°**: {len(training_data['epochs'])}\n",
    "- **æœ€ç»ˆè®­ç»ƒæŸå¤±**: {training_data['train_loss'][-1]:.3f}\n",
    "- **æœ€ç»ˆéªŒè¯æŸå¤±**: {training_data['eval_loss'][-1]:.3f}\n",
    "- **æŸå¤±é™å¹…**: {((training_data['train_loss'][0] - training_data['train_loss'][-1]) / training_data['train_loss'][0] * 100):.1f}%\n",
    "\"\"\"\n",
    "    \n",
    "    # æ·»åŠ è¯„ä¼°ç»“æœ\n",
    "    if eval_results:\n",
    "        report_content += f\"\"\"\n",
    "## ğŸ¯ æ¨¡å‹è¯„ä¼°ç»“æœ\n",
    "\n",
    "### æœ€ä½³æ€§èƒ½æ¨¡å‹\n",
    "\"\"\"\n",
    "        for metric in ['bleu', 'rouge1', 'rougeL']:\n",
    "            best_idx = eval_results['metrics'][metric].index(max(eval_results['metrics'][metric]))\n",
    "            best_model = eval_results['models'][best_idx]\n",
    "            best_score = eval_results['metrics'][metric][best_idx]\n",
    "            \n",
    "            report_content += f\"\"\"\n",
    "- **{metric.upper()}æœ€ä½³**: {best_model} ({best_score:.3f})\n",
    "\"\"\"\n",
    "    \n",
    "    # æ·»åŠ ç»“è®ºå’Œå»ºè®®\n",
    "    report_content += f\"\"\"\n",
    "## ğŸ’¡ åˆ†æç»“è®ºä¸å»ºè®®\n",
    "\n",
    "### æ•°æ®è´¨é‡\n",
    "- æ•°æ®é›†è§„æ¨¡é€‚ä¸­ï¼Œé€‚åˆLoRAå¾®è°ƒ\n",
    "- æ–‡æœ¬é•¿åº¦åˆ†å¸ƒåˆç†ï¼Œç¬¦åˆæ¨¡å‹è¾“å…¥è¦æ±‚\n",
    "- å»ºè®®è¿›ä¸€æ­¥æ¸…æ´—æ•°æ®ï¼Œæé«˜è´¨é‡\n",
    "\n",
    "### è®­ç»ƒæ•ˆæœ\n",
    "- è®­ç»ƒæŸå¤±å‘ˆç°è‰¯å¥½çš„ä¸‹é™è¶‹åŠ¿\n",
    "- éªŒè¯æŸå¤±ä¸è®­ç»ƒæŸå¤±å·®è·åˆç†ï¼Œæ— æ˜æ˜¾è¿‡æ‹Ÿåˆ\n",
    "- å»ºè®®è°ƒæ•´å­¦ä¹ ç‡ç­–ç•¥ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–\n",
    "\n",
    "### æ¨¡å‹æ€§èƒ½\n",
    "- å„æ¨¡å‹åœ¨ä¸åŒæŒ‡æ ‡ä¸Šè¡¨ç°æœ‰å·®å¼‚\n",
    "- ChatGLM3-LoRA åœ¨ç»¼åˆæ€§èƒ½ä¸Šè¡¨ç°è¾ƒå¥½\n",
    "- å»ºè®®è¿›è¡Œæ›´å¤šè½®æ¬¡çš„è®­ç»ƒå’Œè¶…å‚æ•°è°ƒä¼˜\n",
    "\n",
    "## ğŸ“ ç”Ÿæˆæ–‡ä»¶æ¸…å•\n",
    "\n",
    "æœ¬æ¬¡åˆ†æç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬:\n",
    "- æ•°æ®é•¿åº¦åˆ†å¸ƒå›¾\n",
    "- äº¤äº’å¼åˆ†æå›¾è¡¨\n",
    "- æ–‡æœ¬å†…å®¹åˆ†æå›¾\n",
    "- è®­ç»ƒè¿‡ç¨‹ç›‘æ§å›¾\n",
    "- æ¨¡å‹è¯„ä¼°å¯¹æ¯”å›¾\n",
    "- æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾\n",
    "- æœ¬åˆ†ææŠ¥å‘Š\n",
    "\n",
    "---\n",
    "*æŠ¥å‘Šç”±ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹LoRAå¾®è°ƒæ¡†æ¶è‡ªåŠ¨ç”Ÿæˆ*\n",
    "\"\"\"\n",
    "    \n",
    "    # ä¿å­˜æŠ¥å‘Š\n",
    "    report_path = OUTPUT_DIR / 'analysis_report.md'\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_content)\n",
    "    \n",
    "    print(f\"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦\n",
    "    print(\"\\nğŸ“‹ æŠ¥å‘Šæ‘˜è¦:\")\n",
    "    print(f\"  - åˆ†ææ•°æ®é›†: {len(sample_datasets)} ä¸ª\")\n",
    "    print(f\"  - ç”Ÿæˆå›¾è¡¨: çº¦ 6-8 ä¸ª\")\n",
    "    print(f\"  - è¾“å‡ºç›®å½•: {OUTPUT_DIR.absolute()}\")\n",
    "    \n",
    "    return report_path\n",
    "\n",
    "report_path = generate_analysis_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a34b95c",
   "metadata": {},
   "source": [
    "## ğŸ‰ åˆ†æå®Œæˆæ€»ç»“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€»ç»“åˆ†æç»“æœ\n",
    "print(\"\\nğŸ‰ æ•°æ®åˆ†æå®Œæˆï¼\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ç»Ÿè®¡ç”Ÿæˆçš„æ–‡ä»¶\n",
    "output_files = list(OUTPUT_DIR.glob('*'))\n",
    "print(f\"\\nğŸ“ ç”Ÿæˆæ–‡ä»¶æ•°é‡: {len(output_files)}\")\n",
    "print(\"\\nğŸ“‹ æ–‡ä»¶æ¸…å•:\")\n",
    "\n",
    "for file_path in sorted(output_files):\n",
    "    file_size = file_path.stat().st_size / 1024  # KB\n",
    "    print(f\"  ğŸ“„ {file_path.name} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(f\"\\nğŸ“ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {OUTPUT_DIR.absolute()}\")\n",
    "print(\"\\nğŸ’¡ ä½¿ç”¨å»ºè®®:\")\n",
    "print(\"  1. æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨äº†è§£æ•°æ®ç‰¹å¾\")\n",
    "print(\"  2. é˜…è¯»åˆ†ææŠ¥å‘Šè·å–è¯¦ç»†ä¿¡æ¯\")\n",
    "print(\"  3. æ ¹æ®åˆ†æç»“æœè°ƒæ•´è®­ç»ƒç­–ç•¥\")\n",
    "print(\"  4. å®šæœŸè¿è¡Œæ­¤notebookç›‘æ§è®­ç»ƒè¿›å±•\")\n",
    "\n",
    "print(\"\\nğŸš€ ç¥æ‚¨çš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹LoRAå¾®è°ƒé¡¹ç›®æˆåŠŸï¼\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
