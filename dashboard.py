#!/usr/bin/env python3
"""
Chinese LLM LoRA è®­ç»ƒç»¼åˆçŠ¶æ€ä»ªè¡¨æ¿
æä¾›è®­ç»ƒçŠ¶æ€æ¦‚è§ˆã€ç³»ç»Ÿç›‘æ§å’Œæ“ä½œå»ºè®®
"""

import json
import subprocess
import psutil
from pathlib import Path
import datetime
import time

class TrainingDashboard:
    """è®­ç»ƒçŠ¶æ€ä»ªè¡¨æ¿"""
    
    def __init__(self):
        self.project_root = Path(".")
        self.results_dir = self.project_root / "results"
        self.models_dir = self.results_dir / "models"
        self.cache_dir = self.project_root / "cache"
        
    def get_emoji_status(self, condition):
        """æ ¹æ®æ¡ä»¶è¿”å›è¡¨æƒ…ç¬¦å·"""
        return "âœ…" if condition else "âŒ"
    
    def get_gpu_info(self):
        """è·å–GPUä¿¡æ¯"""
        try:
            result = subprocess.run([
                'nvidia-smi', '--query-gpu=name,memory.total,memory.used,memory.free,utilization.gpu,temperature.gpu',
                '--format=csv,noheader,nounits'
            ], capture_output=True, text=True, timeout=5)
            
            if result.returncode == 0:
                gpu_data = result.stdout.strip().split(', ')
                return {
                    "name": gpu_data[0],
                    "memory_total": int(gpu_data[1]),
                    "memory_used": int(gpu_data[2]),
                    "memory_free": int(gpu_data[3]),
                    "utilization": int(gpu_data[4]),
                    "temperature": int(gpu_data[5])
                }
        except:
            pass
        return None
    
    def check_models_status(self):
        """æ£€æŸ¥æ¨¡å‹çŠ¶æ€"""
        status = {
            "qwen_downloaded": False,
            "qwen_size": 0,
            "distilgpt2_downloaded": False,
            "chatglm3_downloaded": False,
            "completed_trainings": [],
            "total_models": 0
        }
        
        # æ£€æŸ¥ç¼“å­˜çš„æ¨¡å‹
        if (self.cache_dir / "models--Qwen--Qwen-1_8B-Chat").exists():
            status["qwen_downloaded"] = True
            # è®¡ç®—æ¨¡å‹å¤§å°
            qwen_path = self.cache_dir / "models--Qwen--Qwen-1_8B-Chat"
            try:
                total_size = sum(f.stat().st_size for f in qwen_path.rglob('*') if f.is_file())
                status["qwen_size"] = total_size / (1024**3)  # GB
            except:
                pass
        
        if (self.cache_dir / "models--distilgpt2").exists():
            status["distilgpt2_downloaded"] = True
            
        if (self.cache_dir / "models--THUDM--chatglm3-6b").exists():
            status["chatglm3_downloaded"] = True
        
        # æ£€æŸ¥è®­ç»ƒç»“æœ
        if self.models_dir.exists():
            for model_dir in self.models_dir.glob("*"):
                if model_dir.is_dir() and model_dir.name != ".gitkeep":
                    status["total_models"] += 1
                    trainer_state_file = model_dir / "trainer_state.json"
                    if trainer_state_file.exists():
                        try:
                            with open(trainer_state_file, 'r') as f:
                                trainer_state = json.load(f)
                            
                            model_info = {
                                "name": model_dir.name,
                                "global_step": trainer_state.get('global_step', 0),
                                "max_steps": trainer_state.get('max_steps', 0),
                                "epoch": trainer_state.get('epoch', 0),
                                "is_completed": trainer_state.get('global_step', 0) >= trainer_state.get('max_steps', 1),
                                "last_modified": trainer_state_file.stat().st_mtime
                            }
                            status["completed_trainings"].append(model_info)
                        except:
                            pass
        
        return status
    
    def get_system_resources(self):
        """è·å–ç³»ç»Ÿèµ„æºä¿¡æ¯"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('.')
        
        return {
            "cpu_percent": cpu_percent,
            "memory_total": memory.total / (1024**3),
            "memory_used": memory.used / (1024**3),
            "memory_percent": memory.percent,
            "disk_total": disk.total / (1024**3),
            "disk_used": disk.used / (1024**3),
            "disk_percent": (disk.used / disk.total) * 100
        }
    
    def check_training_readiness(self):
        """æ£€æŸ¥è®­ç»ƒå‡†å¤‡çŠ¶æ€"""
        checks = {
            "gpu_available": False,
            "cuda_available": False,
            "pytorch_installed": False,
            "transformers_installed": False,
            "peft_installed": False,
            "qwen_model_ready": False,
            "disk_space_ok": False,
            "memory_sufficient": False
        }
        
        # GPUæ£€æŸ¥
        gpu_info = self.get_gpu_info()
        checks["gpu_available"] = gpu_info is not None
        
        # CUDAæ£€æŸ¥
        try:
            import torch
            checks["cuda_available"] = torch.cuda.is_available()
            checks["pytorch_installed"] = True
        except ImportError:
            pass
        
        # åº“æ£€æŸ¥
        try:
            import transformers
            checks["transformers_installed"] = True
        except ImportError:
            pass
        
        try:
            import peft
            checks["peft_installed"] = True
        except ImportError:
            pass
        
        # æ¨¡å‹æ£€æŸ¥
        models_status = self.check_models_status()
        checks["qwen_model_ready"] = models_status["qwen_downloaded"]
        
        # èµ„æºæ£€æŸ¥
        system = self.get_system_resources()
        checks["disk_space_ok"] = system["disk_percent"] < 90
        checks["memory_sufficient"] = system["memory_total"] >= 16  # è‡³å°‘16GBå†…å­˜
        
        return checks
    
    def display_dashboard(self):
        """æ˜¾ç¤ºå®Œæ•´ä»ªè¡¨æ¿"""
        print("=" * 80)
        print("ğŸš€ Chinese LLM LoRA Fine-tuning Dashboard")
        print("=" * 80)
        print(f"ğŸ“… æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # è®­ç»ƒå‡†å¤‡çŠ¶æ€
        print("\nğŸ“‹ è®­ç»ƒå‡†å¤‡çŠ¶æ€æ£€æŸ¥:")
        print("-" * 40)
        readiness = self.check_training_readiness()
        
        print(f"{self.get_emoji_status(readiness['gpu_available'])} GPUå¯ç”¨")
        print(f"{self.get_emoji_status(readiness['cuda_available'])} CUDAå¯ç”¨")
        print(f"{self.get_emoji_status(readiness['pytorch_installed'])} PyTorchå·²å®‰è£…")
        print(f"{self.get_emoji_status(readiness['transformers_installed'])} Transformerså·²å®‰è£…")
        print(f"{self.get_emoji_status(readiness['peft_installed'])} PEFTå·²å®‰è£…")
        print(f"{self.get_emoji_status(readiness['qwen_model_ready'])} Qwenæ¨¡å‹å·²ä¸‹è½½")
        print(f"{self.get_emoji_status(readiness['disk_space_ok'])} ç£ç›˜ç©ºé—´å……è¶³")
        print(f"{self.get_emoji_status(readiness['memory_sufficient'])} å†…å­˜å……è¶³")
        
        ready_count = sum(readiness.values())
        total_checks = len(readiness)
        print(f"\nğŸ“Š å‡†å¤‡å°±ç»ª: {ready_count}/{total_checks} ({ready_count/total_checks*100:.1f}%)")
        
        # GPUçŠ¶æ€
        print("\nğŸ® GPUçŠ¶æ€:")
        print("-" * 40)
        gpu_info = self.get_gpu_info()
        if gpu_info:
            print(f"ğŸ’» å‹å·: {gpu_info['name']}")
            print(f"ğŸ’¾ æ˜¾å­˜: {gpu_info['memory_used']}MB / {gpu_info['memory_total']}MB ({gpu_info['memory_used']/gpu_info['memory_total']*100:.1f}%)")
            print(f"âš¡ ä½¿ç”¨ç‡: {gpu_info['utilization']}%")
            print(f"ğŸŒ¡ï¸  æ¸©åº¦: {gpu_info['temperature']}Â°C")
            
            # GPUçŠ¶æ€æŒ‡ç¤º
            if gpu_info['utilization'] > 80:
                gpu_status = "ğŸ”¥ é«˜è´Ÿè½½è¿è¡Œ"
            elif gpu_info['utilization'] > 30:
                gpu_status = "âš¡ ä½¿ç”¨ä¸­"
            else:
                gpu_status = "ğŸ˜´ ç©ºé—²"
            print(f"ğŸ“Š çŠ¶æ€: {gpu_status}")
        else:
            print("âŒ æ— æ³•è·å–GPUä¿¡æ¯")
        
        # ç³»ç»Ÿèµ„æº
        print("\nğŸ’» ç³»ç»Ÿèµ„æº:")
        print("-" * 40)
        system = self.get_system_resources()
        print(f"ğŸ§  CPUä½¿ç”¨ç‡: {system['cpu_percent']:.1f}%")
        print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨: {system['memory_used']:.1f}GB / {system['memory_total']:.1f}GB ({system['memory_percent']:.1f}%)")
        print(f"ğŸ’¿ ç£ç›˜ä½¿ç”¨: {system['disk_used']:.1f}GB / {system['disk_total']:.1f}GB ({system['disk_percent']:.1f}%)")
        
        # æ¨¡å‹çŠ¶æ€
        print("\nğŸ¤– æ¨¡å‹çŠ¶æ€:")
        print("-" * 40)
        models_status = self.check_models_status()
        
        print(f"{self.get_emoji_status(models_status['qwen_downloaded'])} Qwen-1.8B-Chat: ", end="")
        if models_status['qwen_downloaded']:
            print(f"å·²ä¸‹è½½ ({models_status['qwen_size']:.1f}GB)")
        else:
            print("æœªä¸‹è½½")
        
        print(f"{self.get_emoji_status(models_status['distilgpt2_downloaded'])} DistilGPT2: ", end="")
        print("å·²ä¸‹è½½" if models_status['distilgpt2_downloaded'] else "æœªä¸‹è½½")
        
        print(f"{self.get_emoji_status(models_status['chatglm3_downloaded'])} ChatGLM3-6B: ", end="")
        print("å·²ä¸‹è½½" if models_status['chatglm3_downloaded'] else "æœªä¸‹è½½")
        
        # è®­ç»ƒå†å²
        print("\nğŸ“š è®­ç»ƒå†å²:")
        print("-" * 40)
        if models_status['completed_trainings']:
            for training in models_status['completed_trainings']:
                completion = training['global_step'] / max(training['max_steps'], 1) * 100
                last_modified = datetime.datetime.fromtimestamp(training['last_modified'])
                age = datetime.datetime.now() - last_modified
                
                status_icon = "âœ…" if training['is_completed'] else "ğŸ”„"
                print(f"{status_icon} {training['name']}: {completion:.1f}% ({training['global_step']}/{training['max_steps']} æ­¥)")
                print(f"    ğŸ“… æœ€åæ›´æ–°: {last_modified.strftime('%Y-%m-%d %H:%M:%S')} ({age.days}å¤©å‰)")
        else:
            print("ğŸ“­ æš‚æ— è®­ç»ƒè®°å½•")
        
        # æ“ä½œå»ºè®®
        print("\nğŸ’¡ æ“ä½œå»ºè®®:")
        print("-" * 40)
        
        if ready_count == total_checks:
            print("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒ")
            print("ğŸ“ è¿è¡Œå‘½ä»¤: python start_qwen_training.py")
            print("ğŸ” ç›‘æ§è®­ç»ƒ: python simple_monitor.py")
        else:
            print("âš ï¸ å­˜åœ¨é—®é¢˜éœ€è¦è§£å†³:")
            if not readiness['gpu_available']:
                print("   â€¢ æ£€æŸ¥GPUé©±åŠ¨å®‰è£…")
            if not readiness['cuda_available']:
                print("   â€¢ å®‰è£…CUDAæˆ–æ£€æŸ¥PyTorch CUDAç‰ˆæœ¬")
            if not readiness['qwen_model_ready']:
                print("   â€¢ ä¸‹è½½Qwenæ¨¡å‹: è¿è¡Œ python download_models.py")
            if not readiness['memory_sufficient']:
                print("   â€¢ å»ºè®®è‡³å°‘16GBå†…å­˜ç”¨äºå¤§æ¨¡å‹è®­ç»ƒ")
        
        # å¿«æ·æ“ä½œ
        print("\nğŸ”§ å¿«æ·æ“ä½œ:")
        print("-" * 40)
        print("1. ğŸ‹ï¸  å¼€å§‹Qwenè®­ç»ƒ: python start_qwen_training.py")
        print("2. ğŸ“Š æŸ¥çœ‹è®­ç»ƒçŠ¶æ€: python simple_monitor.py")
        print("3. ğŸ§ª å¿«é€Ÿæµ‹è¯•è®­ç»ƒ: python quick_training_test.py")
        print("4. ğŸ” ç³»ç»Ÿè¯Šæ–­: python verify_installation.py")
        
        print("\n" + "=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    dashboard = TrainingDashboard()
    dashboard.display_dashboard()

if __name__ == "__main__":
    main()