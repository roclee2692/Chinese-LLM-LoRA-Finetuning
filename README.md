# Chinese-LLM-LoRA-Finetuning

<div align="center">

ä¸€ä¸ªåŸºäº LoRA çš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ¡†æ¶

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

</div>

## ä¸­æ–‡

### ğŸš€ ç‰¹æ€§

- ğŸ¯ **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒ ChatGLM3ã€Qwenã€Baichuan2ã€Yi ç­‰ä¸»æµä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹
- âš¡ **é«˜æ•ˆå¾®è°ƒ**: åŸºäº LoRA çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œæ”¯æŒ QLoRA é‡åŒ–è®­ç»ƒ
- ğŸ“Š **å®Œæ•´è¯„ä¼°**: å†…ç½® BLEUã€ROUGE ç­‰å¤šç§è¯„ä¼°æŒ‡æ ‡
- ğŸŒ **Webç•Œé¢**: åŸºäº Gradio çš„äº¤äº’å¼æ¼”ç¤ºå’Œæ¨¡å‹æ¯”è¾ƒç•Œé¢
- ğŸ“ˆ **å®éªŒè·Ÿè¸ª**: é›†æˆ Weights & Biases è¿›è¡Œå®éªŒç®¡ç†
- ğŸ³ **å®¹å™¨åŒ–éƒ¨ç½²**: æä¾› Docker æ”¯æŒï¼Œä¸€é”®éƒ¨ç½²
- ğŸ”§ **ä¸­æ–‡ä¼˜åŒ–**: ä¸“é—¨é’ˆå¯¹ä¸­æ–‡æ•°æ®é›†å’Œä»»åŠ¡è¿›è¡Œä¼˜åŒ–

### ğŸ“¦ å®‰è£…

#### ç¯å¢ƒè¦æ±‚
- Python 3.8+
- CUDA 11.8+
- PyTorch 2.0+

#### å¿«é€Ÿå®‰è£…
```bash
git clone https://github.com/your-username/Chinese-LLM-LoRA-Finetuning.git
cd Chinese-LLM-LoRA-Finetuning
pip install -r requirements.txt
```

### ğŸ› ï¸ ä½¿ç”¨æ–¹æ³•

#### 1. æ•°æ®å‡†å¤‡
```bash
# ä¸‹è½½å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®
python scripts/download_data.py
```

#### 2. é…ç½®æ¨¡å‹
ç¼–è¾‘ `configs/` ç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶ï¼Œä¾‹å¦‚ `chatglm3_lora.yaml`ï¼š
```yaml
model_name: "THUDM/chatglm3-6b"
lora_rank: 8
lora_alpha: 32
target_modules: ["query_key_value"]
```

#### 3. å¼€å§‹è®­ç»ƒ
```bash
# Windows
scripts\run_training.bat

# Linux/Mac
bash scripts/run_training.sh

# æˆ–ç›´æ¥ä½¿ç”¨ Python
python src/train.py --config configs/chatglm3_lora.yaml
```

#### 4. æ¨¡å‹è¯„ä¼°
```bash
python src/evaluate.py --model_path results/models/chatglm3-lora --test_file data/processed/test.json
```

#### 5. å¯åŠ¨ Web æ¼”ç¤º
```bash
python demo/gradio_demo.py
```

### ğŸ“ é¡¹ç›®ç»“æ„
```
Chinese-LLM-LoRA-Finetuning/
â”œâ”€â”€ src/                    # æ ¸å¿ƒæºä»£ç 
â”‚   â”œâ”€â”€ data_preprocessing.py   # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ train.py               # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ evaluate.py            # è¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ inference.py           # æ¨ç†è„šæœ¬
â”‚   â””â”€â”€ utils.py              # å·¥å…·å‡½æ•°
â”œâ”€â”€ configs/               # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ model_config.yaml     # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ training_config.yaml  # è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ chatglm3_lora.yaml   # ChatGLM3 LoRAé…ç½®
â”œâ”€â”€ scripts/               # è„šæœ¬å’Œå·¥å…·
â”‚   â”œâ”€â”€ download_data.py      # æ•°æ®ä¸‹è½½
â”‚   â”œâ”€â”€ run_training.sh       # è®­ç»ƒè„šæœ¬(Linux)
â”‚   â”œâ”€â”€ run_training.bat      # è®­ç»ƒè„šæœ¬(Windows)
â”‚   â””â”€â”€ generate_report.py    # æŠ¥å‘Šç”Ÿæˆ
â”œâ”€â”€ demo/                  # Webæ¼”ç¤ºç•Œé¢
â”‚   â””â”€â”€ gradio_demo.py       # Gradioæ¼”ç¤ºåº”ç”¨
â”œâ”€â”€ notebooks/             # æ•°æ®åˆ†æç¬”è®°æœ¬
â”‚   â””â”€â”€ data_analysis.ipynb  # æ•°æ®æ¢ç´¢åˆ†æ
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/              # åŸå§‹æ•°æ®
â”‚   â””â”€â”€ processed/        # å¤„ç†åæ•°æ®
â””â”€â”€ results/              # è®­ç»ƒç»“æœ
    â”œâ”€â”€ models/           # ä¿å­˜çš„æ¨¡å‹
    â”œâ”€â”€ logs/             # è®­ç»ƒæ—¥å¿—
    â””â”€â”€ evaluation/       # è¯„ä¼°ç»“æœ
```

### ğŸ”§ é«˜çº§é…ç½®

#### LoRA å‚æ•°è°ƒä¼˜
```yaml
# configs/custom_lora.yaml
lora_config:
  r: 8                    # LoRA rank
  lora_alpha: 32         # LoRA scaling parameter
  target_modules:        # ç›®æ ‡æ¨¡å—
    - "query_key_value"
    - "dense"
  lora_dropout: 0.1      # LoRA dropout
  bias: "none"           # åç½®è®¾ç½®
```

#### é‡åŒ–è®­ç»ƒ
```yaml
# å¯ç”¨ QLoRA 4-bit é‡åŒ–
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true
```

### ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | BLEU-4 | ROUGE-L |
|------|--------|----------|--------|---------|
| ChatGLM3-6B (Full) | 6B | 24h | 23.5 | 45.2 |
| ChatGLM3-6B (LoRA) | 6B+4M | 6h | 22.8 | 44.1 |
| Qwen-7B (LoRA) | 7B+5M | 7h | 24.1 | 46.3 |
| Baichuan2-7B (LoRA) | 7B+5M | 7h | 23.2 | 45.0 |

### ğŸ³ Docker éƒ¨ç½²

#### æ„å»ºé•œåƒ
```bash
docker build -t chinese-llm-lora .
```

#### è¿è¡Œå®¹å™¨
```bash
docker run -p 7860:7860 -v $(pwd)/data:/app/data chinese-llm-lora
```

### ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚

### ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä½¿ç”¨ MIT è®¸å¯è¯ã€‚è¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

### ğŸ™ è‡´è°¢

- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [PEFT](https://github.com/huggingface/peft)
- [ChatGLM](https://github.com/THUDM/ChatGLM-6B)
- [Qwen](https://github.com/QwenLM/Qwen)

---

## English

### ğŸš€ Features

- ğŸ¯ **Multi-Model Support**: Support for mainstream Chinese LLMs like ChatGLM3, Qwen, Baichuan2, Yi
- âš¡ **Efficient Fine-tuning**: LoRA-based parameter-efficient fine-tuning with QLoRA quantization
- ğŸ“Š **Comprehensive Evaluation**: Built-in metrics including BLEU, ROUGE, and more
- ğŸŒ **Web Interface**: Interactive Gradio demo with model comparison
- ğŸ“ˆ **Experiment Tracking**: Weights & Biases integration
- ğŸ³ **Containerized Deployment**: Docker support for easy deployment
- ğŸ”§ **Chinese Optimization**: Specifically optimized for Chinese datasets and tasks

### ğŸ“¦ Installation

#### Requirements
- Python 3.8+
- CUDA 11.8+
- PyTorch 2.0+

#### Quick Install
```bash
git clone https://github.com/your-username/Chinese-LLM-LoRA-Finetuning.git
cd Chinese-LLM-LoRA-Finetuning
pip install -r requirements.txt
```

### ğŸ› ï¸ Usage

#### 1. Data Preparation
```bash
python scripts/download_data.py
```

#### 2. Model Configuration
Edit configuration files in `configs/` directory, e.g., `chatglm3_lora.yaml`

#### 3. Start Training
```bash
python src/train.py --config configs/chatglm3_lora.yaml
```

#### 4. Model Evaluation
```bash
python src/evaluate.py --model_path results/models/chatglm3-lora --test_file data/processed/test.json
```

#### 5. Launch Web Demo
```bash
python demo/gradio_demo.py
```

### ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details.

### ğŸ“„ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.
