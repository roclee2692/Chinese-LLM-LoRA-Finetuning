#!/usr/bin/env python3
"""
ç®€åŒ–çš„Webæ¼”ç¤ºç•Œé¢
æ— éœ€é¢„è®­ç»ƒæ¨¡å‹ï¼Œå±•ç¤ºæ¡†æ¶åŠŸèƒ½
"""

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
import json

class SimpleDemo:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.model_loaded = False
        
    def load_model(self, model_name="distilgpt2"):
        """åŠ è½½è½»é‡çº§æ¨¡å‹è¿›è¡Œæ¼”ç¤º"""
        try:
            print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None
            )
            
            self.model_loaded = True
            return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼"
        except Exception as e:
            return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
    
    def generate_text(self, prompt, max_length=100, temperature=0.7, top_p=0.9):
        """æ–‡æœ¬ç”Ÿæˆ"""
        if not self.model_loaded:
            return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹"
        
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs["input_ids"],
                    max_length=max_length,
                    temperature=temperature,
                    top_p=top_p,
                    num_return_sequences=1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    do_sample=True
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return generated_text
            
        except Exception as e:
            return f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def get_system_info(self):
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        info = f"""
ğŸ–¥ï¸ **ç³»ç»Ÿä¿¡æ¯**
- Python: {torch.__version__ if torch else "æœªå®‰è£…"}
- PyTorch: {torch.__version__ if torch else "æœªå®‰è£…"}
- CUDAå¯ç”¨: {torch.cuda.is_available() if torch else "æœªçŸ¥"}
- è®¾å¤‡: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"}

ğŸ“ **é¡¹ç›®çŠ¶æ€**
- æ•°æ®å·²é¢„å¤„ç†: âœ…
- é…ç½®æ–‡ä»¶å®Œæ•´: âœ…
- è®­ç»ƒç¯å¢ƒå°±ç»ª: âœ…
- Webç•Œé¢è¿è¡Œ: âœ…

ğŸ¯ **åŠŸèƒ½å±•ç¤º**
è¿™æ˜¯ä¸­æ–‡LLM LoRAå¾®è°ƒæ¡†æ¶çš„æ¼”ç¤ºç•Œé¢ã€‚
æ‚¨å¯ä»¥ï¼š
1. åŠ è½½è½»é‡çº§æ¨¡å‹è¿›è¡Œæµ‹è¯•
2. ä½“éªŒæ–‡æœ¬ç”ŸæˆåŠŸèƒ½
3. æŸ¥çœ‹æ¡†æ¶å®Œæ•´æ€§

ğŸ“‹ **ä¸‹ä¸€æ­¥**
- è¿è¡Œå®Œæ•´è®­ç»ƒ: `python src/train.py --config configs/quick_test.yaml`
- åŠ è½½è®­ç»ƒåçš„æ¨¡å‹è¿›è¡Œæ›´å¥½çš„ä¸­æ–‡å¯¹è¯
        """
        return info

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    demo_instance = SimpleDemo()
    
    with gr.Blocks(
        title="ä¸­æ–‡LLM LoRAå¾®è°ƒæ¡†æ¶æ¼”ç¤º",
        theme=gr.themes.Soft()
    ) as interface:
        
        gr.Markdown("# ğŸš€ ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹LoRAå¾®è°ƒæ¡†æ¶")
        gr.Markdown("*A Comprehensive LoRA Fine-tuning Framework for Chinese LLMs*")
        
        with gr.Tab("ğŸ“Š ç³»ç»ŸçŠ¶æ€"):
            info_output = gr.Markdown(demo_instance.get_system_info())
            gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€").click(
                lambda: demo_instance.get_system_info(),
                outputs=info_output
            )
        
        with gr.Tab("ğŸ¤– æ¨¡å‹æ¼”ç¤º"):
            with gr.Row():
                with gr.Column():
                    load_btn = gr.Button("ğŸ“¦ åŠ è½½æ¼”ç¤ºæ¨¡å‹", variant="primary")
                    load_status = gr.Textbox(label="åŠ è½½çŠ¶æ€", interactive=False)
                    
                    load_btn.click(
                        demo_instance.load_model,
                        outputs=load_status
                    )
            
            with gr.Row():
                with gr.Column():
                    prompt_input = gr.Textbox(
                        label="è¾“å…¥æç¤ºæ–‡æœ¬",
                        placeholder="è¯·è¾“å…¥æ‚¨æƒ³è¦çš„æ–‡æœ¬æç¤º...",
                        lines=3
                    )
                    
                    with gr.Row():
                        max_length = gr.Slider(50, 200, 100, label="æœ€å¤§é•¿åº¦")
                        temperature = gr.Slider(0.1, 1.0, 0.7, label="æ¸©åº¦")
                        top_p = gr.Slider(0.1, 1.0, 0.9, label="Top-p")
                    
                    generate_btn = gr.Button("âœ¨ ç”Ÿæˆæ–‡æœ¬", variant="secondary")
                
                with gr.Column():
                    output_text = gr.Textbox(
                        label="ç”Ÿæˆç»“æœ",
                        lines=8,
                        interactive=False
                    )
            
            generate_btn.click(
                demo_instance.generate_text,
                inputs=[prompt_input, max_length, temperature, top_p],
                outputs=output_text
            )
        
        with gr.Tab("ğŸ“– ä½¿ç”¨æŒ‡å—"):
            gr.Markdown("""
## ğŸ¯ æ¡†æ¶ç‰¹æ€§

### âœ¨ æ ¸å¿ƒåŠŸèƒ½
- **å¤šæ¨¡å‹æ”¯æŒ**: ChatGLM3, Qwen, Baichuan2, Yi
- **é«˜æ•ˆè®­ç»ƒ**: LoRAå‚æ•°é«˜æ•ˆå¾®è°ƒ
- **æ•°æ®å¤„ç†**: ä¸­æ–‡æŒ‡ä»¤æ•°æ®é›†ä¼˜åŒ–
- **Webç•Œé¢**: äº¤äº’å¼æ¨¡å‹å¯¹è¯
- **å®éªŒè·Ÿè¸ª**: Weights & Biasesé›†æˆ

### ğŸš€ å¿«é€Ÿå¼€å§‹

1. **ç¯å¢ƒå‡†å¤‡** (å·²å®Œæˆ âœ…)
```bash
pip install -r requirements.txt
```

2. **æ•°æ®é¢„å¤„ç†** (å·²å®Œæˆ âœ…)
```bash
python fix_data_format.py
```

3. **å¼€å§‹è®­ç»ƒ**
```bash
python src/train.py --config configs/quick_test.yaml
```

4. **æ¨¡å‹æ¨ç†**
```bash
python src/inference.py --model_path results/models/your-model
```

### ğŸ“ é¡¹ç›®ç»“æ„
```
â”œâ”€â”€ src/                 # æ ¸å¿ƒæºä»£ç 
â”œâ”€â”€ configs/             # é…ç½®æ–‡ä»¶
â”œâ”€â”€ demo/               # Webæ¼”ç¤ºç•Œé¢
â”œâ”€â”€ data/               # æ•°æ®ç›®å½•
â”œâ”€â”€ results/            # è®­ç»ƒç»“æœ
â””â”€â”€ scripts/            # å·¥å…·è„šæœ¬
```

### ğŸ”§ é…ç½®è¯´æ˜
- `quick_test.yaml`: å¿«é€Ÿæµ‹è¯•é…ç½®
- `production_training.yaml`: ç”Ÿäº§ç¯å¢ƒé…ç½®
- `chatglm3_lora.yaml`: ChatGLM3ä¸“ç”¨é…ç½®

### ğŸ’¡ æç¤º
å½“å‰æ¼”ç¤ºä½¿ç”¨è½»é‡çº§æ¨¡å‹ã€‚å®Œæ•´è®­ç»ƒåï¼Œæ‚¨å°†è·å¾—ä¸“ä¸šçš„ä¸­æ–‡å¯¹è¯èƒ½åŠ›ï¼
            """)
    
    return interface

if __name__ == "__main__":
    print("ğŸŒ å¯åŠ¨ä¸­æ–‡LLM LoRAå¾®è°ƒæ¡†æ¶æ¼”ç¤ºç•Œé¢...")
    
    try:
        interface = create_interface()
        interface.launch(
            server_name="127.0.0.1",
            server_port=7860,
            share=False,
            show_error=True,
            quiet=False
        )
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿å·²å®‰è£…gradio: pip install gradio")