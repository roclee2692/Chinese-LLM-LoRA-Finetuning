"""
åŸºäºGradioçš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹LoRAå¾®è°ƒæ¼”ç¤ºç•Œé¢
æä¾›æ¨¡å‹æ¨ç†ã€å¯¹æ¯”å’Œäº¤äº’åŠŸèƒ½
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# å°è¯•å¯¼å…¥ç›¸å…³åº“
try:
    import gradio as gr
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    import yaml
except ImportError as e:
    print(f"æŸäº›åº“æœªå®‰è£…: {e}")
    print("è¯·å®‰è£…: pip install gradio torch transformers peft")

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

try:
    from inference import ModelInference, ChatBot
    from utils import load_config
except ImportError as e:
    print(f"æ— æ³•å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—: {e}")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GradioDemo:
    """Gradioæ¼”ç¤ºç•Œé¢ç±»"""
    
    def __init__(self):
        self.models = {}  # å­˜å‚¨åŠ è½½çš„æ¨¡å‹
        self.current_model = None
        self.chat_history = []
        self.max_history_length = 10
        
        # é¢„å®šä¹‰çš„ç¤ºä¾‹
        self.examples = [
            ["è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚", ""],
            ["è§£é‡Šä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿ", ""],
            ["è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ã€‚", ""],
            ["ç¿»è¯‘ä»¥ä¸‹æ–‡æœ¬", "Hello, how are you?"],
            ["æ€»ç»“ä»¥ä¸‹å†…å®¹çš„è¦ç‚¹", "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè¯•å›¾ç†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚"],
        ]
    
    def load_model(self, model_path: str, base_model_path: str = "") -> str:
        """åŠ è½½æ¨¡å‹"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
            
            # åˆ›å»ºæ¨¡å‹æ¨ç†å™¨
            inference = ModelInference(
                model_path=model_path,
                base_model_path=base_model_path if base_model_path else None
            )
            inference.load_model()
            
            # å­˜å‚¨æ¨¡å‹
            model_name = Path(model_path).name
            self.models[model_name] = inference
            self.current_model = model_name
            
            return f"âœ… æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸï¼"
            
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def generate_response(
        self, 
        instruction: str, 
        input_text: str, 
        model_name: str,
        max_length: int,
        temperature: float,
        top_p: float,
        repetition_penalty: float
    ) -> str:
        """ç”Ÿæˆå›å¤"""
        try:
            if not instruction.strip():
                return "âš ï¸ è¯·è¾“å…¥æŒ‡ä»¤å†…å®¹"
            
            if model_name not in self.models:
                return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹"
            
            inference = self.models[model_name]
            
            # ç”Ÿæˆå›å¤
            if input_text.strip():
                response = inference.instruction_following(
                    instruction=instruction,
                    input_text=input_text
                )
            else:
                response = inference.generate(
                    prompt=f"### æŒ‡ä»¤:\n{instruction}\n\n### å›ç­”:\n",
                    max_new_tokens=max_length,
                    temperature=temperature,
                    top_p=top_p,
                    repetition_penalty=repetition_penalty
                )
            
            return response
            
        except Exception as e:
            error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def chat_response(
        self,
        message: str,
        history: List[List[str]],
        model_name: str,
        max_length: int,
        temperature: float,
        top_p: float
    ) -> Tuple[str, List[List[str]]]:
        """èŠå¤©å›å¤"""
        try:
            if not message.strip():
                return "", history
            
            if model_name not in self.models:
                return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", history
            
            inference = self.models[model_name]
            
            # æ„å»ºå¯¹è¯å†å²
            chat_history = []
            for user_msg, bot_msg in history:
                chat_history.append({"user": user_msg, "assistant": bot_msg})
            
            # ç”Ÿæˆå›å¤
            response = inference.chat(
                message=message,
                history=chat_history
            )
            
            # æ›´æ–°å†å²
            history.append([message, response])
            
            # é™åˆ¶å†å²é•¿åº¦
            if len(history) > self.max_history_length:
                history = history[-self.max_history_length:]
            
            return "", history
            
        except Exception as e:
            error_msg = f"âŒ èŠå¤©å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg, history
    
    def compare_models(
        self,
        instruction: str,
        input_text: str,
        model1_name: str,
        model2_name: str,
        max_length: int,
        temperature: float
    ) -> Tuple[str, str]:
        """æ¨¡å‹å¯¹æ¯”"""
        try:
            if not instruction.strip():
                return "âš ï¸ è¯·è¾“å…¥æŒ‡ä»¤å†…å®¹", "âš ï¸ è¯·è¾“å…¥æŒ‡ä»¤å†…å®¹"
            
            response1 = "âŒ æ¨¡å‹1æœªåŠ è½½"
            response2 = "âŒ æ¨¡å‹2æœªåŠ è½½"
            
            # æ¨¡å‹1ç”Ÿæˆ
            if model1_name in self.models:
                inference1 = self.models[model1_name]
                if input_text.strip():
                    response1 = inference1.instruction_following(instruction, input_text)
                else:
                    response1 = inference1.generate(
                        f"### æŒ‡ä»¤:\n{instruction}\n\n### å›ç­”:\n",
                        max_new_tokens=max_length,
                        temperature=temperature
                    )
            
            # æ¨¡å‹2ç”Ÿæˆ
            if model2_name in self.models:
                inference2 = self.models[model2_name]
                if input_text.strip():
                    response2 = inference2.instruction_following(instruction, input_text)
                else:
                    response2 = inference2.generate(
                        f"### æŒ‡ä»¤:\n{instruction}\n\n### å›ç­”:\n",
                        max_new_tokens=max_length,
                        temperature=temperature
                    )
            
            return response1, response2
            
        except Exception as e:
            error_msg = f"âŒ å¯¹æ¯”å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg, error_msg
    
    def get_model_list(self) -> List[str]:
        """è·å–å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨"""
        return list(self.models.keys())
    
    def create_interface(self) -> gr.Blocks:
        """åˆ›å»ºGradioç•Œé¢"""
        
        with gr.Blocks(
            title="ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹LoRAå¾®è°ƒæ¼”ç¤º",
            theme=gr.themes.Soft(),
            css="""
            .gradio-container {
                max-width: 1200px !important;
                margin: auto !important;
            }
            .header {
                text-align: center;
                background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 20px;
                border-radius: 10px;
                margin-bottom: 20px;
            }
            """
        ) as demo:
            
            # æ ‡é¢˜
            gr.HTML("""
                <div class="header">
                    <h1>ğŸš€ ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹LoRAå¾®è°ƒæ¼”ç¤º</h1>
                    <p>æ”¯æŒå¤šæ¨¡å‹åŠ è½½ã€æ¨ç†å¯¹æ¯”å’Œäº¤äº’å¼èŠå¤©</p>
                </div>
            """)
            
            # æ¨¡å‹ç®¡ç†é€‰é¡¹å¡
            with gr.Tab("ğŸ”§ æ¨¡å‹ç®¡ç†"):
                with gr.Row():
                    with gr.Column():
                        model_path_input = gr.Textbox(
                            label="æ¨¡å‹è·¯å¾„",
                            placeholder="è¾“å…¥æ¨¡å‹è·¯å¾„ï¼Œå¦‚: ./results/models/chatglm3-lora",
                            lines=1
                        )
                        base_model_path_input = gr.Textbox(
                            label="åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºLoRAï¼‰",
                            placeholder="å¦‚: THUDM/chatglm3-6b",
                            lines=1
                        )
                        load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
                    
                    with gr.Column():
                        load_status = gr.Textbox(
                            label="åŠ è½½çŠ¶æ€",
                            lines=3,
                            interactive=False
                        )
                        loaded_models = gr.Textbox(
                            label="å·²åŠ è½½æ¨¡å‹",
                            lines=3,
                            interactive=False
                        )
                
                load_btn.click(
                    fn=self.load_model,
                    inputs=[model_path_input, base_model_path_input],
                    outputs=[load_status]
                ).then(
                    fn=lambda: "\n".join(self.get_model_list()),
                    outputs=[loaded_models]
                )
            
            # å•æ¨¡å‹æ¨ç†é€‰é¡¹å¡
            with gr.Tab("ğŸ’¬ æ¨¡å‹æ¨ç†"):
                with gr.Row():
                    with gr.Column(scale=2):
                        instruction_input = gr.Textbox(
                            label="æŒ‡ä»¤",
                            placeholder="è¯·è¾“å…¥æŒ‡ä»¤ï¼Œå¦‚ï¼šè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
                            lines=3
                        )
                        input_text_input = gr.Textbox(
                            label="è¾“å…¥å†…å®¹ï¼ˆå¯é€‰ï¼‰",
                            placeholder="å¦‚æœæŒ‡ä»¤éœ€è¦å¤„ç†ç‰¹å®šå†…å®¹ï¼Œè¯·åœ¨æ­¤è¾“å…¥",
                            lines=3
                        )
                        
                        with gr.Row():
                            model_dropdown = gr.Dropdown(
                                label="é€‰æ‹©æ¨¡å‹",
                                choices=[],
                                interactive=True
                            )
                            generate_btn = gr.Button("âœ¨ ç”Ÿæˆå›å¤", variant="primary")
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### âš™ï¸ ç”Ÿæˆå‚æ•°")
                        max_length_slider = gr.Slider(
                            minimum=50,
                            maximum=1000,
                            value=256,
                            step=50,
                            label="æœ€å¤§é•¿åº¦"
                        )
                        temperature_slider = gr.Slider(
                            minimum=0.1,
                            maximum=2.0,
                            value=0.7,
                            step=0.1,
                            label="åˆ›é€ æ€§ï¼ˆTemperatureï¼‰"
                        )
                        top_p_slider = gr.Slider(
                            minimum=0.1,
                            maximum=1.0,
                            value=0.9,
                            step=0.05,
                            label="å¤šæ ·æ€§ï¼ˆTop-pï¼‰"
                        )
                        repetition_penalty_slider = gr.Slider(
                            minimum=1.0,
                            maximum=2.0,
                            value=1.1,
                            step=0.05,
                            label="é‡å¤æƒ©ç½š"
                        )
                
                response_output = gr.Textbox(
                    label="æ¨¡å‹å›å¤",
                    lines=8,
                    interactive=False
                )
                
                # ç¤ºä¾‹
                gr.Examples(
                    examples=self.examples,
                    inputs=[instruction_input, input_text_input],
                    label="ğŸ’¡ ç¤ºä¾‹"
                )
                
                generate_btn.click(
                    fn=self.generate_response,
                    inputs=[
                        instruction_input,
                        input_text_input,
                        model_dropdown,
                        max_length_slider,
                        temperature_slider,
                        top_p_slider,
                        repetition_penalty_slider
                    ],
                    outputs=[response_output]
                )
            
            # å¯¹è¯èŠå¤©é€‰é¡¹å¡
            with gr.Tab("ğŸ’­ å¯¹è¯èŠå¤©"):
                with gr.Row():
                    with gr.Column(scale=3):
                        chatbot = gr.Chatbot(
                            label="å¯¹è¯å†å²",
                            height=400,
                            type="messages"
                        )
                        
                        with gr.Row():
                            chat_input = gr.Textbox(
                                label="è¾“å…¥æ¶ˆæ¯",
                                placeholder="è¯·è¾“å…¥æ‚¨çš„æ¶ˆæ¯...",
                                lines=2,
                                scale=4
                            )
                            chat_btn = gr.Button("å‘é€", variant="primary", scale=1)
                            clear_btn = gr.Button("æ¸…ç©º", scale=1)
                    
                    with gr.Column(scale=1):
                        chat_model_dropdown = gr.Dropdown(
                            label="é€‰æ‹©æ¨¡å‹",
                            choices=[],
                            interactive=True
                        )
                        
                        gr.Markdown("### âš™ï¸ èŠå¤©å‚æ•°")
                        chat_max_length = gr.Slider(
                            minimum=50,
                            maximum=500,
                            value=200,
                            step=25,
                            label="å›å¤æœ€å¤§é•¿åº¦"
                        )
                        chat_temperature = gr.Slider(
                            minimum=0.1,
                            maximum=1.5,
                            value=0.7,
                            step=0.1,
                            label="åˆ›é€ æ€§"
                        )
                        chat_top_p = gr.Slider(
                            minimum=0.1,
                            maximum=1.0,
                            value=0.9,
                            step=0.05,
                            label="å¤šæ ·æ€§"
                        )
                
                chat_btn.click(
                    fn=self.chat_response,
                    inputs=[
                        chat_input,
                        chatbot,
                        chat_model_dropdown,
                        chat_max_length,
                        chat_temperature,
                        chat_top_p
                    ],
                    outputs=[chat_input, chatbot]
                )
                
                clear_btn.click(
                    fn=lambda: ([], ""),
                    outputs=[chatbot, chat_input]
                )
                
                # å›è½¦å‘é€
                chat_input.submit(
                    fn=self.chat_response,
                    inputs=[
                        chat_input,
                        chatbot,
                        chat_model_dropdown,
                        chat_max_length,
                        chat_temperature,
                        chat_top_p
                    ],
                    outputs=[chat_input, chatbot]
                )
            
            # æ¨¡å‹å¯¹æ¯”é€‰é¡¹å¡
            with gr.Tab("âš–ï¸ æ¨¡å‹å¯¹æ¯”"):
                with gr.Row():
                    with gr.Column():
                        compare_instruction = gr.Textbox(
                            label="æŒ‡ä»¤",
                            placeholder="è¾“å…¥è¦å¯¹æ¯”çš„æŒ‡ä»¤",
                            lines=2
                        )
                        compare_input = gr.Textbox(
                            label="è¾“å…¥å†…å®¹ï¼ˆå¯é€‰ï¼‰",
                            lines=2
                        )
                        
                        with gr.Row():
                            model1_dropdown = gr.Dropdown(
                                label="æ¨¡å‹1",
                                choices=[],
                                interactive=True
                            )
                            model2_dropdown = gr.Dropdown(
                                label="æ¨¡å‹2", 
                                choices=[],
                                interactive=True
                            )
                        
                        with gr.Row():
                            compare_max_length = gr.Slider(50, 500, 200, label="æœ€å¤§é•¿åº¦")
                            compare_temperature = gr.Slider(0.1, 1.5, 0.7, label="åˆ›é€ æ€§")
                        
                        compare_btn = gr.Button("ğŸ”„ å¼€å§‹å¯¹æ¯”", variant="primary")
                
                with gr.Row():
                    model1_output = gr.Textbox(
                        label="æ¨¡å‹1å›å¤",
                        lines=8,
                        interactive=False
                    )
                    model2_output = gr.Textbox(
                        label="æ¨¡å‹2å›å¤",
                        lines=8,
                        interactive=False
                    )
                
                compare_btn.click(
                    fn=self.compare_models,
                    inputs=[
                        compare_instruction,
                        compare_input,
                        model1_dropdown,
                        model2_dropdown,
                        compare_max_length,
                        compare_temperature
                    ],
                    outputs=[model1_output, model2_output]
                )
            
            # æ›´æ–°æ¨¡å‹åˆ—è¡¨çš„å‡½æ•°
            def update_model_choices():
                choices = self.get_model_list()
                return (
                    gr.Dropdown(choices=choices),
                    gr.Dropdown(choices=choices),
                    gr.Dropdown(choices=choices),
                    gr.Dropdown(choices=choices)
                )
            
            # å®šæœŸæ›´æ–°æ¨¡å‹é€‰æ‹©å™¨
            demo.load(
                fn=update_model_choices,
                outputs=[model_dropdown, chat_model_dropdown, model1_dropdown, model2_dropdown]
            )
            
            # é¡µè„šä¿¡æ¯
            gr.HTML("""
                <div style="text-align: center; margin-top: 20px; padding: 10px; 
                           background-color: #f8f9fa; border-radius: 5px;">
                    <p>ğŸ”— é¡¹ç›®åœ°å€: <a href="https://github.com/roclee2692/Chinese-LLM-LoRA-Finetuning" target="_blank">GitHub</a></p>
                    <p>ğŸ“– ä½¿ç”¨è¯´æ˜: å…ˆåœ¨"æ¨¡å‹ç®¡ç†"ä¸­åŠ è½½æ¨¡å‹ï¼Œç„¶ååœ¨å…¶ä»–é€‰é¡¹å¡ä¸­ä½¿ç”¨</p>
                </div>
            """)
        
        return demo


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹LoRAå¾®è°ƒGradioæ¼”ç¤º")
    parser.add_argument('--host', type=str, default='127.0.0.1', help='æœåŠ¡å™¨åœ°å€')
    parser.add_argument('--port', type=int, default=7860, help='ç«¯å£å·')
    parser.add_argument('--share', action='store_true', help='åˆ›å»ºå…¬å…±é“¾æ¥')
    parser.add_argument('--model_path', type=str, help='é»˜è®¤åŠ è½½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--base_model_path', type=str, help='åŸºç¡€æ¨¡å‹è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¼”ç¤ºç•Œé¢
    demo_app = GradioDemo()
    
    # å¦‚æœæŒ‡å®šäº†æ¨¡å‹è·¯å¾„ï¼Œåˆ™è‡ªåŠ¨åŠ è½½
    if args.model_path:
        logger.info(f"è‡ªåŠ¨åŠ è½½æ¨¡å‹: {args.model_path}")
        result = demo_app.load_model(args.model_path, args.base_model_path or "")
        print(result)
    
    # åˆ›å»ºç•Œé¢
    demo = demo_app.create_interface()
    
    # å¯åŠ¨æœåŠ¡
    print("å¯åŠ¨Gradioæ¼”ç¤ºç•Œé¢...")
    print(f"åœ°å€: http://{args.host}:{args.port}")
    
    demo.launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share,
        show_error=True
    )


if __name__ == "__main__":
    main()