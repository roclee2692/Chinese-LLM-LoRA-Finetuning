#!/usr/bin/env python3
"""
å¯åŠ¨ Qwen-1.8B LoRA è®­ç»ƒè„šæœ¬
ä½¿ç”¨é¢„è®¾çš„é…ç½®å¯åŠ¨å®é™…çš„å¤§æ¨¡å‹è®­ç»ƒ
"""

import os
import sys
import subprocess
import json
from pathlib import Path
import datetime
import argparse

def check_environment():
    """æ£€æŸ¥è®­ç»ƒç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥è®­ç»ƒç¯å¢ƒ...")
    
    # æ£€æŸ¥GPU
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode != 0:
            print("âŒ æœªæ£€æµ‹åˆ°NVIDIA GPU")
            return False
        print("âœ… GPUæ£€æŸ¥é€šè¿‡")
    except FileNotFoundError:
        print("âŒ nvidia-smi ä¸å¯ç”¨")
        return False
    
    # æ£€æŸ¥CUDA
    try:
        import torch
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨")
            return False
        print(f"âœ… CUDAå¯ç”¨ï¼Œæ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    
    return True

def prepare_training_config():
    """å‡†å¤‡è®­ç»ƒé…ç½®"""
    config = {
        "model_name_or_path": "cache/models--Qwen--Qwen-1_8B-Chat",
        "dataset_name": "data/processed",
        "output_dir": "results/models/qwen-1.8b-lora-chat",
        "logging_dir": "results/logs/qwen-1.8b-lora-chat",
        "per_device_train_batch_size": 1,
        "gradient_accumulation_steps": 8,
        "num_train_epochs": 3,
        "learning_rate": 2e-4,
        "max_seq_length": 512,
        "lora_r": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.1,
        "warmup_ratio": 0.1,
        "weight_decay": 0.01,
        "logging_steps": 10,
        "save_steps": 100,
        "evaluation_strategy": "steps",
        "eval_steps": 100,
        "save_total_limit": 3,
        "load_best_model_at_end": True,
        "metric_for_best_model": "eval_loss",
        "greater_is_better": False,
        "dataloader_num_workers": 4,
        "remove_unused_columns": False,
        "optim": "adamw_torch",
        "lr_scheduler_type": "cosine",
        "bf16": True,
        "tf32": True,
        "gradient_checkpointing": True,
        "use_lora": True,
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "task_type": "CAUSAL_LM"
    }
    
    return config

def create_training_script(config):
    """åˆ›å»ºè®­ç»ƒè„šæœ¬"""
    script_content = f'''#!/usr/bin/env python3
"""
Qwen-1.8B LoRA è®­ç»ƒè„šæœ¬
è‡ªåŠ¨ç”Ÿæˆçš„è®­ç»ƒè„šæœ¬
"""

import os
import sys
import torch
import transformers
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset, Dataset
import json
from datetime import datetime

def setup_model_and_tokenizer():
    """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
    model_path = "{config['model_name_or_path']}"
    
    print(f"ğŸš€ åŠ è½½æ¨¡å‹: {{model_path}}")
    
    # é…ç½®é‡åŒ–
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        padding_side="right"
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16
    )
    
    # é…ç½®LoRA
    lora_config = LoraConfig(
        r={config['lora_r']},
        lora_alpha={config['lora_alpha']},
        lora_dropout={config['lora_dropout']},
        target_modules={config['target_modules']},
        task_type=TaskType.{config['task_type']}
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    return model, tokenizer

def prepare_dataset(tokenizer):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
    train_data = [
        {{"instruction": "ä½ å¥½", "output": "ä½ å¥½ï¼æˆ‘æ˜¯Qwenï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"}},
        {{"instruction": "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", "output": "æˆ‘æ˜¯Qwenï¼Œä¸€ä¸ªç”±é˜¿é‡Œäº‘å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨å›ç­”é—®é¢˜ã€åˆ›ä½œå†…å®¹ã€ç¿»è¯‘æ–‡æœ¬ç­‰å¤šç§ä»»åŠ¡ã€‚"}},
        {{"instruction": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", "output": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥ã€è¯­è¨€ç†è§£ç­‰èƒ½åŠ›ã€‚"}},
        {{"instruction": "è¯·è§£é‡Šæœºå™¨å­¦ä¹ ", "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ï¼Œæ— éœ€æ˜ç¡®ç¼–ç¨‹å°±èƒ½åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚"}},
        {{"instruction": "æ·±åº¦å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ", "output": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘å¤„ç†ä¿¡æ¯çš„æ–¹å¼ï¼Œç‰¹åˆ«æ“…é•¿å¤„ç†å›¾åƒã€è¯­éŸ³å’Œæ–‡æœ¬ç­‰å¤æ‚æ•°æ®ã€‚"}}
    ]
    
    def format_instruction(sample):
        return f"<|im_start|>system\\nä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚<|im_end|>\\n<|im_start|>user\\n{{sample['instruction']}}<|im_end|>\\n<|im_start|>assistant\\n{{sample['output']}}<|im_end|>"
    
    def tokenize_function(examples):
        texts = [format_instruction(ex) for ex in examples]
        tokenized = tokenizer(
            texts,
            truncation=True,
            padding=False,
            max_length={config['max_seq_length']},
            return_tensors="pt"
        )
        tokenized["labels"] = tokenized["input_ids"].clone()
        return tokenized
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = Dataset.from_list(train_data * 100)  # å¤åˆ¶æ•°æ®ä»¥å¢åŠ è®­ç»ƒæ ·æœ¬
    tokenized_dataset = dataset.map(
        lambda x: tokenize_function([x]),
        batched=False,
        remove_columns=dataset.column_names
    )
    
    return tokenized_dataset

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å¼€å§‹ Qwen-1.8B LoRA è®­ç»ƒ")
    print(f"â° è®­ç»ƒå¼€å§‹æ—¶é—´: {{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}}")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = "{config['output_dir']}"
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs("{config['logging_dir']}", exist_ok=True)
    
    # è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = setup_model_and_tokenizer()
    
    # å‡†å¤‡æ•°æ®é›†
    train_dataset = prepare_dataset(tokenizer)
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size={config['per_device_train_batch_size']},
        gradient_accumulation_steps={config['gradient_accumulation_steps']},
        num_train_epochs={config['num_train_epochs']},
        learning_rate={config['learning_rate']},
        warmup_ratio={config['warmup_ratio']},
        weight_decay={config['weight_decay']},
        logging_dir="{config['logging_dir']}",
        logging_steps={config['logging_steps']},
        save_steps={config['save_steps']},
        save_total_limit={config['save_total_limit']},
        load_best_model_at_end={config['load_best_model_at_end']},
        dataloader_num_workers={config['dataloader_num_workers']},
        remove_unused_columns={config['remove_unused_columns']},
        optim="{config['optim']}",
        lr_scheduler_type="{config['lr_scheduler_type']}",
        bf16={str(config['bf16']).lower()},
        tf32={str(config['tf32']).lower()},
        gradient_checkpointing={str(config['gradient_checkpointing']).lower()},
        report_to=None,  # ç¦ç”¨wandbç­‰
        run_name="qwen-1.8b-lora-training"
    )
    
    # æ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ“š å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {{output_dir}}")

if __name__ == "__main__":
    main()
'''
    
    return script_content

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¯åŠ¨ Qwen-1.8B LoRA è®­ç»ƒ")
    parser.add_argument("--dry-run", action="store_true", help="åªç”Ÿæˆè„šæœ¬ä¸æ‰§è¡Œè®­ç»ƒ")
    parser.add_argument("--monitor", action="store_true", help="è®­ç»ƒåå¯åŠ¨ç›‘æ§")
    args = parser.parse_args()
    
    print("ğŸš€ Qwen-1.8B LoRA è®­ç»ƒå¯åŠ¨å™¨")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿GPUå’ŒCUDAæ­£å¸¸å·¥ä½œ")
        return
    
    # å‡†å¤‡é…ç½®
    config = prepare_training_config()
    print("âœ… è®­ç»ƒé…ç½®å‡†å¤‡å®Œæˆ")
    
    # åˆ›å»ºè®­ç»ƒè„šæœ¬
    script_content = create_training_script(config)
    script_path = Path("train_qwen_lora.py")
    
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print(f"âœ… è®­ç»ƒè„šæœ¬å·²ç”Ÿæˆ: {script_path}")
    
    # ä¿å­˜é…ç½®
    config_path = Path("configs/qwen_training_config.json")
    config_path.parent.mkdir(exist_ok=True)
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_path}")
    
    if args.dry_run:
        print("ğŸ” å¹²è¿è¡Œæ¨¡å¼ - è„šæœ¬å·²ç”Ÿæˆä½†ä¸ä¼šæ‰§è¡Œè®­ç»ƒ")
        print(f"è¦å¼€å§‹è®­ç»ƒï¼Œè¯·è¿è¡Œ: python {script_path}")
        return
    
    # æ‰§è¡Œè®­ç»ƒ
    print("\nğŸš€ å¼€å§‹æ‰§è¡Œè®­ç»ƒ...")
    print("âš ï¸  æ³¨æ„: è¿™ä¸ªè®­ç»ƒå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUå†…å­˜")
    
    # ç¡®è®¤æ˜¯å¦ç»§ç»­
    response = input("æ˜¯å¦ç»§ç»­æ‰§è¡Œè®­ç»ƒï¼Ÿ(y/N): ")
    if response.lower() != 'y':
        print("âŒ è®­ç»ƒå·²å–æ¶ˆ")
        return
    
    try:
        # ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒä¸­çš„Pythonæ‰§è¡Œè®­ç»ƒè„šæœ¬
        python_exe = Path("llm-lora/Scripts/python.exe")
        if python_exe.exists():
            cmd = [str(python_exe), str(script_path)]
        else:
            cmd = ["python", str(script_path)]
        
        print(f"ğŸ“ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        subprocess.run(cmd, check=True)
        
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
        if args.monitor:
            print("\nğŸ” å¯åŠ¨ç›‘æ§...")
            monitor_cmd = [str(python_exe), "simple_monitor.py"] if python_exe.exists() else ["python", "simple_monitor.py"]
            subprocess.run(monitor_cmd)
            
    except subprocess.CalledProcessError as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")

if __name__ == "__main__":
    main()