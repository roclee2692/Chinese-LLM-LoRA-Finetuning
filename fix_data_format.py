#!/usr/bin/env python3
"""
æ•°æ®æ ¼å¼ä¿®å¤å’Œæµ‹è¯•è„šæœ¬
è¿è¡Œ: python fix_data_format.py
"""

import sys
import json
from pathlib import Path
from datasets import load_dataset
from transformers import AutoTokenizer

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))
from data_preprocessing import DataProcessor

def test_data_format():
    """æµ‹è¯•æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®"""
    print("ğŸ”§ å¼€å§‹æ•°æ®æ ¼å¼è¯Šæ–­å’Œä¿®å¤...")
    
    # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
    processor = DataProcessor(
        max_length=256,
        fix_format=True
    )
    
    try:
        # 1. æµ‹è¯•Belleæ•°æ®é›†çš„ä¸€å°éƒ¨åˆ†
        print("\nğŸ“¥ åŠ è½½Belleæ•°æ®é›†æ ·æœ¬...")
        dataset = load_dataset("BelleGroup/train_0.5M_CN", split="train[:50]")  # åªå–50ä¸ªæ ·æœ¬æµ‹è¯•
        
        print(f"âœ… æˆåŠŸåŠ è½½{len(dataset)}ä¸ªæ ·æœ¬")
        
        # 2. æ£€æŸ¥åŸå§‹æ•°æ®æ ¼å¼
        print("\nğŸ” æ£€æŸ¥åŸå§‹æ•°æ®æ ¼å¼...")
        sample = dataset[0]
        print(f"æ ·æœ¬å­—æ®µ: {list(sample.keys())}")
        print(f"æ ·æœ¬å†…å®¹é¢„è§ˆ: {str(sample)[:200]}...")
        
        # 3. æµ‹è¯•æ ¼å¼åŒ–å‡½æ•°
        print("\nğŸ”§ æµ‹è¯•æ•°æ®æ ¼å¼åŒ–...")
        formatted_sample = processor.format_instruction_data(sample)
        print(f"æ ¼å¼åŒ–åå­—æ®µ: {list(formatted_sample.keys())}")
        print(f"æ ¼å¼åŒ–æ–‡æœ¬é•¿åº¦: {len(formatted_sample['text'])}")
        print(f"æ ¼å¼åŒ–æ–‡æœ¬é¢„è§ˆ: {formatted_sample['text'][:300]}...")
        
        # 4. æ‰¹é‡æ ¼å¼åŒ–æµ‹è¯•
        print("\nâš¡ æµ‹è¯•æ‰¹é‡æ ¼å¼åŒ–...")
        formatted_dataset = dataset.map(
            processor.format_instruction_data,
            remove_columns=dataset.column_names,
            desc="æ ¼å¼åŒ–æ•°æ®"
        )
        
        # 5. è¿‡æ»¤é•¿åº¦
        print("\nğŸ“ æµ‹è¯•é•¿åº¦è¿‡æ»¤...")
        filtered_dataset = formatted_dataset.filter(
            processor.filter_by_length,
            desc="è¿‡æ»¤é•¿åº¦"
        )
        
        print(f"è¿‡æ»¤å‰: {len(formatted_dataset)}, è¿‡æ»¤å: {len(filtered_dataset)}")
        
        # 6. æµ‹è¯•åˆ†è¯
        print("\nğŸ”¤ æµ‹è¯•åˆ†è¯...")
        tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
        tokenizer.pad_token = tokenizer.eos_token
        
        # æµ‹è¯•å•ä¸ªæ ·æœ¬åˆ†è¯
        test_sample = {'text': [filtered_dataset[0]['text']]}
        tokenized = processor.tokenize_function(test_sample, tokenizer)
        print(f"åˆ†è¯ç»“æœå­—æ®µ: {list(tokenized.keys())}")
        print(f"input_idsé•¿åº¦: {len(tokenized['input_ids'][0])}")
        print(f"labelsé•¿åº¦: {len(tokenized['labels'][0])}")
        
        # 7. æ‰¹é‡åˆ†è¯æµ‹è¯•
        print("\nâš¡ æµ‹è¯•æ‰¹é‡åˆ†è¯...")
        tokenized_dataset = filtered_dataset.map(
            lambda examples: processor.tokenize_function(examples, tokenizer),
            batched=True,
            remove_columns=filtered_dataset.column_names,
            desc="åˆ†è¯å¤„ç†"
        )
        
        print(f"âœ… æˆåŠŸåˆ†è¯{len(tokenized_dataset)}ä¸ªæ ·æœ¬")
        
        # 8. ä¿å­˜ä¿®å¤åçš„æ•°æ®
        output_dir = Path("data/processed")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å°‘é‡æ ·æœ¬ç”¨äºæµ‹è¯•
        test_data = tokenized_dataset.select(range(min(20, len(tokenized_dataset))))
        test_data.save_to_disk(output_dir / "test_data")
        
        print(f"\nğŸ’¾ å·²ä¿å­˜ä¿®å¤åçš„æµ‹è¯•æ•°æ®åˆ°: {output_dir / 'test_data'}")
        
        print("\nğŸ‰ æ•°æ®æ ¼å¼ä¿®å¤å®Œæˆï¼")
        print("ğŸ“‹ ä¿®å¤æ€»ç»“:")
        print(f"  - åŸå§‹æ ·æœ¬: {len(dataset)}")
        print(f"  - æ ¼å¼åŒ–å: {len(formatted_dataset)}")
        print(f"  - é•¿åº¦è¿‡æ»¤å: {len(filtered_dataset)}")
        print(f"  - åˆ†è¯å: {len(tokenized_dataset)}")
        print(f"  - æµ‹è¯•æ•°æ®: {len(test_data)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ ¼å¼ä¿®å¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_simple_test_data():
    """åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®"""
    print("\nğŸ”§ åˆ›å»ºç®€å•æµ‹è¯•æ•°æ®...")
    
    test_data = [
        {
            "instruction": "è¯·ä»‹ç»äººå·¥æ™ºèƒ½",
            "input": "",
            "output": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›é€ èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½æ‰èƒ½å®Œæˆçš„ä»»åŠ¡çš„æœºå™¨ã€‚"
        },
        {
            "instruction": "ç¿»è¯‘ä»¥ä¸‹è‹±æ–‡",
            "input": "Hello world",
            "output": "ä½ å¥½ä¸–ç•Œ"
        },
        {
            "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ",
            "input": "",
            "output": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„é«˜çº§æŠ½è±¡ã€‚"
        }
    ]
    
    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    output_path = Path("data/raw/simple_test.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… åˆ›å»ºç®€å•æµ‹è¯•æ•°æ®: {output_path}")
    return output_path

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ•°æ®æ ¼å¼è¯Šæ–­å’Œä¿®å¤...")
    
    # é¦–å…ˆå°è¯•ä¿®å¤ç½‘ç»œæ•°æ®
    success = test_data_format()
    
    if not success:
        print("\nâš ï¸ ç½‘ç»œæ•°æ®å¤„ç†å¤±è´¥ï¼Œåˆ›å»ºæœ¬åœ°æµ‹è¯•æ•°æ®...")
        simple_data_path = create_simple_test_data()
        print(f"âœ… å¯ä»¥ä½¿ç”¨æœ¬åœ°æ•°æ®è¿›è¡Œæµ‹è¯•: {simple_data_path}")
    
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print("1. python src/train.py --config configs/quick_test.yaml")
    print("2. æ£€æŸ¥è®­ç»ƒæ˜¯å¦èƒ½æ­£å¸¸å¼€å§‹")
    print("3. å¦‚æœä»æœ‰é—®é¢˜ï¼Œä½¿ç”¨æœ¬åœ°ç®€å•æ•°æ®æµ‹è¯•")