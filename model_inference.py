#!/usr/bin/env python3
"""
çœŸå®æ¨¡å‹æ¨ç†æ¨¡å—
åŠ è½½è®­ç»ƒå¥½çš„Qwen LoRAæ¨¡å‹è¿›è¡Œæ¨ç†
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import os
import json
from pathlib import Path

class QwenLoRAInference:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_loaded = False
        
        # æ¨¡å‹è·¯å¾„é…ç½®
        self.base_model_path = "cache/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081"
        self.lora_adapter_path = "results/models/qwen-1.8b-lora-ultimate"
        
    def load_model(self):
        """åŠ è½½åŸºç¡€æ¨¡å‹å’ŒLoRAé€‚é…å™¨"""
        try:
            print("ğŸš€ å¼€å§‹åŠ è½½Qwen-1.8BåŸºç¡€æ¨¡å‹...")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„
            if not os.path.exists(self.base_model_path):
                print(f"âŒ åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.base_model_path}")
                return False
                
            if not os.path.exists(self.lora_adapter_path):
                print(f"âŒ LoRAé€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {self.lora_adapter_path}")
                return False
            
            # åŠ è½½tokenizer
            print("ğŸ“ åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_path, 
                trust_remote_code=True,
                pad_token='<|endoftext|>'
            )
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            print("ğŸ§  åŠ è½½åŸºç¡€æ¨¡å‹...")
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            print("âš¡ åŠ è½½LoRAé€‚é…å™¨...")
            self.model = PeftModel.from_pretrained(
                base_model,
                self.lora_adapter_path,
                torch_dtype=torch.float16
            )
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            self.model_loaded = True
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def generate_response(self, user_input, max_length=512, temperature=0.7):
        """ç”Ÿæˆæ¨¡å‹å›å¤"""
        if not self.model_loaded:
            if not self.load_model():
                return "âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå›å¤"
        
        try:
            # æ„å»ºå¯¹è¯æ ¼å¼
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œç»è¿‡LoRAå¾®è°ƒï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": user_input}
            ]
            
            # ä½¿ç”¨chat templateæ ¼å¼åŒ–è¾“å…¥
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ç¼–ç è¾“å…¥
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                generated_ids = self.model.generate(
                    model_inputs.input_ids,
                    max_new_tokens=max_length,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    top_p=0.8,
                    repetition_penalty=1.1
                )
            
            # è§£ç è¾“å‡º
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            
            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # æ¸…ç†è¾“å‡º
            response = response.strip()
            if not response:
                response = "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆåˆé€‚çš„å›å¤ã€‚"
            
            return response
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}")
            return f"ç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = {
            "model_loaded": self.model_loaded,
            "device": str(self.device),
            "base_model": "Qwen-1.8B-Chat",
            "lora_adapter": "qwen-1.8b-lora-ultimate",
            "status": "âœ… å·²åŠ è½½" if self.model_loaded else "âŒ æœªåŠ è½½"
        }
        return info

# å…¨å±€æ¨¡å‹å®ä¾‹
model_inference = None

def get_model_instance():
    """è·å–æ¨¡å‹å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global model_inference
    if model_inference is None:
        model_inference = QwenLoRAInference()
    return model_inference

def chat_with_model(user_input):
    """ä¸æ¨¡å‹å¯¹è¯çš„ç®€å•æ¥å£"""
    model = get_model_instance()
    return model.generate_response(user_input)

def test_model():
    """æµ‹è¯•æ¨¡å‹åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹åŠŸèƒ½...")
    model = get_model_instance()
    
    test_questions = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
        "ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ",
        "è¯·è§£é‡Šä¸€ä¸‹LoRAæŠ€æœ¯",
        "ä½ ä¼šä¸­æ–‡å—ï¼Ÿ"
    ]
    
    for question in test_questions:
        print(f"\nâ“ ç”¨æˆ·: {question}")
        response = model.generate_response(question)
        print(f"ğŸ¤– æ¨¡å‹: {response}")
        print("-" * 50)

if __name__ == "__main__":
    test_model()