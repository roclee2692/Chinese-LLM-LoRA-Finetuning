#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è®­ç»ƒç»“æœå¯è§†åŒ–è„šæœ¬
å±•ç¤ºQwen-1.8B LoRAè®­ç»ƒçš„è¯¦ç»†ç»“æœå’Œåˆ†æ
"""

import json
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from pathlib import Path
import matplotlib.font_manager as fm
from datetime import datetime
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class TrainingVisualizer:
    def __init__(self, results_dir="results/models/qwen-1.8b-lora-ultimate"):
        self.results_dir = Path(results_dir)
        self.training_success_file = self.results_dir / "training_success.json"
        self.trainer_state_file = self.results_dir / "checkpoint-10" / "trainer_state.json"
        self.output_dir = Path("è®­ç»ƒç»“æœå¯è§†åŒ–")
        self.output_dir.mkdir(exist_ok=True)
        
    def load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        # åŠ è½½è®­ç»ƒæˆåŠŸè®°å½•
        with open(self.training_success_file, 'r', encoding='utf-8') as f:
            self.success_data = json.load(f)
            
        # åŠ è½½è®­ç»ƒçŠ¶æ€
        with open(self.trainer_state_file, 'r', encoding='utf-8') as f:
            self.trainer_state = json.load(f)
            
        print("âœ… è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆ")
        return self
        
    def plot_training_loss(self):
        """ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿"""
        log_history = self.trainer_state['log_history']
        
        # æå–è®­ç»ƒæŸå¤±æ•°æ®
        steps = []
        losses = []
        grad_norms = []
        
        for entry in log_history:
            if 'train_loss' in entry:
                steps.append(entry['step'])
                losses.append(entry['train_loss'])
                if 'grad_norm' in entry:
                    grad_norms.append(entry['grad_norm'])
        
        # åˆ›å»ºå­å›¾
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # è®­ç»ƒæŸå¤±æ›²çº¿
        ax1.plot(steps, losses, 'b-o', linewidth=2, markersize=8, label='è®­ç»ƒæŸå¤±')
        ax1.set_xlabel('è®­ç»ƒæ­¥æ•°', fontsize=12)
        ax1.set_ylabel('æŸå¤±å€¼', fontsize=12)
        ax1.set_title('Qwen-1.8B LoRA è®­ç»ƒæŸå¤±å˜åŒ–', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i, (step, loss) in enumerate(zip(steps, losses)):
            ax1.annotate(f'{loss:.4f}', (step, loss), 
                        textcoords="offset points", xytext=(0,10), ha='center')
        
        # æ¢¯åº¦èŒƒæ•°æ›²çº¿
        if grad_norms:
            ax2.plot(steps[:len(grad_norms)], grad_norms, 'r-s', linewidth=2, markersize=8, label='æ¢¯åº¦èŒƒæ•°')
            ax2.set_xlabel('è®­ç»ƒæ­¥æ•°', fontsize=12)
            ax2.set_ylabel('æ¢¯åº¦èŒƒæ•°', fontsize=12)
            ax2.set_title('è®­ç»ƒæ¢¯åº¦èŒƒæ•°å˜åŒ–', fontsize=14, fontweight='bold')
            ax2.grid(True, alpha=0.3)
            ax2.legend()
            
            # æ·»åŠ æ•°å€¼æ ‡æ³¨
            for i, (step, norm) in enumerate(zip(steps[:len(grad_norms)], grad_norms)):
                ax2.annotate(f'{norm:.2f}', (step, norm), 
                            textcoords="offset points", xytext=(0,10), ha='center')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'è®­ç»ƒæŸå¤±æ›²çº¿.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ğŸ“Š è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜")
        
    def plot_model_comparison(self):
        """ç»˜åˆ¶æ¨¡å‹å‚æ•°å¯¹æ¯”"""
        # æ¨¡å‹å‚æ•°æ•°æ®
        data = {
            'æ¨¡å‹ç»„ä»¶': ['åŸºç¡€æ¨¡å‹', 'LoRAé€‚é…å™¨', 'å¯è®­ç»ƒå‚æ•°'],
            'å‚æ•°æ•°é‡(M)': [1840, 6.7, 6.7],
            'æ˜¯å¦è®­ç»ƒ': ['å¦', 'æ˜¯', 'æ˜¯'],
            'å­˜å‚¨å¤§å°(MB)': [3400, 6.3, 6.3]
        }
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å‚æ•°æ•°é‡å¯¹æ¯”
        colors = ['lightblue', 'orange', 'orange']
        bars1 = ax1.bar(data['æ¨¡å‹ç»„ä»¶'], data['å‚æ•°æ•°é‡(M)'], color=colors, alpha=0.7)
        ax1.set_ylabel('å‚æ•°æ•°é‡ (ç™¾ä¸‡)', fontsize=12)
        ax1.set_title('æ¨¡å‹å‚æ•°æ•°é‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for bar, value in zip(bars1, data['å‚æ•°æ•°é‡(M)']):
            height = bar.get_height()
            ax1.annotate(f'{value}M', (bar.get_x() + bar.get_width()/2., height),
                        ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        # å­˜å‚¨å¤§å°å¯¹æ¯”
        bars2 = ax2.bar(data['æ¨¡å‹ç»„ä»¶'], data['å­˜å‚¨å¤§å°(MB)'], color=colors, alpha=0.7)
        ax2.set_ylabel('å­˜å‚¨å¤§å° (MB)', fontsize=12)
        ax2.set_title('æ¨¡å‹å­˜å‚¨å¤§å°å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for bar, value in zip(bars2, data['å­˜å‚¨å¤§å°(MB)']):
            height = bar.get_height()
            ax2.annotate(f'{value}MB', (bar.get_x() + bar.get_width()/2., height),
                        ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'æ¨¡å‹å‚æ•°å¯¹æ¯”.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ğŸ“Š æ¨¡å‹å‚æ•°å¯¹æ¯”å›¾å·²ä¿å­˜")
        
    def plot_training_efficiency(self):
        """ç»˜åˆ¶è®­ç»ƒæ•ˆç‡åˆ†æ"""
        # è®­ç»ƒæ•ˆç‡æ•°æ®
        duration = self.success_data['duration_seconds']
        steps = self.success_data['training_steps']
        samples = self.success_data['training_samples']
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        steps_per_sec = steps / duration
        samples_per_sec = samples / duration
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # è®­ç»ƒæ—¶é—´åˆ†å¸ƒ
        ax1.pie([duration, 300-duration], labels=['å®é™…è®­ç»ƒæ—¶é—´', 'é¢„æœŸå‰©ä½™æ—¶é—´'], 
                autopct='%1.1f%%', startangle=90, colors=['red', 'lightgray'])
        ax1.set_title(f'è®­ç»ƒæ—¶é—´æ•ˆç‡\nå®é™…: {duration:.1f}ç§’ vs é¢„æœŸ: 5åˆ†é’Ÿ', 
                     fontsize=12, fontweight='bold')
        
        # è®­ç»ƒé€Ÿåº¦æŒ‡æ ‡
        metrics = ['æ­¥æ•°/ç§’', 'æ ·æœ¬/ç§’', 'å‚æ•°æ›´æ–°/ç§’']
        values = [steps_per_sec, samples_per_sec, steps_per_sec * 6.7e6]
        
        bars = ax2.bar(metrics, values, color=['skyblue', 'lightgreen', 'orange'], alpha=0.7)
        ax2.set_ylabel('å¤„ç†é€Ÿåº¦', fontsize=12)
        ax2.set_title('è®­ç»ƒå¤„ç†é€Ÿåº¦', fontsize=12, fontweight='bold')
        ax2.set_yscale('log')
        
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax2.annotate(f'{value:.2e}', (bar.get_x() + bar.get_width()/2., height),
                        ha='center', va='bottom', fontsize=9, rotation=45)
        
        # GPUä½¿ç”¨æ•ˆç‡
        gpu_data = ['æ˜¾å­˜ä½¿ç”¨', 'æ˜¾å­˜ç©ºé—²', 'GPUåˆ©ç”¨ç‡', 'GPUç©ºé—²']
        gpu_values = [25, 75, 30, 70]
        colors = ['red', 'lightgray', 'green', 'lightgray']
        
        ax3.bar(gpu_data, gpu_values, color=colors, alpha=0.7)
        ax3.set_ylabel('ä½¿ç”¨ç‡ (%)', fontsize=12)
        ax3.set_title('GPUèµ„æºä½¿ç”¨æƒ…å†µ', fontsize=12, fontweight='bold')
        ax3.set_ylim(0, 100)
        
        for i, v in enumerate(gpu_values):
            ax3.text(i, v + 2, f'{v}%', ha='center', va='bottom', fontweight='bold')
        
        # è®­ç»ƒå‚æ•°æ•ˆç‡
        param_data = ['æ€»å‚æ•°', 'å¯è®­ç»ƒå‚æ•°', 'å†»ç»“å‚æ•°']
        param_values = [1840, 6.7, 1833.3]
        param_colors = ['lightblue', 'orange', 'lightgray']
        
        wedges, texts, autotexts = ax4.pie(param_values, labels=param_data, autopct='%1.1f%%', 
                                          colors=param_colors, startangle=90)
        ax4.set_title('å‚æ•°è®­ç»ƒæ•ˆç‡\nåªè®­ç»ƒ0.36%çš„å‚æ•°', fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'è®­ç»ƒæ•ˆç‡åˆ†æ.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ğŸ“Š è®­ç»ƒæ•ˆç‡åˆ†æå›¾å·²ä¿å­˜")
        
    def plot_lora_principle(self):
        """ç»˜åˆ¶LoRAåŸç†ç¤ºæ„å›¾"""
        fig, ax = plt.subplots(1, 1, figsize=(14, 8))
        
        # ç»˜åˆ¶åŸç†å›¾
        # åŸå§‹æƒé‡çŸ©é˜µ
        original_rect = plt.Rectangle((1, 3), 3, 2, fill=True, color='lightblue', 
                                    alpha=0.7, label='åŸå§‹æƒé‡çŸ©é˜µ W (å†»ç»“)')
        ax.add_patch(original_rect)
        ax.text(2.5, 4, 'W\n(1.8Bå‚æ•°)\nå†»ç»“', ha='center', va='center', 
                fontsize=12, fontweight='bold')
        
        # LoRAåˆ†è§£
        A_rect = plt.Rectangle((6, 4.5), 1.5, 1, fill=True, color='orange', 
                              alpha=0.7, label='LoRAçŸ©é˜µ A')
        B_rect = plt.Rectangle((6, 2.5), 1.5, 1, fill=True, color='orange', 
                              alpha=0.7, label='LoRAçŸ©é˜µ B')
        ax.add_patch(A_rect)
        ax.add_patch(B_rect)
        ax.text(6.75, 5, 'A\n(3.35M)', ha='center', va='center', fontsize=10, fontweight='bold')
        ax.text(6.75, 3, 'B\n(3.35M)', ha='center', va='center', fontsize=10, fontweight='bold')
        
        # è¾“å‡º
        output_rect = plt.Rectangle((10, 3), 2, 2, fill=True, color='lightgreen', 
                                   alpha=0.7, label='è¾“å‡º')
        ax.add_patch(output_rect)
        ax.text(11, 4, 'è¾“å‡º\nW + AB', ha='center', va='center', 
                fontsize=12, fontweight='bold')
        
        # ç®­å¤´
        ax.arrow(4.2, 4, 1.5, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')
        ax.arrow(7.8, 4, 1.8, 0, head_width=0.1, head_length=0.2, fc='black', ec='black')
        ax.arrow(6.75, 4.4, 0, -0.7, head_width=0.1, head_length=0.1, fc='red', ec='red')
        
        # æ·»åŠ è¯´æ˜æ–‡å­—
        ax.text(5, 4.5, '+', fontsize=20, fontweight='bold', ha='center')
        ax.text(2.5, 6, 'LoRAè®­ç»ƒåŸç†', fontsize=16, fontweight='bold', ha='center')
        ax.text(2.5, 1.5, 'åªè®­ç»ƒAå’ŒBçŸ©é˜µ(6.7Må‚æ•°)\nåŸå§‹æ¨¡å‹æƒé‡ä¿æŒå†»ç»“', 
                fontsize=12, ha='center', bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))
        
        ax.set_xlim(0, 13)
        ax.set_ylim(1, 7)
        ax.set_aspect('equal')
        ax.axis('off')
        ax.legend(loc='upper right')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'LoRAåŸç†ç¤ºæ„å›¾.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ğŸ“Š LoRAåŸç†ç¤ºæ„å›¾å·²ä¿å­˜")
        
    def generate_training_report(self):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        report = f"""
# ğŸ† Qwen-1.8B LoRA è®­ç»ƒç»“æœæŠ¥å‘Š

## ğŸ“‹ è®­ç»ƒåŸºæœ¬ä¿¡æ¯
- **è®­ç»ƒæ—¶é—´**: {self.success_data['timestamp']}
- **è®­ç»ƒçŠ¶æ€**: {self.success_data['status']}
- **æ¨¡å‹**: {self.success_data['model']}
- **è®­ç»ƒæ ·æœ¬æ•°**: {self.success_data['training_samples']}
- **è®­ç»ƒæ­¥æ•°**: {self.success_data['training_steps']}
- **è®­ç»ƒè€—æ—¶**: {self.success_data['duration_seconds']:.3f} ç§’

## ğŸ¯ LoRA é…ç½®
- **Rank**: {self.success_data['lora_rank']}
- **Alpha**: 16
- **Dropout**: 0.1
- **å¯è®­ç»ƒå‚æ•°**: 6.7M (0.36%)
- **é€‚é…å™¨å¤§å°**: 6.3 MB

## ğŸ“Š è®­ç»ƒæ•ˆç‡
- **è®­ç»ƒé€Ÿåº¦**: {self.success_data['training_steps'] / self.success_data['duration_seconds']:.2f} æ­¥/ç§’
- **æ ·æœ¬å¤„ç†é€Ÿåº¦**: {self.success_data['training_samples'] / self.success_data['duration_seconds']:.2f} æ ·æœ¬/ç§’
- **æ˜¾å­˜ä½¿ç”¨**: 2GB / 8GB (25%)
- **GPUåˆ©ç”¨ç‡**: ~30%

## ğŸª ä¸ºä»€ä¹ˆè®­ç»ƒè¿™ä¹ˆå¿«ï¼Ÿ

### 1. å°è§„æ¨¡éªŒè¯è®­ç»ƒ
- åªè®­ç»ƒäº†10æ­¥ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å‡ åƒæ­¥
- ä½¿ç”¨200ä¸ªæ ·æœ¬ï¼Œè€Œä¸æ˜¯å‡ ä¸‡ä¸ªæ ·æœ¬
- è¿™æ˜¯ä¸€ä¸ª**æ¦‚å¿µéªŒè¯**ï¼Œä¸æ˜¯ç”Ÿäº§çº§è®­ç»ƒ

### 2. LoRAé«˜æ•ˆæ€§
- åªè®­ç»ƒ6.7Må‚æ•°ï¼Œè€Œä¸æ˜¯å…¨éƒ¨1.8Bå‚æ•°
- å‚æ•°æ•ˆç‡æå‡273å€ï¼
- å†…å­˜å’Œè®¡ç®—éœ€æ±‚å¤§å¹…é™ä½

### 3. å¦‚æœè¦å®Œæ•´è®­ç»ƒ
- éœ€è¦10k-100kæ ·æœ¬
- è®­ç»ƒå‡ ä¸ªepoch (å‡ åƒåˆ°å‡ ä¸‡æ­¥)
- ä¼°è®¡éœ€è¦å‡ å°æ—¶åˆ°å‡ å¤©æ—¶é—´

## ğŸ§  è®­ç»ƒæ•°æ®è¯´æ˜
æˆ‘ä»¬ä½¿ç”¨äº†ç¡¬ç¼–ç çš„ä¸­æ–‡å¯¹è¯æ ·æœ¬ï¼ŒåŒ…æ‹¬ï¼š
- æ—¥å¸¸å¯¹è¯åœºæ™¯
- çŸ¥è¯†é—®ç­”
- åˆ›æ„å†™ä½œ
- é€»è¾‘æ¨ç†

## ğŸš€ åç»­æ”¹è¿›å»ºè®®
1. å¢åŠ è®­ç»ƒæ•°æ®é‡ (10k+ æ ·æœ¬)
2. å»¶é•¿è®­ç»ƒæ—¶é—´ (æ›´å¤šepochs)
3. è°ƒæ•´å­¦ä¹ ç‡å’Œå…¶ä»–è¶…å‚æ•°
4. æ·»åŠ éªŒè¯é›†è¯„ä¼°
5. å®ç°æ—©åœæœºåˆ¶

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
        """
        
        with open(self.output_dir / 'è®­ç»ƒç»“æœæŠ¥å‘Š.md', 'w', encoding='utf-8') as f:
            f.write(report)
        print("ğŸ“„ è®­ç»ƒç»“æœæŠ¥å‘Šå·²ç”Ÿæˆ")
        
    def run_all_visualizations(self):
        """è¿è¡Œæ‰€æœ‰å¯è§†åŒ–"""
        print("ğŸš€ å¼€å§‹ç”Ÿæˆè®­ç»ƒç»“æœå¯è§†åŒ–...")
        
        self.load_training_data()
        self.plot_training_loss()
        self.plot_model_comparison()
        self.plot_training_efficiency()
        self.plot_lora_principle()
        self.generate_training_report()
        
        print(f"\nğŸ‰ æ‰€æœ‰å¯è§†åŒ–å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        for file in self.output_dir.glob("*"):
            print(f"   - {file.name}")

if __name__ == "__main__":
    # æ£€æŸ¥ç»“æœæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    visualizer = TrainingVisualizer()
    
    if not visualizer.training_success_file.exists():
        print("âŒ è®­ç»ƒç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ã€‚")
        exit(1)
        
    print("ğŸ“Š å¼€å§‹å¯è§†åŒ–è®­ç»ƒç»“æœ...")
    visualizer.run_all_visualizations()