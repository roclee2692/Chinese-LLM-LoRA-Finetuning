#!/usr/bin/env python3
"""
Qwen-1.8B LoRA å®é™…è®­ç»ƒè„šæœ¬
åŸºäºæµ‹è¯•æˆåŠŸçš„é…ç½®è¿›è¡Œå®é™…è®­ç»ƒ
"""

import os
import torch
import json
from datetime import datetime
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset

def main():
    print("ğŸš€ å¼€å§‹ Qwen-1.8B LoRA å®é™…è®­ç»ƒ")
    print(f"â° è®­ç»ƒå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ¨¡å‹è·¯å¾„
    model_path = "cache/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081"
    output_dir = "results/models/qwen-1.8b-lora-training"
    
    print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. åŠ è½½åˆ†è¯å™¨
    print("ğŸ“ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        padding_side="right"
    )
    tokenizer.pad_token = tokenizer.eos_token
    print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
    
    # 2. åŠ è½½æ¨¡å‹
    print("ğŸ¤– åŠ è½½æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # 3. é…ç½®LoRA
    print("âš™ï¸ é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=["c_attn", "c_proj", "w1", "w2"],
        task_type=TaskType.CAUSAL_LM
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    print("âœ… LoRAé…ç½®æˆåŠŸ")
    
    # 4. å‡†å¤‡è®­ç»ƒæ•°æ®
    print("ğŸ“š å‡†å¤‡è®­ç»ƒæ•°æ®...")
    
    # ç¤ºä¾‹å¯¹è¯æ•°æ®
    train_data = [
        {"instruction": "ä½ å¥½", "output": "ä½ å¥½ï¼æˆ‘æ˜¯Qwenï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚"},
        {"instruction": "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", "output": "æˆ‘æ˜¯Qwenï¼Œä¸€ä¸ªç”±é˜¿é‡Œäº‘å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚"},
        {"instruction": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", "output": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"},
        {"instruction": "è¯·è§£é‡Šæœºå™¨å­¦ä¹ ", "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚"},
        {"instruction": "æ·±åº¦å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ", "output": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘å¤„ç†ä¿¡æ¯ã€‚"},
        {"instruction": "Pythonæ˜¯ä»€ä¹ˆï¼Ÿ", "output": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´æ˜“è¯»çš„è¯­æ³•è€Œé—»åã€‚"},
        {"instruction": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ", "output": "å­¦ä¹ ç¼–ç¨‹éœ€è¦é€‰æ‹©ä¸€é—¨è¯­è¨€å¼€å§‹ï¼Œå¤šç»ƒä¹ é¡¹ç›®ï¼Œç†è§£ç®—æ³•å’Œæ•°æ®ç»“æ„ã€‚"},
        {"instruction": "LoRAæ˜¯ä»€ä¹ˆï¼Ÿ", "output": "LoRAæ˜¯ä½ç§©é€‚åº”æ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆåœ°å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚"}
    ]
    
    # æ‰©å±•æ•°æ®é›†
    extended_data = train_data * 50  # é‡å¤50æ¬¡ä»¥å¢åŠ è®­ç»ƒæ ·æœ¬
    
    def format_instruction(sample):
        return f"<|im_start|>system\\nä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚<|im_end|>\\n<|im_start|>user\\n{sample['instruction']}<|im_end|>\\n<|im_start|>assistant\\n{sample['output']}<|im_end|>"
    
    def tokenize_function(examples):
        texts = [format_instruction(ex) for ex in examples]
        tokenized = tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt"
        )
        tokenized["labels"] = tokenized["input_ids"].clone()
        return tokenized
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = Dataset.from_list(extended_data)
    tokenized_dataset = dataset.map(
        lambda x: tokenize_function([x]),
        batched=False,
        remove_columns=dataset.column_names
    )
    
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®é‡: {len(tokenized_dataset)} æ ·æœ¬")
    
    # 5. è®­ç»ƒé…ç½®
    print("âš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        num_train_epochs=2,
        learning_rate=2e-4,
        warmup_ratio=0.1,
        weight_decay=0.01,
        logging_steps=10,
        save_steps=50,
        save_total_limit=3,
        dataloader_num_workers=0,  # é¿å…Windowsä¸Šçš„å¤šè¿›ç¨‹é—®é¢˜
        remove_unused_columns=False,
        fp16=True,
        gradient_checkpointing=True,
        report_to=None,
        run_name="qwen-1.8b-lora-training",
        load_best_model_at_end=False
    )
    
    # 6. æ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # 7. åˆ›å»ºè®­ç»ƒå™¨
    print("ğŸ—ï¸ åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # 8. å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š æ‰¹é‡å¤§å°: {training_args.per_device_train_batch_size}")
    print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {training_args.num_train_epochs}")
    print(f"ğŸ“š æ•°æ®é‡: {len(tokenized_dataset)} æ ·æœ¬")
    print("âš¡ å¼€å§‹è®­ç»ƒè¿‡ç¨‹...")
    
    try:
        trainer.train()
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        
        # ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        config_info = {
            "model_name": "Qwen-1.8B",
            "training_method": "LoRA",
            "lora_config": {
                "r": lora_config.r,
                "alpha": lora_config.lora_alpha,
                "dropout": lora_config.lora_dropout,
                "target_modules": lora_config.target_modules
            },
            "training_args": {
                "epochs": training_args.num_train_epochs,
                "batch_size": training_args.per_device_train_batch_size,
                "learning_rate": training_args.learning_rate
            },
            "completion_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "dataset_size": len(tokenized_dataset)
        }
        
        with open(os.path.join(output_dir, "training_info.json"), 'w', encoding='utf-8') as f:
            json.dump(config_info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        print("ğŸ” è¿è¡Œ python simple_monitor.py æŸ¥çœ‹è®­ç»ƒç»“æœ")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()