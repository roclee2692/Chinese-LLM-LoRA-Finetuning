#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆ Qwen-1.8B LoRA è®­ç»ƒè„šæœ¬
"""

import os
import torch
import sys
print("ğŸš€ å¼€å§‹ Qwen-1.8B LoRA è®­ç»ƒ")
print(f"Pythonç‰ˆæœ¬: {sys.version}")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import LoraConfig, get_peft_model, TaskType
    print("âœ… ä¾èµ–åº“åŠ è½½æˆåŠŸ")
    
    # æ¨¡å‹è·¯å¾„
    model_path = "cache/models--Qwen--Qwen-1_8B-Chat/snapshots/1d0f68de57b88cfde81f3c3e537f24464d889081"
    print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        sys.exit(1)
    
    print("ğŸ” æ£€æŸ¥é…ç½®æ–‡ä»¶...")
    config_file = os.path.join(model_path, "config.json")
    if os.path.exists(config_file):
        print("âœ… æ‰¾åˆ°config.json")
        with open(config_file, 'r', encoding='utf-8') as f:
            import json
            config = json.load(f)
            print(f"æ¨¡å‹ç±»å‹: {config.get('model_type', 'æœªçŸ¥')}")
            print(f"æ¶æ„: {config.get('architectures', 'æœªçŸ¥')}")
    else:
        print("âŒ æœªæ‰¾åˆ°config.json")
    
    print("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        padding_side="right"
    )
    tokenizer.pad_token = tokenizer.eos_token
    print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
    
    # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨è¾ƒå°‘çš„æ˜¾å­˜ï¼‰
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # é…ç½®LoRA
    lora_config = LoraConfig(
        r=8,  # å‡å°rankä»¥èŠ‚çœå†…å­˜
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=["c_attn", "c_proj", "w1", "w2"],  # Qwenç‰¹å®šçš„æ¨¡å—
        task_type=TaskType.CAUSAL_LM
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    print("âœ… LoRAé…ç½®åº”ç”¨æˆåŠŸ")
    
    # ç®€å•æµ‹è¯•
    test_input = "ä½ å¥½"
    inputs = tokenizer(test_input, return_tensors="pt").to(model.device)
    print("ğŸ§ª è¿è¡Œç®€å•æµ‹è¯•...")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=20,
            do_sample=True,
            temperature=0.7
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"æµ‹è¯•è¾“å‡º: {response}")
    
    print("ğŸ‰ æ¨¡å‹æµ‹è¯•æˆåŠŸï¼")
    print("ğŸ’¡ å¦‚éœ€å®é™…è®­ç»ƒï¼Œè¯·è¿è¡Œå®Œæ•´çš„è®­ç»ƒè„šæœ¬")
    
except Exception as e:
    print(f"âŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()