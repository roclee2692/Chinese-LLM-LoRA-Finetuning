#!/usr/bin/env python3
"""
ç¯å¢ƒè®¾ç½®è„šæœ¬
è¿è¡Œ: python setup_environment.py
"""

import os
import sys
from pathlib import Path

def create_project_structure():
    """åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„"""
    
    directories = [
        "configs",
        "data/raw",
        "data/processed", 
        "src",
        "scripts",
        "notebooks",
        "demo",
        "results/models",
        "results/logs", 
        "results/evaluation",
        "tests",
        "docs"
    ]
    
    print("ğŸ“ åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„...")
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        print(f"   åˆ›å»ºç›®å½•: {directory}")
    
    # åˆ›å»ºç©ºçš„__init__.pyæ–‡ä»¶
    init_files = ["src/__init__.py", "tests/__init__.py"]
    for init_file in init_files:
        Path(init_file).touch()
        print(f"   åˆ›å»ºæ–‡ä»¶: {init_file}")

def create_config_files():
    """åˆ›å»ºé…ç½®æ–‡ä»¶"""
    
    # åˆ›å»ºåŸºç¡€é…ç½®æ–‡ä»¶
    config_files = {
        ".env.example": """# Environment Variables Template
# å¤åˆ¶æ­¤æ–‡ä»¶ä¸º .env å¹¶å¡«å…¥å®é™…å€¼

# Weights & Biases
WANDB_PROJECT=chinese-llm-lora
WANDB_ENTITY=your_username

# Hugging Face
HF_TOKEN=your_huggingface_token

# Model paths
MODEL_CACHE_DIR=./cache
OUTPUT_DIR=./results
""",

        "requirements-full.txt": """# Complete requirements for production
torch>=2.1.0
torchvision>=0.16.0
torchaudio>=2.1.0
transformers>=4.35.0
tokenizers>=0.14.0
accelerate>=0.24.0
peft>=0.6.0
datasets>=2.14.0
huggingface-hub>=0.19.0
bitsandbytes>=0.41.0
sentencepiece>=0.1.99
protobuf>=4.24.0
evaluate>=0.4.0
nltk>=3.8.0
rouge-score>=0.1.2
sacrebleu>=2.3.0
wandb>=0.16.0
tensorboard>=2.15.0
gradio>=4.0.0
streamlit>=1.28.0
jupyter>=1.0.0
notebook>=7.0.0
pandas>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
omegaconf>=2.3.0
pyyaml>=6.0.0
tqdm>=4.66.0
rich>=13.6.0
packaging>=23.2
psutil>=5.9.0
""",

        "configs/quick_test.yaml": """# Quick Test Configuration for 8GB GPU
model:
  name: "THUDM/chatglm3-6b"
  trust_remote_code: true
  torch_dtype: "bfloat16"
  device_map: "auto"
  load_in_8bit: true  # Enable 8-bit quantization for 8GB GPU

lora:
  r: 8  # Reduced rank for smaller memory footprint
  lora_alpha: 16
  target_modules: 
    - "query_key_value"
    - "dense"
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

training:
  output_dir: "./results/quick_test"
  num_train_epochs: 1
  per_device_train_batch_size: 1  # Small batch size for 8GB GPU
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 8  # Larger accumulation steps
  learning_rate: 2e-4
  warmup_steps: 50
  logging_steps: 5
  save_steps: 100
  eval_steps: 100
  max_seq_length: 256  # Shorter sequences for memory efficiency
  dataloader_num_workers: 0
  remove_unused_columns: false
  
optimizer:
  name: "adamw_torch"
  weight_decay: 0.01
  
scheduler:
  name: "cosine"
  
data:
  dataset_name: "BelleGroup/train_0.5M_CN"
  validation_split: 0.1
  max_samples: 1000  # Very small dataset for quick test
"""
    }
    
    print("\nâš™ï¸  åˆ›å»ºé…ç½®æ–‡ä»¶...")
    for file_path, content in config_files.items():
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"   åˆ›å»ºæ–‡ä»¶: {file_path}")

def create_gitkeep_files():
    """åœ¨ç©ºç›®å½•ä¸­åˆ›å»º.gitkeepæ–‡ä»¶"""
    empty_dirs = [
        "data/raw",
        "data/processed",
        "results/models",
        "results/logs",
        "results/evaluation",
        "tests",
        "docs"
    ]
    
    print("\nğŸ“„ åˆ›å»º.gitkeepæ–‡ä»¶...")
    for directory in empty_dirs:
        gitkeep_path = Path(directory) / ".gitkeep"
        gitkeep_path.touch()
        print(f"   åˆ›å»ºæ–‡ä»¶: {gitkeep_path}")

def create_batch_scripts():
    """åˆ›å»ºæ‰¹å¤„ç†è„šæœ¬"""
    scripts = {
        "activate_env.bat": """@echo off
echo æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ...
call .\llm-lora\Scripts\activate.bat
echo ç¯å¢ƒå·²æ¿€æ´»ï¼è™šæ‹Ÿç¯å¢ƒ: llm-lora
echo ä½¿ç”¨ 'deactivate' é€€å‡ºç¯å¢ƒ
cmd /k
""",
        
        "run_quick_test.bat": """@echo off
echo å¼€å§‹å¿«é€Ÿæµ‹è¯•...
call .\llm-lora\Scripts\activate.bat
python src/train.py --config configs/quick_test.yaml
pause
""",
        
        "start_gradio_demo.bat": """@echo off
echo å¯åŠ¨Gradioæ¼”ç¤ºç•Œé¢...
call .\llm-lora\Scripts\activate.bat
python demo/gradio_demo.py
pause
"""
    }
    
    print("\nğŸ”§ åˆ›å»ºæ‰¹å¤„ç†è„šæœ¬...")
    for script_name, content in scripts.items():
        with open(script_name, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"   åˆ›å»ºæ–‡ä»¶: {script_name}")

def main():
    print("ğŸš€ å¼€å§‹è®¾ç½®é¡¹ç›®ç¯å¢ƒ...")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨é¡¹ç›®æ ¹ç›®å½•
    if not Path("README.md").exists():
        print("âŒ è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
        sys.exit(1)
    
    create_project_structure()
    create_config_files() 
    create_gitkeep_files()
    create_batch_scripts()
    
    print("\nâœ… é¡¹ç›®ç¯å¢ƒè®¾ç½®å®Œæˆ!")
    print("\nğŸ“‹ åç»­æ­¥éª¤:")
    print("1. å¤åˆ¶ .env.example ä¸º .env å¹¶é…ç½®API tokens")
    print("2. è¿è¡Œ 'activate_env.bat' æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ")
    print("3. è¿è¡Œ 'run_quick_test.bat' è¿›è¡Œå¿«é€Ÿæµ‹è¯•")
    print("4. è¿è¡Œ 'start_gradio_demo.bat' å¯åŠ¨Webç•Œé¢")
    print("\nğŸ¯ æ¨èçš„ä¸‹ä¸€æ­¥:")
    print("- è¿è¡Œå¿«é€Ÿæµ‹è¯•éªŒè¯ç¯å¢ƒ: python src/train.py --config configs/quick_test.yaml")
    print("- å¯åŠ¨Gradioç•Œé¢: python demo/gradio_demo.py")

if __name__ == "__main__":
    main()