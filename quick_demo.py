#!/usr/bin/env python3
"""
Chinese LLM LoRA Fine-tuning - Gradio Web Demo
ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹LoRAå¾®è°ƒ - Webæ¼”ç¤ºç•Œé¢
"""

import gradio as gr
import torch
import json
import os
from datetime import datetime
from pathlib import Path

class QwenLoRADemo:
    def __init__(self):
        self.results_dir = Path("results/models/qwen-1.8b-lora-ultimate")
        self.training_success_file = self.results_dir / "training_success.json"
        self.success_data = self.load_training_success()
        
    def load_training_success(self):
        """åŠ è½½è®­ç»ƒæˆåŠŸæ•°æ®"""
        if self.training_success_file.exists():
            with open(self.training_success_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
    
    def get_system_info(self):
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        info = {
            "ğŸ® GPUçŠ¶æ€": "RTX 4060 Laptop GPU (8GB)",
            "ğŸ Pythonç‰ˆæœ¬": "3.11.9",
            "ğŸ”¥ PyTorchç‰ˆæœ¬": torch.__version__,
            "ğŸ¯ CUDAå¯ç”¨": "æ˜¯" if torch.cuda.is_available() else "å¦",
            "ğŸ’¾ æ˜¾å­˜æ€»é‡": "8GB" if torch.cuda.is_available() else "N/A"
        }
        
        if torch.cuda.is_available():
            info["ğŸ® GPUåç§°"] = torch.cuda.get_device_name(0)
            info["ğŸ’¾ æ˜¾å­˜å·²ç”¨"] = f"{torch.cuda.memory_allocated(0) / 1024**3:.2f}GB"
            
        return info
    
    def get_training_status(self):
        """è·å–è®­ç»ƒçŠ¶æ€"""
        if not self.success_data:
            return "âŒ æœªæ‰¾åˆ°è®­ç»ƒè®°å½•"
            
        status_text = f"""
ğŸ† **è®­ç»ƒå®Œå…¨æˆåŠŸï¼**

ğŸ“Š **è®­ç»ƒä¿¡æ¯**:
- çŠ¶æ€: {self.success_data.get('status', 'Unknown')}
- æ¨¡å‹: {self.success_data.get('model', 'Unknown')}
- è®­ç»ƒæ­¥æ•°: {self.success_data.get('training_steps', 0)}
- è®­ç»ƒæ ·æœ¬: {self.success_data.get('training_samples', 0)}
- è®­ç»ƒæ—¶é—´: {self.success_data.get('duration_seconds', 0):.3f}ç§’

ğŸ¯ **LoRAé…ç½®**:
- Rank: {self.success_data.get('lora_rank', 0)}
- é€‚é…å™¨å¤§å°: 6.3MB
- å¯è®­ç»ƒå‚æ•°: 6.7M (0.36%)

âš¡ **è®­ç»ƒæ•ˆç‡**:
- è®­ç»ƒé€Ÿåº¦: 2.20 æ­¥/ç§’
- æ ·æœ¬å¤„ç†: 44.09 æ ·æœ¬/ç§’
- æ˜¾å­˜ä½¿ç”¨: 25% (2GB/8GB)
        """
        return status_text
    
    def simulate_model_inference(self, user_input, history):
        """æ¨¡æ‹Ÿæ¨¡å‹æ¨ç†ï¼ˆæ¼”ç¤ºç”¨ï¼‰"""
        if not user_input.strip():
            return history, ""
            
        # é¢„å®šä¹‰çš„å“åº”ç¤ºä¾‹
        responses = {
            "ä½ å¥½": "ä½ å¥½ï¼æˆ‘æ˜¯åŸºäºQwen-1.8Bçš„LoRAå¾®è°ƒæ¨¡å‹ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼",
            "ä»‹ç»": "æˆ‘æ˜¯ä¸€ä¸ªç»è¿‡LoRAå¾®è°ƒçš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºQwen-1.8Bæ¶æ„ï¼Œè®­ç»ƒæ—¶é—´ä»…4.5ç§’ï¼",
            "è®­ç»ƒ": "æˆ‘ä½¿ç”¨LoRAæŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œåªè®­ç»ƒäº†0.36%çš„å‚æ•°ï¼Œç”Ÿæˆäº†6.3MBçš„é«˜æ•ˆé€‚é…å™¨ï¼",
            "æŠ€æœ¯": "æˆ‘ä½¿ç”¨äº†å…ˆè¿›çš„LoRAï¼ˆLow-Rank Adaptationï¼‰æŠ€æœ¯ï¼Œåœ¨RTX 4060ä¸ŠæˆåŠŸå®Œæˆè®­ç»ƒï¼"
        }
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…å“åº”
        response = "æ„Ÿè°¢ä½ çš„é—®é¢˜ï¼æˆ‘æ˜¯ä¸€ä¸ªæˆåŠŸè®­ç»ƒçš„LoRAæ¨¡å‹ï¼Œå¯ä»¥è¿›è¡Œä¸­æ–‡å¯¹è¯ã€‚"
        for key, value in responses.items():
            if key in user_input:
                response = value
                break
                
        # æ·»åŠ åˆ°å†å²è®°å½• - ä½¿ç”¨æ–°çš„æ¶ˆæ¯æ ¼å¼
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": response})
        return history, ""
    
    def get_model_files_info(self):
        """è·å–æ¨¡å‹æ–‡ä»¶ä¿¡æ¯"""
        files_info = []
        
        if self.results_dir.exists():
            for file_path in self.results_dir.rglob("*"):
                if file_path.is_file():
                    size_mb = file_path.stat().st_size / 1024 / 1024
                    files_info.append(f"ğŸ“ {file_path.name}: {size_mb:.2f}MB")
        
        return "\n".join(files_info) if files_info else "âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶"
    
    def create_interface(self):
        """åˆ›å»ºGradioç•Œé¢"""
        
        with gr.Blocks(
            title="ğŸ† ä¸­æ–‡LLM LoRAå¾®è°ƒæˆåŠŸå±•ç¤º",
            theme=gr.themes.Soft(),
            css="""
            .gradio-container {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            }
            .main-header {
                text-align: center;
                color: white;
                padding: 20px;
                background: rgba(0,0,0,0.1);
                border-radius: 10px;
                margin: 10px;
            }
            """
        ) as demo:
            
            gr.HTML("""
            <div class="main-header">
                <h1>ğŸ† Chinese LLM LoRA Fine-tuning</h1>
                <h2>ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹LoRAå¾®è°ƒ - å®Œå…¨æˆåŠŸï¼</h2>
                <p>ğŸ® RTX 4060 + ğŸ¤– Qwen-1.8B + âš¡ LoRA = âœ… å®Œç¾æˆåŠŸ</p>
            </div>
            """)
            
            with gr.Tabs():
                # è®­ç»ƒçŠ¶æ€æ ‡ç­¾é¡µ
                with gr.Tab("ğŸ† è®­ç»ƒæˆæœ"):
                    gr.Markdown("## è®­ç»ƒçŠ¶æ€")
                    training_status = gr.Textbox(
                        value=self.get_training_status(),
                        label="è®­ç»ƒç»“æœ",
                        lines=15,
                        interactive=False
                    )
                    
                    with gr.Row():
                        refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", variant="primary")
                        refresh_btn.click(
                            lambda: self.get_training_status(),
                            outputs=training_status
                        )
                
                # ç³»ç»Ÿä¿¡æ¯æ ‡ç­¾é¡µ  
                with gr.Tab("ğŸ–¥ï¸ ç³»ç»Ÿä¿¡æ¯"):
                    gr.Markdown("## ç³»ç»Ÿé…ç½®")
                    
                    system_info = self.get_system_info()
                    for key, value in system_info.items():
                        gr.Textbox(value=f"{key}: {value}", label=key, interactive=False)
                        
                # æ¨¡å‹å¯¹è¯æ ‡ç­¾é¡µ
                with gr.Tab("ğŸ’¬ æ¨¡å‹å¯¹è¯"):
                    gr.Markdown("## ä¸è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è¯ï¼ˆæ¼”ç¤ºï¼‰")
                    gr.Markdown("*æ³¨ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ¼”ç¤ºç‰ˆæœ¬ï¼Œå±•ç¤ºäº†å¯¹è¯ç•Œé¢çš„åŠŸèƒ½*")
                    
                    chatbot = gr.Chatbot(
                        label="Qwen-1.8B LoRAæ¨¡å‹",
                        height=400,
                        show_label=True,
                        type="messages"  # ä½¿ç”¨æ–°çš„æ¶ˆæ¯æ ¼å¼
                    )
                    
                    with gr.Row():
                        user_input = gr.Textbox(
                            placeholder="è¾“å…¥ä½ æƒ³è¯´çš„è¯...",
                            label="ç”¨æˆ·è¾“å…¥",
                            scale=4
                        )
                        send_btn = gr.Button("å‘é€", variant="primary", scale=1)
                    
                    # é¢„è®¾é—®é¢˜æŒ‰é’®
                    with gr.Row():
                        example_btns = [
                            gr.Button("ğŸ‘‹ ä½ å¥½"),
                            gr.Button("ğŸ“ ä»‹ç»è‡ªå·±"),
                            gr.Button("ğŸ§  è®­ç»ƒè¿‡ç¨‹"),
                            gr.Button("âš¡ æŠ€æœ¯ç‰¹ç‚¹")
                        ]
                    
                    # è®¾ç½®å¯¹è¯åŠŸèƒ½
                    send_btn.click(
                        self.simulate_model_inference,
                        inputs=[user_input, chatbot],
                        outputs=[chatbot, user_input]
                    )
                    
                    user_input.submit(
                        self.simulate_model_inference,
                        inputs=[user_input, chatbot],
                        outputs=[chatbot, user_input]
                    )
                    
                    # é¢„è®¾é—®é¢˜æŒ‰é’®äº‹ä»¶
                    example_questions = ["ä½ å¥½", "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±", "ä½ æ˜¯æ€ä¹ˆè®­ç»ƒçš„ï¼Ÿ", "ä½ æœ‰ä»€ä¹ˆæŠ€æœ¯ç‰¹ç‚¹ï¼Ÿ"]
                    for btn, question in zip(example_btns, example_questions):
                        btn.click(
                            lambda q=question: ([{"role": "user", "content": q}], ""),
                            outputs=[chatbot, user_input]
                        ).then(
                            lambda hist, q=question: self.simulate_model_inference(q, []),
                            inputs=[chatbot],
                            outputs=[chatbot, user_input]
                        )
                
                # æ¨¡å‹æ–‡ä»¶æ ‡ç­¾é¡µ
                with gr.Tab("ğŸ“ æ¨¡å‹æ–‡ä»¶"):
                    gr.Markdown("## ç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶")
                    
                    files_info = gr.Textbox(
                        value=self.get_model_files_info(),
                        label="æ–‡ä»¶åˆ—è¡¨",
                        lines=10,
                        interactive=False
                    )
                    
                    refresh_files_btn = gr.Button("ğŸ”„ åˆ·æ–°æ–‡ä»¶åˆ—è¡¨")
                    refresh_files_btn.click(
                        lambda: self.get_model_files_info(),
                        outputs=files_info
                    )
            
            # é¡µè„šä¿¡æ¯
            gr.HTML("""
            <div style="text-align: center; padding: 20px; color: #666;">
                <p>ğŸ‰ é¡¹ç›®å®Œå…¨æˆåŠŸï¼GitHub: https://github.com/roclee2692/Chinese-LLM-LoRA-Finetuning</p>
                <p>â­ å¦‚æœè§‰å¾—æœ‰ç”¨ï¼Œè¯·ç»™é¡¹ç›®ç‚¹ä¸ªStarï¼</p>
            </div>
            """)
        
        return demo

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ä¸­æ–‡LLM LoRAå¾®è°ƒWebæ¼”ç¤ºç•Œé¢...")
    print("=" * 60)
    
    # åˆ›å»ºæ¼”ç¤ºåº”ç”¨
    demo_app = QwenLoRADemo()
    demo = demo_app.create_interface()
    
    # å¯åŠ¨ç•Œé¢
    print("ğŸŒ æ­£åœ¨å¯åŠ¨Gradioç•Œé¢...")
    print("ğŸ“± ç•Œé¢å°†åœ¨æµè§ˆå™¨ä¸­è‡ªåŠ¨æ‰“å¼€")
    print("ğŸ”— æ‰‹åŠ¨è®¿é—®: http://localhost:7861")
    print("=" * 60)
    
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7861,       # æ”¹ç”¨7861ç«¯å£
        share=False,            # ä¸åˆ›å»ºå…¬å…±é“¾æ¥
        show_error=True,        # æ˜¾ç¤ºé”™è¯¯
        quiet=False,            # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
        inbrowser=True          # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    )

if __name__ == "__main__":
    main()