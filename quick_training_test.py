#!/usr/bin/env python3
"""
å¿«é€Ÿè®­ç»ƒæµ‹è¯•è„šæœ¬
"""

import os
import sys
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model
from datasets import load_from_disk

def quick_training_test():
    """å¿«é€Ÿè®­ç»ƒæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹å¿«é€Ÿè®­ç»ƒæµ‹è¯•...")
    
    # æ£€æŸ¥GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    model_name = "distilgpt2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None
    )
    
    # 2. é…ç½®LoRA
    print("\nâš™ï¸  é…ç½®LoRA...")
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["c_attn", "c_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 3. åŠ è½½ä¿®å¤åçš„æ•°æ®
    print("\nğŸ“¥ åŠ è½½æ•°æ®...")
    data_path = Path("data/processed/test_data")
    
    if data_path.exists():
        dataset = load_from_disk(data_path)
        print(f"âœ… æˆåŠŸåŠ è½½{len(dataset)}ä¸ªæ ·æœ¬")
        
        # åˆ†å‰²æ•°æ®
        train_size = int(0.8 * len(dataset))
        train_dataset = dataset.select(range(train_size))
        eval_dataset = dataset.select(range(train_size, len(dataset)))
        
        print(f"è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(eval_dataset)}")
    else:
        print("âŒ æ‰¾ä¸åˆ°é¢„å¤„ç†æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ fix_data_format.py")
        return False
    
    # 4. é…ç½®è®­ç»ƒå‚æ•°
    print("\nâš™ï¸  é…ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        output_dir="./results/quick_test",
        num_train_epochs=1,
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=2,
        learning_rate=2e-4,
        warmup_steps=10,
        logging_steps=1,
        save_steps=50,
        evaluation_strategy="steps",
        eval_steps=10,
        save_total_limit=2,
        remove_unused_columns=False,
        dataloader_num_workers=0,
        fp16=device == "cuda",
        report_to="none",  # ä¸ä½¿ç”¨wandb
    )
    
    # 5. æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # 6. åˆ›å»ºè®­ç»ƒå™¨
    print("\nğŸ‹ï¸  åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # 7. å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    try:
        trainer.train()
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
        # 8. ä¿å­˜æ¨¡å‹
        trainer.save_model()
        print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = quick_training_test()
    if success:
        print("\nğŸ‰ å¿«é€Ÿè®­ç»ƒæµ‹è¯•æˆåŠŸï¼")
        print("ğŸ“‹ ä¸‹ä¸€æ­¥å¯ä»¥:")
        print("1. æµ‹è¯•æ¨¡å‹æ¨ç†")
        print("2. è¿è¡Œå®Œæ•´è®­ç»ƒ")
        print("3. å¯åŠ¨Webç•Œé¢")
    else:
        print("\nâŒ è®­ç»ƒæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")